2024-04-27 04:12:32,127:INFO: Effective parameters:
2024-04-27 04:12:32,127:INFO:   <<< batch_size: 16
2024-04-27 04:12:32,127:INFO:   <<< batch_size_val: 16
2024-04-27 04:12:32,127:INFO:   <<< cache_dir: 
2024-04-27 04:12:32,127:INFO:   <<< coef_lr: 0.001
2024-04-27 04:12:32,127:INFO:   <<< cross_model: cross-base
2024-04-27 04:12:32,127:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 04:12:32,127:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 04:12:32,127:INFO:   <<< datatype: msrvtt
2024-04-27 04:12:32,127:INFO:   <<< do_eval: True
2024-04-27 04:12:32,127:INFO:   <<< do_lower_case: False
2024-04-27 04:12:32,127:INFO:   <<< do_pretrain: False
2024-04-27 04:12:32,127:INFO:   <<< do_train: False
2024-04-27 04:12:32,128:INFO:   <<< epochs: 5
2024-04-27 04:12:32,128:INFO:   <<< eval_frame_order: 0
2024-04-27 04:12:32,128:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 04:12:32,128:INFO:   <<< feature_framerate: 1
2024-04-27 04:12:32,128:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 04:12:32,128:INFO:   <<< fp16: False
2024-04-27 04:12:32,128:INFO:   <<< fp16_opt_level: O1
2024-04-27 04:12:32,128:INFO:   <<< freeze_layer_num: 0
2024-04-27 04:12:32,128:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 04:12:32,128:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 04:12:32,128:INFO:   <<< init_model: None
2024-04-27 04:12:32,128:INFO:   <<< linear_patch: 2d
2024-04-27 04:12:32,128:INFO:   <<< local_rank: 0
2024-04-27 04:12:32,128:INFO:   <<< loose_type: True
2024-04-27 04:12:32,128:INFO:   <<< lr: 0.0001
2024-04-27 04:12:32,128:INFO:   <<< lr_decay: 0.9
2024-04-27 04:12:32,128:INFO:   <<< margin: 0.1
2024-04-27 04:12:32,128:INFO:   <<< max_frames: 12
2024-04-27 04:12:32,128:INFO:   <<< max_words: 32
2024-04-27 04:12:32,128:INFO:   <<< n_display: 100
2024-04-27 04:12:32,128:INFO:   <<< n_gpu: 1
2024-04-27 04:12:32,128:INFO:   <<< n_pair: 1
2024-04-27 04:12:32,128:INFO:   <<< negative_weighting: 1
2024-04-27 04:12:32,128:INFO:   <<< num_thread_reader: 0
2024-04-27 04:12:32,128:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 04:12:32,128:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 04:12:32,128:INFO:   <<< rank: 0
2024-04-27 04:12:32,128:INFO:   <<< resume_model: None
2024-04-27 04:12:32,128:INFO:   <<< sampled_use_mil: False
2024-04-27 04:12:32,128:INFO:   <<< seed: 42
2024-04-27 04:12:32,128:INFO:   <<< sim_header: meanP
2024-04-27 04:12:32,128:INFO:   <<< slice_framepos: 2
2024-04-27 04:12:32,128:INFO:   <<< task_type: retrieval
2024-04-27 04:12:32,128:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 04:12:32,128:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 04:12:32,128:INFO:   <<< train_frame_order: 0
2024-04-27 04:12:32,128:INFO:   <<< use_mil: False
2024-04-27 04:12:32,128:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 04:12:32,128:INFO:   <<< video_dim: 1024
2024-04-27 04:12:32,128:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 04:12:32,128:INFO:   <<< warmup_proportion: 0.1
2024-04-27 04:12:32,128:INFO:   <<< world_size: 1
2024-04-27 04:12:32,128:INFO: device: cuda:0 n_gpu: 1
2024-04-27 04:12:32,709:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 04:12:32,709:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 04:12:32,709:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 04:12:32,709:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 04:12:32,709:WARNING: Test retrieval by loose type.
2024-04-27 04:12:32,709:WARNING: 	 embed_dim: 512
2024-04-27 04:12:32,710:WARNING: 	 image_resolution: 224
2024-04-27 04:12:32,710:WARNING: 	 vision_layers: 12
2024-04-27 04:12:32,710:WARNING: 	 vision_width: 768
2024-04-27 04:12:32,710:WARNING: 	 vision_patch_size: 32
2024-04-27 04:12:32,710:WARNING: 	 context_length: 77
2024-04-27 04:12:32,710:WARNING: 	 vocab_size: 49408
2024-04-27 04:12:32,710:WARNING: 	 transformer_width: 512
2024-04-27 04:12:32,710:WARNING: 	 transformer_heads: 8
2024-04-27 04:12:32,710:WARNING: 	 transformer_layers: 12
2024-04-27 04:12:32,710:WARNING: 		 linear_patch: 2d
2024-04-27 04:12:32,710:WARNING: 	 cut_top_layer: 0
2024-04-27 04:12:33,534:WARNING: 	 sim_header: meanP
2024-04-27 04:12:37,122:INFO: --------------------
2024-04-27 04:12:37,122:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 04:12:39,145:INFO: ***** Running test *****
2024-04-27 04:12:39,145:INFO:   Num examples = 1000
2024-04-27 04:12:39,145:INFO:   Batch size = 16
2024-04-27 04:12:39,145:INFO:   Num steps = 63
2024-04-27 04:12:39,145:INFO: ***** Running val *****
2024-04-27 04:12:39,145:INFO:   Num examples = 1000
2024-04-27 04:14:49,871:INFO: sim matrix size: 1000, 1000
2024-04-27 04:14:49,936:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:14:49,936:INFO: Text-to-Video:
2024-04-27 04:14:49,936:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 04:14:49,936:INFO: Video-to-Text:
2024-04-27 04:14:49,936:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 04:19:37,185:INFO: Effective parameters:
2024-04-27 04:19:37,185:INFO:   <<< batch_size: 16
2024-04-27 04:19:37,185:INFO:   <<< batch_size_val: 16
2024-04-27 04:19:37,185:INFO:   <<< cache_dir: 
2024-04-27 04:19:37,185:INFO:   <<< coef_lr: 0.001
2024-04-27 04:19:37,185:INFO:   <<< cross_model: cross-base
2024-04-27 04:19:37,185:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 04:19:37,185:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 04:19:37,185:INFO:   <<< datatype: msrvtt
2024-04-27 04:19:37,185:INFO:   <<< do_eval: False
2024-04-27 04:19:37,185:INFO:   <<< do_lower_case: False
2024-04-27 04:19:37,185:INFO:   <<< do_pretrain: False
2024-04-27 04:19:37,185:INFO:   <<< do_train: True
2024-04-27 04:19:37,185:INFO:   <<< epochs: 5
2024-04-27 04:19:37,185:INFO:   <<< eval_frame_order: 0
2024-04-27 04:19:37,185:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 04:19:37,185:INFO:   <<< feature_framerate: 1
2024-04-27 04:19:37,185:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 04:19:37,185:INFO:   <<< fp16: False
2024-04-27 04:19:37,185:INFO:   <<< fp16_opt_level: O1
2024-04-27 04:19:37,185:INFO:   <<< freeze_layer_num: 0
2024-04-27 04:19:37,185:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 04:19:37,185:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 04:19:37,185:INFO:   <<< init_model: None
2024-04-27 04:19:37,185:INFO:   <<< linear_patch: 2d
2024-04-27 04:19:37,185:INFO:   <<< local_rank: 0
2024-04-27 04:19:37,185:INFO:   <<< loose_type: True
2024-04-27 04:19:37,185:INFO:   <<< lr: 0.0001
2024-04-27 04:19:37,185:INFO:   <<< lr_decay: 0.9
2024-04-27 04:19:37,185:INFO:   <<< margin: 0.1
2024-04-27 04:19:37,185:INFO:   <<< max_frames: 12
2024-04-27 04:19:37,185:INFO:   <<< max_words: 32
2024-04-27 04:19:37,185:INFO:   <<< n_display: 100
2024-04-27 04:19:37,185:INFO:   <<< n_gpu: 1
2024-04-27 04:19:37,185:INFO:   <<< n_pair: 1
2024-04-27 04:19:37,185:INFO:   <<< negative_weighting: 1
2024-04-27 04:19:37,185:INFO:   <<< num_thread_reader: 0
2024-04-27 04:19:37,185:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 04:19:37,185:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 04:19:37,185:INFO:   <<< rank: 0
2024-04-27 04:19:37,185:INFO:   <<< resume_model: None
2024-04-27 04:19:37,185:INFO:   <<< sampled_use_mil: False
2024-04-27 04:19:37,185:INFO:   <<< seed: 42
2024-04-27 04:19:37,185:INFO:   <<< sim_header: meanP
2024-04-27 04:19:37,185:INFO:   <<< slice_framepos: 2
2024-04-27 04:19:37,185:INFO:   <<< task_type: retrieval
2024-04-27 04:19:37,185:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 04:19:37,185:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 04:19:37,185:INFO:   <<< train_frame_order: 0
2024-04-27 04:19:37,186:INFO:   <<< use_mil: False
2024-04-27 04:19:37,186:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 04:19:37,186:INFO:   <<< video_dim: 1024
2024-04-27 04:19:37,186:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 04:19:37,186:INFO:   <<< warmup_proportion: 0.1
2024-04-27 04:19:37,186:INFO:   <<< world_size: 1
2024-04-27 04:19:37,186:INFO: device: cuda:0 n_gpu: 1
2024-04-27 04:19:37,744:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 04:19:37,744:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 04:19:37,744:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 04:19:37,744:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 04:19:37,744:WARNING: Test retrieval by loose type.
2024-04-27 04:19:37,744:WARNING: 	 embed_dim: 512
2024-04-27 04:19:37,744:WARNING: 	 image_resolution: 224
2024-04-27 04:19:37,744:WARNING: 	 vision_layers: 12
2024-04-27 04:19:37,744:WARNING: 	 vision_width: 768
2024-04-27 04:19:37,744:WARNING: 	 vision_patch_size: 32
2024-04-27 04:19:37,744:WARNING: 	 context_length: 77
2024-04-27 04:19:37,744:WARNING: 	 vocab_size: 49408
2024-04-27 04:19:37,744:WARNING: 	 transformer_width: 512
2024-04-27 04:19:37,744:WARNING: 	 transformer_heads: 8
2024-04-27 04:19:37,744:WARNING: 	 transformer_layers: 12
2024-04-27 04:19:37,744:WARNING: 		 linear_patch: 2d
2024-04-27 04:19:37,744:WARNING: 	 cut_top_layer: 0
2024-04-27 04:19:38,571:WARNING: 	 sim_header: meanP
2024-04-27 04:19:42,180:INFO: --------------------
2024-04-27 04:19:42,180:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 04:19:44,203:INFO: ***** Running test *****
2024-04-27 04:19:44,204:INFO:   Num examples = 1000
2024-04-27 04:19:44,204:INFO:   Batch size = 16
2024-04-27 04:19:44,204:INFO:   Num steps = 63
2024-04-27 04:19:44,204:INFO: ***** Running val *****
2024-04-27 04:19:44,204:INFO:   Num examples = 1000
2024-04-27 04:19:44,486:INFO: ***** Running training *****
2024-04-27 04:19:44,486:INFO:   Num examples = 1520
2024-04-27 04:19:44,486:INFO:   Batch size = 16
2024-04-27 04:19:44,486:INFO:   Num steps = 475
2024-04-27 04:23:24,763:INFO: Epoch 1/5 Finished, Train Loss: 0.637211
2024-04-27 04:23:25,428:INFO: Model saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0
2024-04-27 04:23:25,428:INFO: Optimizer saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.0
2024-04-27 04:25:39,376:INFO: sim matrix size: 1000, 1000
2024-04-27 04:25:39,443:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:25:39,443:INFO: Text-to-Video:
2024-04-27 04:25:39,443:INFO: 	>>>  R@1: 33.3 - R@5: 60.6 - R@10: 70.1 - Median R: 3.0 - Mean R: 23.5
2024-04-27 04:25:39,443:INFO: Video-to-Text:
2024-04-27 04:25:39,443:INFO: 	>>>  V2T$R@1: 35.0 - V2T$R@5: 62.0 - V2T$R@10: 72.9 - V2T$Median R: 3.0 - V2T$Mean R: 19.5
2024-04-27 04:25:39,443:INFO: The best model is: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.0, the R1 is: 33.3000
2024-04-27 04:25:50,584:INFO: Epoch: 2/5, Step: 5/95, Lr: 0.000000089, Loss: 0.645119, Time/step: 0.110386
2024-04-27 04:29:18,983:INFO: Epoch 2/5 Finished, Train Loss: 0.363149
2024-04-27 04:29:19,652:INFO: Model saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1
2024-04-27 04:29:19,652:INFO: Optimizer saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.1
2024-04-27 04:31:41,990:INFO: sim matrix size: 1000, 1000
2024-04-27 04:31:42,055:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:31:42,055:INFO: Text-to-Video:
2024-04-27 04:31:42,055:INFO: 	>>>  R@1: 34.5 - R@5: 59.2 - R@10: 70.6 - Median R: 3.0 - Mean R: 23.0
2024-04-27 04:31:42,055:INFO: Video-to-Text:
2024-04-27 04:31:42,055:INFO: 	>>>  V2T$R@1: 33.4 - V2T$R@5: 59.8 - V2T$R@10: 71.3 - V2T$Median R: 3.0 - V2T$Mean R: 22.0
2024-04-27 04:31:42,058:INFO: The best model is: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.1, the R1 is: 34.5000
2024-04-27 04:32:07,070:INFO: Epoch: 3/5, Step: 10/95, Lr: 0.000000062, Loss: 0.185085, Time/step: 0.249058
2024-04-27 04:35:47,433:INFO: Epoch 3/5 Finished, Train Loss: 0.242742
2024-04-27 04:35:48,108:INFO: Model saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2
2024-04-27 04:35:48,108:INFO: Optimizer saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.2
2024-04-27 04:38:04,288:INFO: sim matrix size: 1000, 1000
2024-04-27 04:38:04,352:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:38:04,352:INFO: Text-to-Video:
2024-04-27 04:38:04,352:INFO: 	>>>  R@1: 35.4 - R@5: 60.7 - R@10: 71.4 - Median R: 3.0 - Mean R: 22.2
2024-04-27 04:38:04,352:INFO: Video-to-Text:
2024-04-27 04:38:04,352:INFO: 	>>>  V2T$R@1: 35.3 - V2T$R@5: 61.8 - V2T$R@10: 72.6 - V2T$Median R: 3.0 - V2T$Mean R: 20.0
2024-04-27 04:38:04,355:INFO: The best model is: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.2, the R1 is: 35.4000
2024-04-27 04:38:38,965:INFO: Epoch: 4/5, Step: 15/95, Lr: 0.000000030, Loss: 0.186186, Time/step: 0.345034
2024-04-27 04:41:41,293:INFO: Epoch 4/5 Finished, Train Loss: 0.205359
2024-04-27 04:41:41,962:INFO: Model saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3
2024-04-27 04:41:41,962:INFO: Optimizer saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.3
2024-04-27 04:43:56,971:INFO: sim matrix size: 1000, 1000
2024-04-27 04:43:57,037:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:43:57,037:INFO: Text-to-Video:
2024-04-27 04:43:57,037:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 04:43:57,037:INFO: Video-to-Text:
2024-04-27 04:43:57,037:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.8 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 04:43:57,037:INFO: The best model is: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.3, the R1 is: 35.6000
2024-04-27 04:44:44,828:INFO: Epoch: 5/5, Step: 20/95, Lr: 0.000000006, Loss: 0.158917, Time/step: 0.476842
2024-04-27 04:47:40,734:INFO: Epoch 5/5 Finished, Train Loss: 0.229389
2024-04-27 04:47:41,368:INFO: Model saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 04:47:41,368:INFO: Optimizer saved to /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_opt.bin.4
2024-04-27 04:49:54,961:INFO: sim matrix size: 1000, 1000
2024-04-27 04:49:55,026:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:49:55,026:INFO: Text-to-Video:
2024-04-27 04:49:55,026:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 04:49:55,026:INFO: Video-to-Text:
2024-04-27 04:49:55,026:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 04:49:55,028:INFO: The best model is: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4, the R1 is: 35.6000
2024-04-27 04:51:08,197:INFO: Effective parameters:
2024-04-27 04:51:08,197:INFO:   <<< batch_size: 16
2024-04-27 04:51:08,197:INFO:   <<< batch_size_val: 16
2024-04-27 04:51:08,197:INFO:   <<< cache_dir: 
2024-04-27 04:51:08,197:INFO:   <<< coef_lr: 0.001
2024-04-27 04:51:08,197:INFO:   <<< cross_model: cross-base
2024-04-27 04:51:08,197:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 04:51:08,197:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 04:51:08,197:INFO:   <<< datatype: msrvtt
2024-04-27 04:51:08,197:INFO:   <<< do_eval: True
2024-04-27 04:51:08,197:INFO:   <<< do_lower_case: False
2024-04-27 04:51:08,197:INFO:   <<< do_pretrain: False
2024-04-27 04:51:08,197:INFO:   <<< do_train: False
2024-04-27 04:51:08,197:INFO:   <<< epochs: 5
2024-04-27 04:51:08,197:INFO:   <<< eval_frame_order: 0
2024-04-27 04:51:08,197:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 04:51:08,197:INFO:   <<< feature_framerate: 1
2024-04-27 04:51:08,197:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 04:51:08,197:INFO:   <<< fp16: False
2024-04-27 04:51:08,197:INFO:   <<< fp16_opt_level: O1
2024-04-27 04:51:08,197:INFO:   <<< freeze_layer_num: 0
2024-04-27 04:51:08,197:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 04:51:08,197:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 04:51:08,198:INFO:   <<< init_model: None
2024-04-27 04:51:08,198:INFO:   <<< linear_patch: 2d
2024-04-27 04:51:08,198:INFO:   <<< local_rank: 0
2024-04-27 04:51:08,198:INFO:   <<< loose_type: True
2024-04-27 04:51:08,198:INFO:   <<< lr: 0.0001
2024-04-27 04:51:08,198:INFO:   <<< lr_decay: 0.9
2024-04-27 04:51:08,198:INFO:   <<< margin: 0.1
2024-04-27 04:51:08,198:INFO:   <<< max_frames: 12
2024-04-27 04:51:08,198:INFO:   <<< max_words: 32
2024-04-27 04:51:08,198:INFO:   <<< n_display: 100
2024-04-27 04:51:08,198:INFO:   <<< n_gpu: 1
2024-04-27 04:51:08,198:INFO:   <<< n_pair: 1
2024-04-27 04:51:08,198:INFO:   <<< negative_weighting: 1
2024-04-27 04:51:08,198:INFO:   <<< num_thread_reader: 0
2024-04-27 04:51:08,198:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 04:51:08,198:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 04:51:08,198:INFO:   <<< rank: 0
2024-04-27 04:51:08,198:INFO:   <<< resume_model: None
2024-04-27 04:51:08,198:INFO:   <<< sampled_use_mil: False
2024-04-27 04:51:08,198:INFO:   <<< seed: 42
2024-04-27 04:51:08,198:INFO:   <<< sim_header: meanP
2024-04-27 04:51:08,198:INFO:   <<< slice_framepos: 2
2024-04-27 04:51:08,198:INFO:   <<< task_type: retrieval
2024-04-27 04:51:08,198:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 04:51:08,198:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 04:51:08,198:INFO:   <<< train_frame_order: 0
2024-04-27 04:51:08,198:INFO:   <<< use_mil: False
2024-04-27 04:51:08,198:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 04:51:08,198:INFO:   <<< video_dim: 1024
2024-04-27 04:51:08,198:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 04:51:08,198:INFO:   <<< warmup_proportion: 0.1
2024-04-27 04:51:08,198:INFO:   <<< world_size: 1
2024-04-27 04:51:08,198:INFO: device: cuda:0 n_gpu: 1
2024-04-27 04:51:08,784:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 04:51:08,785:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 04:51:08,785:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 04:51:08,785:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 04:51:08,785:WARNING: Test retrieval by loose type.
2024-04-27 04:51:08,785:WARNING: 	 embed_dim: 512
2024-04-27 04:51:08,785:WARNING: 	 image_resolution: 224
2024-04-27 04:51:08,785:WARNING: 	 vision_layers: 12
2024-04-27 04:51:08,785:WARNING: 	 vision_width: 768
2024-04-27 04:51:08,785:WARNING: 	 vision_patch_size: 32
2024-04-27 04:51:08,785:WARNING: 	 context_length: 77
2024-04-27 04:51:08,785:WARNING: 	 vocab_size: 49408
2024-04-27 04:51:08,785:WARNING: 	 transformer_width: 512
2024-04-27 04:51:08,785:WARNING: 	 transformer_heads: 8
2024-04-27 04:51:08,785:WARNING: 	 transformer_layers: 12
2024-04-27 04:51:08,785:WARNING: 		 linear_patch: 2d
2024-04-27 04:51:08,785:WARNING: 	 cut_top_layer: 0
2024-04-27 04:51:09,610:WARNING: 	 sim_header: meanP
2024-04-27 04:51:13,189:INFO: --------------------
2024-04-27 04:51:13,190:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 04:51:15,201:INFO: ***** Running test *****
2024-04-27 04:51:15,201:INFO:   Num examples = 1000
2024-04-27 04:51:15,201:INFO:   Batch size = 16
2024-04-27 04:51:15,201:INFO:   Num steps = 63
2024-04-27 04:51:15,201:INFO: ***** Running val *****
2024-04-27 04:51:15,201:INFO:   Num examples = 1000
2024-04-27 04:53:29,017:INFO: sim matrix size: 1000, 1000
2024-04-27 04:53:29,082:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:53:29,082:INFO: Text-to-Video:
2024-04-27 04:53:29,082:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 04:53:29,082:INFO: Video-to-Text:
2024-04-27 04:53:29,082:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 04:54:57,063:INFO: Effective parameters:
2024-04-27 04:54:57,063:INFO:   <<< batch_size: 16
2024-04-27 04:54:57,063:INFO:   <<< batch_size_val: 16
2024-04-27 04:54:57,063:INFO:   <<< cache_dir: 
2024-04-27 04:54:57,063:INFO:   <<< coef_lr: 0.001
2024-04-27 04:54:57,063:INFO:   <<< cross_model: cross-base
2024-04-27 04:54:57,063:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 04:54:57,063:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 04:54:57,063:INFO:   <<< datatype: msrvtt
2024-04-27 04:54:57,063:INFO:   <<< do_eval: True
2024-04-27 04:54:57,063:INFO:   <<< do_lower_case: False
2024-04-27 04:54:57,063:INFO:   <<< do_pretrain: False
2024-04-27 04:54:57,063:INFO:   <<< do_train: False
2024-04-27 04:54:57,063:INFO:   <<< epochs: 5
2024-04-27 04:54:57,063:INFO:   <<< eval_frame_order: 0
2024-04-27 04:54:57,063:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 04:54:57,063:INFO:   <<< feature_framerate: 1
2024-04-27 04:54:57,063:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 04:54:57,063:INFO:   <<< fp16: False
2024-04-27 04:54:57,063:INFO:   <<< fp16_opt_level: O1
2024-04-27 04:54:57,063:INFO:   <<< freeze_layer_num: 0
2024-04-27 04:54:57,063:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 04:54:57,063:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 04:54:57,063:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 04:54:57,063:INFO:   <<< linear_patch: 2d
2024-04-27 04:54:57,063:INFO:   <<< local_rank: 0
2024-04-27 04:54:57,063:INFO:   <<< loose_type: True
2024-04-27 04:54:57,063:INFO:   <<< lr: 0.0001
2024-04-27 04:54:57,063:INFO:   <<< lr_decay: 0.9
2024-04-27 04:54:57,063:INFO:   <<< margin: 0.1
2024-04-27 04:54:57,063:INFO:   <<< max_frames: 12
2024-04-27 04:54:57,063:INFO:   <<< max_words: 32
2024-04-27 04:54:57,063:INFO:   <<< n_display: 100
2024-04-27 04:54:57,063:INFO:   <<< n_gpu: 1
2024-04-27 04:54:57,063:INFO:   <<< n_pair: 1
2024-04-27 04:54:57,063:INFO:   <<< negative_weighting: 1
2024-04-27 04:54:57,063:INFO:   <<< num_thread_reader: 0
2024-04-27 04:54:57,063:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 04:54:57,063:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 04:54:57,063:INFO:   <<< rank: 0
2024-04-27 04:54:57,063:INFO:   <<< resume_model: None
2024-04-27 04:54:57,063:INFO:   <<< sampled_use_mil: False
2024-04-27 04:54:57,063:INFO:   <<< seed: 42
2024-04-27 04:54:57,063:INFO:   <<< sim_header: meanP
2024-04-27 04:54:57,063:INFO:   <<< slice_framepos: 2
2024-04-27 04:54:57,063:INFO:   <<< task_type: retrieval
2024-04-27 04:54:57,063:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 04:54:57,063:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 04:54:57,063:INFO:   <<< train_frame_order: 0
2024-04-27 04:54:57,063:INFO:   <<< use_mil: False
2024-04-27 04:54:57,064:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 04:54:57,064:INFO:   <<< video_dim: 1024
2024-04-27 04:54:57,064:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 04:54:57,064:INFO:   <<< warmup_proportion: 0.1
2024-04-27 04:54:57,064:INFO:   <<< world_size: 1
2024-04-27 04:54:57,064:INFO: device: cuda:0 n_gpu: 1
2024-04-27 04:54:57,667:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 04:54:57,668:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 04:54:57,668:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 04:54:57,668:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 04:54:57,668:WARNING: Test retrieval by loose type.
2024-04-27 04:54:57,668:WARNING: 	 embed_dim: 512
2024-04-27 04:54:57,668:WARNING: 	 image_resolution: 224
2024-04-27 04:54:57,668:WARNING: 	 vision_layers: 12
2024-04-27 04:54:57,668:WARNING: 	 vision_width: 768
2024-04-27 04:54:57,668:WARNING: 	 vision_patch_size: 32
2024-04-27 04:54:57,668:WARNING: 	 context_length: 77
2024-04-27 04:54:57,668:WARNING: 	 vocab_size: 49408
2024-04-27 04:54:57,668:WARNING: 	 transformer_width: 512
2024-04-27 04:54:57,668:WARNING: 	 transformer_heads: 8
2024-04-27 04:54:57,668:WARNING: 	 transformer_layers: 12
2024-04-27 04:54:57,668:WARNING: 		 linear_patch: 2d
2024-04-27 04:54:57,668:WARNING: 	 cut_top_layer: 0
2024-04-27 04:54:58,492:WARNING: 	 sim_header: meanP
2024-04-27 04:55:02,083:INFO: --------------------
2024-04-27 04:55:02,084:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 04:55:04,114:INFO: ***** Running test *****
2024-04-27 04:55:04,114:INFO:   Num examples = 1000
2024-04-27 04:55:04,114:INFO:   Batch size = 16
2024-04-27 04:55:04,114:INFO:   Num steps = 63
2024-04-27 04:55:04,114:INFO: ***** Running val *****
2024-04-27 04:55:04,114:INFO:   Num examples = 1000
2024-04-27 04:57:19,860:INFO: sim matrix size: 1000, 1000
2024-04-27 04:57:19,925:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 04:57:19,925:INFO: Text-to-Video:
2024-04-27 04:57:19,925:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 04:57:19,925:INFO: Video-to-Text:
2024-04-27 04:57:19,925:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 05:20:15,984:INFO: Effective parameters:
2024-04-27 05:20:15,984:INFO:   <<< batch_size: 16
2024-04-27 05:20:15,984:INFO:   <<< batch_size_val: 16
2024-04-27 05:20:15,984:INFO:   <<< cache_dir: 
2024-04-27 05:20:15,984:INFO:   <<< coef_lr: 0.001
2024-04-27 05:20:15,984:INFO:   <<< cross_model: cross-base
2024-04-27 05:20:15,984:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:20:15,984:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:20:15,984:INFO:   <<< datatype: msrvtt
2024-04-27 05:20:15,984:INFO:   <<< do_eval: True
2024-04-27 05:20:15,984:INFO:   <<< do_lower_case: False
2024-04-27 05:20:15,984:INFO:   <<< do_pretrain: False
2024-04-27 05:20:15,984:INFO:   <<< do_train: False
2024-04-27 05:20:15,985:INFO:   <<< epochs: 5
2024-04-27 05:20:15,985:INFO:   <<< eval_frame_order: 0
2024-04-27 05:20:15,985:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:20:15,985:INFO:   <<< feature_framerate: 1
2024-04-27 05:20:15,985:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:20:15,985:INFO:   <<< fp16: False
2024-04-27 05:20:15,985:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:20:15,985:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:20:15,985:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:20:15,985:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:20:15,985:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 05:20:15,985:INFO:   <<< linear_patch: 2d
2024-04-27 05:20:15,985:INFO:   <<< local_rank: 0
2024-04-27 05:20:15,985:INFO:   <<< loose_type: True
2024-04-27 05:20:15,985:INFO:   <<< lr: 0.0001
2024-04-27 05:20:15,985:INFO:   <<< lr_decay: 0.9
2024-04-27 05:20:15,985:INFO:   <<< margin: 0.1
2024-04-27 05:20:15,985:INFO:   <<< max_frames: 12
2024-04-27 05:20:15,985:INFO:   <<< max_words: 32
2024-04-27 05:20:15,985:INFO:   <<< n_display: 100
2024-04-27 05:20:15,985:INFO:   <<< n_gpu: 1
2024-04-27 05:20:15,985:INFO:   <<< n_pair: 1
2024-04-27 05:20:15,985:INFO:   <<< negative_weighting: 1
2024-04-27 05:20:15,985:INFO:   <<< num_thread_reader: 0
2024-04-27 05:20:15,985:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:20:15,985:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:20:15,985:INFO:   <<< rank: 0
2024-04-27 05:20:15,985:INFO:   <<< resume_model: None
2024-04-27 05:20:15,985:INFO:   <<< sampled_use_mil: False
2024-04-27 05:20:15,985:INFO:   <<< seed: 42
2024-04-27 05:20:15,985:INFO:   <<< sim_header: meanP
2024-04-27 05:20:15,985:INFO:   <<< slice_framepos: 2
2024-04-27 05:20:15,985:INFO:   <<< task_type: retrieval
2024-04-27 05:20:15,985:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:20:15,985:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:20:15,985:INFO:   <<< train_frame_order: 0
2024-04-27 05:20:15,985:INFO:   <<< use_mil: False
2024-04-27 05:20:15,985:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:20:15,985:INFO:   <<< video_dim: 1024
2024-04-27 05:20:15,985:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:20:15,985:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:20:15,985:INFO:   <<< world_size: 1
2024-04-27 05:20:15,985:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:20:16,593:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:20:16,593:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 05:20:16,593:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 05:20:16,593:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:20:16,593:WARNING: Test retrieval by loose type.
2024-04-27 05:20:16,593:WARNING: 	 embed_dim: 512
2024-04-27 05:20:16,593:WARNING: 	 image_resolution: 224
2024-04-27 05:20:16,593:WARNING: 	 vision_layers: 12
2024-04-27 05:20:16,593:WARNING: 	 vision_width: 768
2024-04-27 05:20:16,593:WARNING: 	 vision_patch_size: 32
2024-04-27 05:20:16,593:WARNING: 	 context_length: 77
2024-04-27 05:20:16,593:WARNING: 	 vocab_size: 49408
2024-04-27 05:20:16,593:WARNING: 	 transformer_width: 512
2024-04-27 05:20:16,593:WARNING: 	 transformer_heads: 8
2024-04-27 05:20:16,593:WARNING: 	 transformer_layers: 12
2024-04-27 05:20:16,593:WARNING: 		 linear_patch: 2d
2024-04-27 05:20:16,593:WARNING: 	 cut_top_layer: 0
2024-04-27 05:20:17,445:WARNING: 	 sim_header: meanP
2024-04-27 05:20:21,048:INFO: --------------------
2024-04-27 05:20:21,048:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 05:20:23,070:INFO: ***** Running test *****
2024-04-27 05:20:23,070:INFO:   Num examples = 1000
2024-04-27 05:20:23,070:INFO:   Batch size = 16
2024-04-27 05:20:23,070:INFO:   Num steps = 63
2024-04-27 05:20:23,070:INFO: ***** Running val *****
2024-04-27 05:20:23,070:INFO:   Num examples = 1000
2024-04-27 05:22:34,588:INFO: sim matrix size: 1000, 1000
2024-04-27 05:22:34,652:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 05:22:34,652:INFO: Text-to-Video:
2024-04-27 05:22:34,653:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 05:22:34,653:INFO: Video-to-Text:
2024-04-27 05:22:34,653:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 05:28:34,349:INFO: Effective parameters:
2024-04-27 05:28:34,350:INFO:   <<< batch_size: 16
2024-04-27 05:28:34,350:INFO:   <<< batch_size_val: 16
2024-04-27 05:28:34,350:INFO:   <<< cache_dir: 
2024-04-27 05:28:34,350:INFO:   <<< coef_lr: 0.001
2024-04-27 05:28:34,350:INFO:   <<< cross_model: cross-base
2024-04-27 05:28:34,350:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:28:34,350:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:28:34,350:INFO:   <<< datatype: msrvtt
2024-04-27 05:28:34,350:INFO:   <<< do_eval: True
2024-04-27 05:28:34,350:INFO:   <<< do_lower_case: False
2024-04-27 05:28:34,350:INFO:   <<< do_pretrain: False
2024-04-27 05:28:34,350:INFO:   <<< do_train: False
2024-04-27 05:28:34,350:INFO:   <<< epochs: 5
2024-04-27 05:28:34,350:INFO:   <<< eval_frame_order: 0
2024-04-27 05:28:34,350:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:28:34,350:INFO:   <<< feature_framerate: 1
2024-04-27 05:28:34,350:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:28:34,350:INFO:   <<< fp16: False
2024-04-27 05:28:34,350:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:28:34,350:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:28:34,350:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:28:34,350:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:28:34,350:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin
2024-04-27 05:28:34,350:INFO:   <<< linear_patch: 2d
2024-04-27 05:28:34,350:INFO:   <<< local_rank: 0
2024-04-27 05:28:34,350:INFO:   <<< loose_type: True
2024-04-27 05:28:34,350:INFO:   <<< lr: 0.0001
2024-04-27 05:28:34,350:INFO:   <<< lr_decay: 0.9
2024-04-27 05:28:34,350:INFO:   <<< margin: 0.1
2024-04-27 05:28:34,350:INFO:   <<< max_frames: 12
2024-04-27 05:28:34,350:INFO:   <<< max_words: 32
2024-04-27 05:28:34,350:INFO:   <<< n_display: 100
2024-04-27 05:28:34,350:INFO:   <<< n_gpu: 1
2024-04-27 05:28:34,350:INFO:   <<< n_pair: 1
2024-04-27 05:28:34,350:INFO:   <<< negative_weighting: 1
2024-04-27 05:28:34,350:INFO:   <<< num_thread_reader: 0
2024-04-27 05:28:34,350:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:28:34,350:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:28:34,350:INFO:   <<< rank: 0
2024-04-27 05:28:34,350:INFO:   <<< resume_model: None
2024-04-27 05:28:34,350:INFO:   <<< sampled_use_mil: False
2024-04-27 05:28:34,350:INFO:   <<< seed: 42
2024-04-27 05:28:34,350:INFO:   <<< sim_header: meanP
2024-04-27 05:28:34,350:INFO:   <<< slice_framepos: 2
2024-04-27 05:28:34,350:INFO:   <<< task_type: retrieval
2024-04-27 05:28:34,350:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:28:34,350:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:28:34,350:INFO:   <<< train_frame_order: 0
2024-04-27 05:28:34,350:INFO:   <<< use_mil: False
2024-04-27 05:28:34,350:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:28:34,350:INFO:   <<< video_dim: 1024
2024-04-27 05:28:34,350:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:28:34,350:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:28:34,350:INFO:   <<< world_size: 1
2024-04-27 05:28:34,350:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:28:35,085:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:28:35,085:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 05:28:35,086:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 05:28:35,086:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:28:35,086:WARNING: Test retrieval by loose type.
2024-04-27 05:28:35,086:WARNING: 	 embed_dim: 512
2024-04-27 05:28:35,086:WARNING: 	 image_resolution: 224
2024-04-27 05:28:35,086:WARNING: 	 vision_layers: 12
2024-04-27 05:28:35,086:WARNING: 	 vision_width: 768
2024-04-27 05:28:35,086:WARNING: 	 vision_patch_size: 32
2024-04-27 05:28:35,086:WARNING: 	 context_length: 77
2024-04-27 05:28:35,086:WARNING: 	 vocab_size: 49408
2024-04-27 05:28:35,086:WARNING: 	 transformer_width: 512
2024-04-27 05:28:35,086:WARNING: 	 transformer_heads: 8
2024-04-27 05:28:35,086:WARNING: 	 transformer_layers: 12
2024-04-27 05:28:35,086:WARNING: 		 linear_patch: 2d
2024-04-27 05:28:35,086:WARNING: 	 cut_top_layer: 0
2024-04-27 05:28:35,921:WARNING: 	 sim_header: meanP
2024-04-27 05:28:39,515:INFO: --------------------
2024-04-27 05:28:39,515:INFO: Weights from pretrained model not used in CLIP4Clip: 
   logit_scale
   text_model.embeddings.position_ids
   text_model.embeddings.token_embedding.weight
   text_model.embeddings.position_embedding.weight
   text_model.encoder.layers.0.self_attn.k_proj.weight
   text_model.encoder.layers.0.self_attn.k_proj.bias
   text_model.encoder.layers.0.self_attn.v_proj.weight
   text_model.encoder.layers.0.self_attn.v_proj.bias
   text_model.encoder.layers.0.self_attn.q_proj.weight
   text_model.encoder.layers.0.self_attn.q_proj.bias
   text_model.encoder.layers.0.self_attn.out_proj.weight
   text_model.encoder.layers.0.self_attn.out_proj.bias
   text_model.encoder.layers.0.layer_norm1.weight
   text_model.encoder.layers.0.layer_norm1.bias
   text_model.encoder.layers.0.mlp.fc1.weight
   text_model.encoder.layers.0.mlp.fc1.bias
   text_model.encoder.layers.0.mlp.fc2.weight
   text_model.encoder.layers.0.mlp.fc2.bias
   text_model.encoder.layers.0.layer_norm2.weight
   text_model.encoder.layers.0.layer_norm2.bias
   text_model.encoder.layers.1.self_attn.k_proj.weight
   text_model.encoder.layers.1.self_attn.k_proj.bias
   text_model.encoder.layers.1.self_attn.v_proj.weight
   text_model.encoder.layers.1.self_attn.v_proj.bias
   text_model.encoder.layers.1.self_attn.q_proj.weight
   text_model.encoder.layers.1.self_attn.q_proj.bias
   text_model.encoder.layers.1.self_attn.out_proj.weight
   text_model.encoder.layers.1.self_attn.out_proj.bias
   text_model.encoder.layers.1.layer_norm1.weight
   text_model.encoder.layers.1.layer_norm1.bias
   text_model.encoder.layers.1.mlp.fc1.weight
   text_model.encoder.layers.1.mlp.fc1.bias
   text_model.encoder.layers.1.mlp.fc2.weight
   text_model.encoder.layers.1.mlp.fc2.bias
   text_model.encoder.layers.1.layer_norm2.weight
   text_model.encoder.layers.1.layer_norm2.bias
   text_model.encoder.layers.2.self_attn.k_proj.weight
   text_model.encoder.layers.2.self_attn.k_proj.bias
   text_model.encoder.layers.2.self_attn.v_proj.weight
   text_model.encoder.layers.2.self_attn.v_proj.bias
   text_model.encoder.layers.2.self_attn.q_proj.weight
   text_model.encoder.layers.2.self_attn.q_proj.bias
   text_model.encoder.layers.2.self_attn.out_proj.weight
   text_model.encoder.layers.2.self_attn.out_proj.bias
   text_model.encoder.layers.2.layer_norm1.weight
   text_model.encoder.layers.2.layer_norm1.bias
   text_model.encoder.layers.2.mlp.fc1.weight
   text_model.encoder.layers.2.mlp.fc1.bias
   text_model.encoder.layers.2.mlp.fc2.weight
   text_model.encoder.layers.2.mlp.fc2.bias
   text_model.encoder.layers.2.layer_norm2.weight
   text_model.encoder.layers.2.layer_norm2.bias
   text_model.encoder.layers.3.self_attn.k_proj.weight
   text_model.encoder.layers.3.self_attn.k_proj.bias
   text_model.encoder.layers.3.self_attn.v_proj.weight
   text_model.encoder.layers.3.self_attn.v_proj.bias
   text_model.encoder.layers.3.self_attn.q_proj.weight
   text_model.encoder.layers.3.self_attn.q_proj.bias
   text_model.encoder.layers.3.self_attn.out_proj.weight
   text_model.encoder.layers.3.self_attn.out_proj.bias
   text_model.encoder.layers.3.layer_norm1.weight
   text_model.encoder.layers.3.layer_norm1.bias
   text_model.encoder.layers.3.mlp.fc1.weight
   text_model.encoder.layers.3.mlp.fc1.bias
   text_model.encoder.layers.3.mlp.fc2.weight
   text_model.encoder.layers.3.mlp.fc2.bias
   text_model.encoder.layers.3.layer_norm2.weight
   text_model.encoder.layers.3.layer_norm2.bias
   text_model.encoder.layers.4.self_attn.k_proj.weight
   text_model.encoder.layers.4.self_attn.k_proj.bias
   text_model.encoder.layers.4.self_attn.v_proj.weight
   text_model.encoder.layers.4.self_attn.v_proj.bias
   text_model.encoder.layers.4.self_attn.q_proj.weight
   text_model.encoder.layers.4.self_attn.q_proj.bias
   text_model.encoder.layers.4.self_attn.out_proj.weight
   text_model.encoder.layers.4.self_attn.out_proj.bias
   text_model.encoder.layers.4.layer_norm1.weight
   text_model.encoder.layers.4.layer_norm1.bias
   text_model.encoder.layers.4.mlp.fc1.weight
   text_model.encoder.layers.4.mlp.fc1.bias
   text_model.encoder.layers.4.mlp.fc2.weight
   text_model.encoder.layers.4.mlp.fc2.bias
   text_model.encoder.layers.4.layer_norm2.weight
   text_model.encoder.layers.4.layer_norm2.bias
   text_model.encoder.layers.5.self_attn.k_proj.weight
   text_model.encoder.layers.5.self_attn.k_proj.bias
   text_model.encoder.layers.5.self_attn.v_proj.weight
   text_model.encoder.layers.5.self_attn.v_proj.bias
   text_model.encoder.layers.5.self_attn.q_proj.weight
   text_model.encoder.layers.5.self_attn.q_proj.bias
   text_model.encoder.layers.5.self_attn.out_proj.weight
   text_model.encoder.layers.5.self_attn.out_proj.bias
   text_model.encoder.layers.5.layer_norm1.weight
   text_model.encoder.layers.5.layer_norm1.bias
   text_model.encoder.layers.5.mlp.fc1.weight
   text_model.encoder.layers.5.mlp.fc1.bias
   text_model.encoder.layers.5.mlp.fc2.weight
   text_model.encoder.layers.5.mlp.fc2.bias
   text_model.encoder.layers.5.layer_norm2.weight
   text_model.encoder.layers.5.layer_norm2.bias
   text_model.encoder.layers.6.self_attn.k_proj.weight
   text_model.encoder.layers.6.self_attn.k_proj.bias
   text_model.encoder.layers.6.self_attn.v_proj.weight
   text_model.encoder.layers.6.self_attn.v_proj.bias
   text_model.encoder.layers.6.self_attn.q_proj.weight
   text_model.encoder.layers.6.self_attn.q_proj.bias
   text_model.encoder.layers.6.self_attn.out_proj.weight
   text_model.encoder.layers.6.self_attn.out_proj.bias
   text_model.encoder.layers.6.layer_norm1.weight
   text_model.encoder.layers.6.layer_norm1.bias
   text_model.encoder.layers.6.mlp.fc1.weight
   text_model.encoder.layers.6.mlp.fc1.bias
   text_model.encoder.layers.6.mlp.fc2.weight
   text_model.encoder.layers.6.mlp.fc2.bias
   text_model.encoder.layers.6.layer_norm2.weight
   text_model.encoder.layers.6.layer_norm2.bias
   text_model.encoder.layers.7.self_attn.k_proj.weight
   text_model.encoder.layers.7.self_attn.k_proj.bias
   text_model.encoder.layers.7.self_attn.v_proj.weight
   text_model.encoder.layers.7.self_attn.v_proj.bias
   text_model.encoder.layers.7.self_attn.q_proj.weight
   text_model.encoder.layers.7.self_attn.q_proj.bias
   text_model.encoder.layers.7.self_attn.out_proj.weight
   text_model.encoder.layers.7.self_attn.out_proj.bias
   text_model.encoder.layers.7.layer_norm1.weight
   text_model.encoder.layers.7.layer_norm1.bias
   text_model.encoder.layers.7.mlp.fc1.weight
   text_model.encoder.layers.7.mlp.fc1.bias
   text_model.encoder.layers.7.mlp.fc2.weight
   text_model.encoder.layers.7.mlp.fc2.bias
   text_model.encoder.layers.7.layer_norm2.weight
   text_model.encoder.layers.7.layer_norm2.bias
   text_model.encoder.layers.8.self_attn.k_proj.weight
   text_model.encoder.layers.8.self_attn.k_proj.bias
   text_model.encoder.layers.8.self_attn.v_proj.weight
   text_model.encoder.layers.8.self_attn.v_proj.bias
   text_model.encoder.layers.8.self_attn.q_proj.weight
   text_model.encoder.layers.8.self_attn.q_proj.bias
   text_model.encoder.layers.8.self_attn.out_proj.weight
   text_model.encoder.layers.8.self_attn.out_proj.bias
   text_model.encoder.layers.8.layer_norm1.weight
   text_model.encoder.layers.8.layer_norm1.bias
   text_model.encoder.layers.8.mlp.fc1.weight
   text_model.encoder.layers.8.mlp.fc1.bias
   text_model.encoder.layers.8.mlp.fc2.weight
   text_model.encoder.layers.8.mlp.fc2.bias
   text_model.encoder.layers.8.layer_norm2.weight
   text_model.encoder.layers.8.layer_norm2.bias
   text_model.encoder.layers.9.self_attn.k_proj.weight
   text_model.encoder.layers.9.self_attn.k_proj.bias
   text_model.encoder.layers.9.self_attn.v_proj.weight
   text_model.encoder.layers.9.self_attn.v_proj.bias
   text_model.encoder.layers.9.self_attn.q_proj.weight
   text_model.encoder.layers.9.self_attn.q_proj.bias
   text_model.encoder.layers.9.self_attn.out_proj.weight
   text_model.encoder.layers.9.self_attn.out_proj.bias
   text_model.encoder.layers.9.layer_norm1.weight
   text_model.encoder.layers.9.layer_norm1.bias
   text_model.encoder.layers.9.mlp.fc1.weight
   text_model.encoder.layers.9.mlp.fc1.bias
   text_model.encoder.layers.9.mlp.fc2.weight
   text_model.encoder.layers.9.mlp.fc2.bias
   text_model.encoder.layers.9.layer_norm2.weight
   text_model.encoder.layers.9.layer_norm2.bias
   text_model.encoder.layers.10.self_attn.k_proj.weight
   text_model.encoder.layers.10.self_attn.k_proj.bias
   text_model.encoder.layers.10.self_attn.v_proj.weight
   text_model.encoder.layers.10.self_attn.v_proj.bias
   text_model.encoder.layers.10.self_attn.q_proj.weight
   text_model.encoder.layers.10.self_attn.q_proj.bias
   text_model.encoder.layers.10.self_attn.out_proj.weight
   text_model.encoder.layers.10.self_attn.out_proj.bias
   text_model.encoder.layers.10.layer_norm1.weight
   text_model.encoder.layers.10.layer_norm1.bias
   text_model.encoder.layers.10.mlp.fc1.weight
   text_model.encoder.layers.10.mlp.fc1.bias
   text_model.encoder.layers.10.mlp.fc2.weight
   text_model.encoder.layers.10.mlp.fc2.bias
   text_model.encoder.layers.10.layer_norm2.weight
   text_model.encoder.layers.10.layer_norm2.bias
   text_model.encoder.layers.11.self_attn.k_proj.weight
   text_model.encoder.layers.11.self_attn.k_proj.bias
   text_model.encoder.layers.11.self_attn.v_proj.weight
   text_model.encoder.layers.11.self_attn.v_proj.bias
   text_model.encoder.layers.11.self_attn.q_proj.weight
   text_model.encoder.layers.11.self_attn.q_proj.bias
   text_model.encoder.layers.11.self_attn.out_proj.weight
   text_model.encoder.layers.11.self_attn.out_proj.bias
   text_model.encoder.layers.11.layer_norm1.weight
   text_model.encoder.layers.11.layer_norm1.bias
   text_model.encoder.layers.11.mlp.fc1.weight
   text_model.encoder.layers.11.mlp.fc1.bias
   text_model.encoder.layers.11.mlp.fc2.weight
   text_model.encoder.layers.11.mlp.fc2.bias
   text_model.encoder.layers.11.layer_norm2.weight
   text_model.encoder.layers.11.layer_norm2.bias
   text_model.final_layer_norm.weight
   text_model.final_layer_norm.bias
   vision_model.embeddings.class_embedding
   vision_model.embeddings.position_ids
   vision_model.embeddings.patch_embedding.weight
   vision_model.embeddings.position_embedding.weight
   vision_model.pre_layrnorm.weight
   vision_model.pre_layrnorm.bias
   vision_model.encoder.layers.0.self_attn.k_proj.weight
   vision_model.encoder.layers.0.self_attn.k_proj.bias
   vision_model.encoder.layers.0.self_attn.v_proj.weight
   vision_model.encoder.layers.0.self_attn.v_proj.bias
   vision_model.encoder.layers.0.self_attn.q_proj.weight
   vision_model.encoder.layers.0.self_attn.q_proj.bias
   vision_model.encoder.layers.0.self_attn.out_proj.weight
   vision_model.encoder.layers.0.self_attn.out_proj.bias
   vision_model.encoder.layers.0.layer_norm1.weight
   vision_model.encoder.layers.0.layer_norm1.bias
   vision_model.encoder.layers.0.mlp.fc1.weight
   vision_model.encoder.layers.0.mlp.fc1.bias
   vision_model.encoder.layers.0.mlp.fc2.weight
   vision_model.encoder.layers.0.mlp.fc2.bias
   vision_model.encoder.layers.0.layer_norm2.weight
   vision_model.encoder.layers.0.layer_norm2.bias
   vision_model.encoder.layers.1.self_attn.k_proj.weight
   vision_model.encoder.layers.1.self_attn.k_proj.bias
   vision_model.encoder.layers.1.self_attn.v_proj.weight
   vision_model.encoder.layers.1.self_attn.v_proj.bias
   vision_model.encoder.layers.1.self_attn.q_proj.weight
   vision_model.encoder.layers.1.self_attn.q_proj.bias
   vision_model.encoder.layers.1.self_attn.out_proj.weight
   vision_model.encoder.layers.1.self_attn.out_proj.bias
   vision_model.encoder.layers.1.layer_norm1.weight
   vision_model.encoder.layers.1.layer_norm1.bias
   vision_model.encoder.layers.1.mlp.fc1.weight
   vision_model.encoder.layers.1.mlp.fc1.bias
   vision_model.encoder.layers.1.mlp.fc2.weight
   vision_model.encoder.layers.1.mlp.fc2.bias
   vision_model.encoder.layers.1.layer_norm2.weight
   vision_model.encoder.layers.1.layer_norm2.bias
   vision_model.encoder.layers.2.self_attn.k_proj.weight
   vision_model.encoder.layers.2.self_attn.k_proj.bias
   vision_model.encoder.layers.2.self_attn.v_proj.weight
   vision_model.encoder.layers.2.self_attn.v_proj.bias
   vision_model.encoder.layers.2.self_attn.q_proj.weight
   vision_model.encoder.layers.2.self_attn.q_proj.bias
   vision_model.encoder.layers.2.self_attn.out_proj.weight
   vision_model.encoder.layers.2.self_attn.out_proj.bias
   vision_model.encoder.layers.2.layer_norm1.weight
   vision_model.encoder.layers.2.layer_norm1.bias
   vision_model.encoder.layers.2.mlp.fc1.weight
   vision_model.encoder.layers.2.mlp.fc1.bias
   vision_model.encoder.layers.2.mlp.fc2.weight
   vision_model.encoder.layers.2.mlp.fc2.bias
   vision_model.encoder.layers.2.layer_norm2.weight
   vision_model.encoder.layers.2.layer_norm2.bias
   vision_model.encoder.layers.3.self_attn.k_proj.weight
   vision_model.encoder.layers.3.self_attn.k_proj.bias
   vision_model.encoder.layers.3.self_attn.v_proj.weight
   vision_model.encoder.layers.3.self_attn.v_proj.bias
   vision_model.encoder.layers.3.self_attn.q_proj.weight
   vision_model.encoder.layers.3.self_attn.q_proj.bias
   vision_model.encoder.layers.3.self_attn.out_proj.weight
   vision_model.encoder.layers.3.self_attn.out_proj.bias
   vision_model.encoder.layers.3.layer_norm1.weight
   vision_model.encoder.layers.3.layer_norm1.bias
   vision_model.encoder.layers.3.mlp.fc1.weight
   vision_model.encoder.layers.3.mlp.fc1.bias
   vision_model.encoder.layers.3.mlp.fc2.weight
   vision_model.encoder.layers.3.mlp.fc2.bias
   vision_model.encoder.layers.3.layer_norm2.weight
   vision_model.encoder.layers.3.layer_norm2.bias
   vision_model.encoder.layers.4.self_attn.k_proj.weight
   vision_model.encoder.layers.4.self_attn.k_proj.bias
   vision_model.encoder.layers.4.self_attn.v_proj.weight
   vision_model.encoder.layers.4.self_attn.v_proj.bias
   vision_model.encoder.layers.4.self_attn.q_proj.weight
   vision_model.encoder.layers.4.self_attn.q_proj.bias
   vision_model.encoder.layers.4.self_attn.out_proj.weight
   vision_model.encoder.layers.4.self_attn.out_proj.bias
   vision_model.encoder.layers.4.layer_norm1.weight
   vision_model.encoder.layers.4.layer_norm1.bias
   vision_model.encoder.layers.4.mlp.fc1.weight
   vision_model.encoder.layers.4.mlp.fc1.bias
   vision_model.encoder.layers.4.mlp.fc2.weight
   vision_model.encoder.layers.4.mlp.fc2.bias
   vision_model.encoder.layers.4.layer_norm2.weight
   vision_model.encoder.layers.4.layer_norm2.bias
   vision_model.encoder.layers.5.self_attn.k_proj.weight
   vision_model.encoder.layers.5.self_attn.k_proj.bias
   vision_model.encoder.layers.5.self_attn.v_proj.weight
   vision_model.encoder.layers.5.self_attn.v_proj.bias
   vision_model.encoder.layers.5.self_attn.q_proj.weight
   vision_model.encoder.layers.5.self_attn.q_proj.bias
   vision_model.encoder.layers.5.self_attn.out_proj.weight
   vision_model.encoder.layers.5.self_attn.out_proj.bias
   vision_model.encoder.layers.5.layer_norm1.weight
   vision_model.encoder.layers.5.layer_norm1.bias
   vision_model.encoder.layers.5.mlp.fc1.weight
   vision_model.encoder.layers.5.mlp.fc1.bias
   vision_model.encoder.layers.5.mlp.fc2.weight
   vision_model.encoder.layers.5.mlp.fc2.bias
   vision_model.encoder.layers.5.layer_norm2.weight
   vision_model.encoder.layers.5.layer_norm2.bias
   vision_model.encoder.layers.6.self_attn.k_proj.weight
   vision_model.encoder.layers.6.self_attn.k_proj.bias
   vision_model.encoder.layers.6.self_attn.v_proj.weight
   vision_model.encoder.layers.6.self_attn.v_proj.bias
   vision_model.encoder.layers.6.self_attn.q_proj.weight
   vision_model.encoder.layers.6.self_attn.q_proj.bias
   vision_model.encoder.layers.6.self_attn.out_proj.weight
   vision_model.encoder.layers.6.self_attn.out_proj.bias
   vision_model.encoder.layers.6.layer_norm1.weight
   vision_model.encoder.layers.6.layer_norm1.bias
   vision_model.encoder.layers.6.mlp.fc1.weight
   vision_model.encoder.layers.6.mlp.fc1.bias
   vision_model.encoder.layers.6.mlp.fc2.weight
   vision_model.encoder.layers.6.mlp.fc2.bias
   vision_model.encoder.layers.6.layer_norm2.weight
   vision_model.encoder.layers.6.layer_norm2.bias
   vision_model.encoder.layers.7.self_attn.k_proj.weight
   vision_model.encoder.layers.7.self_attn.k_proj.bias
   vision_model.encoder.layers.7.self_attn.v_proj.weight
   vision_model.encoder.layers.7.self_attn.v_proj.bias
   vision_model.encoder.layers.7.self_attn.q_proj.weight
   vision_model.encoder.layers.7.self_attn.q_proj.bias
   vision_model.encoder.layers.7.self_attn.out_proj.weight
   vision_model.encoder.layers.7.self_attn.out_proj.bias
   vision_model.encoder.layers.7.layer_norm1.weight
   vision_model.encoder.layers.7.layer_norm1.bias
   vision_model.encoder.layers.7.mlp.fc1.weight
   vision_model.encoder.layers.7.mlp.fc1.bias
   vision_model.encoder.layers.7.mlp.fc2.weight
   vision_model.encoder.layers.7.mlp.fc2.bias
   vision_model.encoder.layers.7.layer_norm2.weight
   vision_model.encoder.layers.7.layer_norm2.bias
   vision_model.encoder.layers.8.self_attn.k_proj.weight
   vision_model.encoder.layers.8.self_attn.k_proj.bias
   vision_model.encoder.layers.8.self_attn.v_proj.weight
   vision_model.encoder.layers.8.self_attn.v_proj.bias
   vision_model.encoder.layers.8.self_attn.q_proj.weight
   vision_model.encoder.layers.8.self_attn.q_proj.bias
   vision_model.encoder.layers.8.self_attn.out_proj.weight
   vision_model.encoder.layers.8.self_attn.out_proj.bias
   vision_model.encoder.layers.8.layer_norm1.weight
   vision_model.encoder.layers.8.layer_norm1.bias
   vision_model.encoder.layers.8.mlp.fc1.weight
   vision_model.encoder.layers.8.mlp.fc1.bias
   vision_model.encoder.layers.8.mlp.fc2.weight
   vision_model.encoder.layers.8.mlp.fc2.bias
   vision_model.encoder.layers.8.layer_norm2.weight
   vision_model.encoder.layers.8.layer_norm2.bias
   vision_model.encoder.layers.9.self_attn.k_proj.weight
   vision_model.encoder.layers.9.self_attn.k_proj.bias
   vision_model.encoder.layers.9.self_attn.v_proj.weight
   vision_model.encoder.layers.9.self_attn.v_proj.bias
   vision_model.encoder.layers.9.self_attn.q_proj.weight
   vision_model.encoder.layers.9.self_attn.q_proj.bias
   vision_model.encoder.layers.9.self_attn.out_proj.weight
   vision_model.encoder.layers.9.self_attn.out_proj.bias
   vision_model.encoder.layers.9.layer_norm1.weight
   vision_model.encoder.layers.9.layer_norm1.bias
   vision_model.encoder.layers.9.mlp.fc1.weight
   vision_model.encoder.layers.9.mlp.fc1.bias
   vision_model.encoder.layers.9.mlp.fc2.weight
   vision_model.encoder.layers.9.mlp.fc2.bias
   vision_model.encoder.layers.9.layer_norm2.weight
   vision_model.encoder.layers.9.layer_norm2.bias
   vision_model.encoder.layers.10.self_attn.k_proj.weight
   vision_model.encoder.layers.10.self_attn.k_proj.bias
   vision_model.encoder.layers.10.self_attn.v_proj.weight
   vision_model.encoder.layers.10.self_attn.v_proj.bias
   vision_model.encoder.layers.10.self_attn.q_proj.weight
   vision_model.encoder.layers.10.self_attn.q_proj.bias
   vision_model.encoder.layers.10.self_attn.out_proj.weight
   vision_model.encoder.layers.10.self_attn.out_proj.bias
   vision_model.encoder.layers.10.layer_norm1.weight
   vision_model.encoder.layers.10.layer_norm1.bias
   vision_model.encoder.layers.10.mlp.fc1.weight
   vision_model.encoder.layers.10.mlp.fc1.bias
   vision_model.encoder.layers.10.mlp.fc2.weight
   vision_model.encoder.layers.10.mlp.fc2.bias
   vision_model.encoder.layers.10.layer_norm2.weight
   vision_model.encoder.layers.10.layer_norm2.bias
   vision_model.encoder.layers.11.self_attn.k_proj.weight
   vision_model.encoder.layers.11.self_attn.k_proj.bias
   vision_model.encoder.layers.11.self_attn.v_proj.weight
   vision_model.encoder.layers.11.self_attn.v_proj.bias
   vision_model.encoder.layers.11.self_attn.q_proj.weight
   vision_model.encoder.layers.11.self_attn.q_proj.bias
   vision_model.encoder.layers.11.self_attn.out_proj.weight
   vision_model.encoder.layers.11.self_attn.out_proj.bias
   vision_model.encoder.layers.11.layer_norm1.weight
   vision_model.encoder.layers.11.layer_norm1.bias
   vision_model.encoder.layers.11.mlp.fc1.weight
   vision_model.encoder.layers.11.mlp.fc1.bias
   vision_model.encoder.layers.11.mlp.fc2.weight
   vision_model.encoder.layers.11.mlp.fc2.bias
   vision_model.encoder.layers.11.layer_norm2.weight
   vision_model.encoder.layers.11.layer_norm2.bias
   vision_model.post_layernorm.weight
   vision_model.post_layernorm.bias
   visual_projection.weight
   text_projection.weight
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 05:28:41,524:INFO: ***** Running test *****
2024-04-27 05:28:41,524:INFO:   Num examples = 1000
2024-04-27 05:28:41,524:INFO:   Batch size = 16
2024-04-27 05:28:41,524:INFO:   Num steps = 63
2024-04-27 05:28:41,524:INFO: ***** Running val *****
2024-04-27 05:28:41,524:INFO:   Num examples = 1000
2024-04-27 05:30:55,451:INFO: sim matrix size: 1000, 1000
2024-04-27 05:30:55,516:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 05:30:55,516:INFO: Text-to-Video:
2024-04-27 05:30:55,516:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 05:30:55,516:INFO: Video-to-Text:
2024-04-27 05:30:55,516:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 05:33:38,377:INFO: Effective parameters:
2024-04-27 05:33:38,378:INFO:   <<< batch_size: 16
2024-04-27 05:33:38,378:INFO:   <<< batch_size_val: 16
2024-04-27 05:33:38,378:INFO:   <<< cache_dir: 
2024-04-27 05:33:38,378:INFO:   <<< coef_lr: 0.001
2024-04-27 05:33:38,378:INFO:   <<< cross_model: cross-base
2024-04-27 05:33:38,378:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:33:38,378:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:33:38,378:INFO:   <<< datatype: msrvtt
2024-04-27 05:33:38,378:INFO:   <<< do_eval: True
2024-04-27 05:33:38,378:INFO:   <<< do_lower_case: False
2024-04-27 05:33:38,378:INFO:   <<< do_pretrain: False
2024-04-27 05:33:38,378:INFO:   <<< do_train: False
2024-04-27 05:33:38,378:INFO:   <<< epochs: 5
2024-04-27 05:33:38,378:INFO:   <<< eval_frame_order: 0
2024-04-27 05:33:38,378:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:33:38,378:INFO:   <<< feature_framerate: 1
2024-04-27 05:33:38,378:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:33:38,378:INFO:   <<< fp16: False
2024-04-27 05:33:38,378:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:33:38,378:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:33:38,378:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:33:38,378:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:33:38,378:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.5
2024-04-27 05:33:38,378:INFO:   <<< linear_patch: 2d
2024-04-27 05:33:38,378:INFO:   <<< local_rank: 0
2024-04-27 05:33:38,378:INFO:   <<< loose_type: True
2024-04-27 05:33:38,378:INFO:   <<< lr: 0.0001
2024-04-27 05:33:38,378:INFO:   <<< lr_decay: 0.9
2024-04-27 05:33:38,378:INFO:   <<< margin: 0.1
2024-04-27 05:33:38,378:INFO:   <<< max_frames: 12
2024-04-27 05:33:38,378:INFO:   <<< max_words: 32
2024-04-27 05:33:38,378:INFO:   <<< n_display: 100
2024-04-27 05:33:38,378:INFO:   <<< n_gpu: 1
2024-04-27 05:33:38,378:INFO:   <<< n_pair: 1
2024-04-27 05:33:38,378:INFO:   <<< negative_weighting: 1
2024-04-27 05:33:38,378:INFO:   <<< num_thread_reader: 0
2024-04-27 05:33:38,378:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:33:38,378:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:33:38,378:INFO:   <<< rank: 0
2024-04-27 05:33:38,378:INFO:   <<< resume_model: None
2024-04-27 05:33:38,378:INFO:   <<< sampled_use_mil: False
2024-04-27 05:33:38,378:INFO:   <<< seed: 42
2024-04-27 05:33:38,378:INFO:   <<< sim_header: meanP
2024-04-27 05:33:38,378:INFO:   <<< slice_framepos: 2
2024-04-27 05:33:38,378:INFO:   <<< task_type: retrieval
2024-04-27 05:33:38,378:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:33:38,378:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:33:38,378:INFO:   <<< train_frame_order: 0
2024-04-27 05:33:38,378:INFO:   <<< use_mil: False
2024-04-27 05:33:38,378:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:33:38,378:INFO:   <<< video_dim: 1024
2024-04-27 05:33:38,378:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:33:38,378:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:33:38,378:INFO:   <<< world_size: 1
2024-04-27 05:33:38,379:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:33:39,099:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:33:39,099:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 05:33:39,099:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 05:33:39,099:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:33:39,099:WARNING: Test retrieval by loose type.
2024-04-27 05:33:39,099:WARNING: 	 embed_dim: 512
2024-04-27 05:33:39,099:WARNING: 	 image_resolution: 224
2024-04-27 05:33:39,099:WARNING: 	 vision_layers: 12
2024-04-27 05:33:39,099:WARNING: 	 vision_width: 768
2024-04-27 05:33:39,099:WARNING: 	 vision_patch_size: 32
2024-04-27 05:33:39,099:WARNING: 	 context_length: 77
2024-04-27 05:33:39,099:WARNING: 	 vocab_size: 49408
2024-04-27 05:33:39,099:WARNING: 	 transformer_width: 512
2024-04-27 05:33:39,099:WARNING: 	 transformer_heads: 8
2024-04-27 05:33:39,099:WARNING: 	 transformer_layers: 12
2024-04-27 05:33:39,099:WARNING: 		 linear_patch: 2d
2024-04-27 05:33:39,099:WARNING: 	 cut_top_layer: 0
2024-04-27 05:33:39,918:WARNING: 	 sim_header: meanP
2024-04-27 05:33:43,513:INFO: --------------------
2024-04-27 05:33:43,513:INFO: Weights from pretrained model not used in CLIP4Clip: 
   logit_scale
   text_model.embeddings.position_ids
   text_model.embeddings.token_embedding.weight
   text_model.embeddings.position_embedding.weight
   text_model.encoder.layers.0.self_attn.k_proj.weight
   text_model.encoder.layers.0.self_attn.k_proj.bias
   text_model.encoder.layers.0.self_attn.v_proj.weight
   text_model.encoder.layers.0.self_attn.v_proj.bias
   text_model.encoder.layers.0.self_attn.q_proj.weight
   text_model.encoder.layers.0.self_attn.q_proj.bias
   text_model.encoder.layers.0.self_attn.out_proj.weight
   text_model.encoder.layers.0.self_attn.out_proj.bias
   text_model.encoder.layers.0.layer_norm1.weight
   text_model.encoder.layers.0.layer_norm1.bias
   text_model.encoder.layers.0.mlp.fc1.weight
   text_model.encoder.layers.0.mlp.fc1.bias
   text_model.encoder.layers.0.mlp.fc2.weight
   text_model.encoder.layers.0.mlp.fc2.bias
   text_model.encoder.layers.0.layer_norm2.weight
   text_model.encoder.layers.0.layer_norm2.bias
   text_model.encoder.layers.1.self_attn.k_proj.weight
   text_model.encoder.layers.1.self_attn.k_proj.bias
   text_model.encoder.layers.1.self_attn.v_proj.weight
   text_model.encoder.layers.1.self_attn.v_proj.bias
   text_model.encoder.layers.1.self_attn.q_proj.weight
   text_model.encoder.layers.1.self_attn.q_proj.bias
   text_model.encoder.layers.1.self_attn.out_proj.weight
   text_model.encoder.layers.1.self_attn.out_proj.bias
   text_model.encoder.layers.1.layer_norm1.weight
   text_model.encoder.layers.1.layer_norm1.bias
   text_model.encoder.layers.1.mlp.fc1.weight
   text_model.encoder.layers.1.mlp.fc1.bias
   text_model.encoder.layers.1.mlp.fc2.weight
   text_model.encoder.layers.1.mlp.fc2.bias
   text_model.encoder.layers.1.layer_norm2.weight
   text_model.encoder.layers.1.layer_norm2.bias
   text_model.encoder.layers.2.self_attn.k_proj.weight
   text_model.encoder.layers.2.self_attn.k_proj.bias
   text_model.encoder.layers.2.self_attn.v_proj.weight
   text_model.encoder.layers.2.self_attn.v_proj.bias
   text_model.encoder.layers.2.self_attn.q_proj.weight
   text_model.encoder.layers.2.self_attn.q_proj.bias
   text_model.encoder.layers.2.self_attn.out_proj.weight
   text_model.encoder.layers.2.self_attn.out_proj.bias
   text_model.encoder.layers.2.layer_norm1.weight
   text_model.encoder.layers.2.layer_norm1.bias
   text_model.encoder.layers.2.mlp.fc1.weight
   text_model.encoder.layers.2.mlp.fc1.bias
   text_model.encoder.layers.2.mlp.fc2.weight
   text_model.encoder.layers.2.mlp.fc2.bias
   text_model.encoder.layers.2.layer_norm2.weight
   text_model.encoder.layers.2.layer_norm2.bias
   text_model.encoder.layers.3.self_attn.k_proj.weight
   text_model.encoder.layers.3.self_attn.k_proj.bias
   text_model.encoder.layers.3.self_attn.v_proj.weight
   text_model.encoder.layers.3.self_attn.v_proj.bias
   text_model.encoder.layers.3.self_attn.q_proj.weight
   text_model.encoder.layers.3.self_attn.q_proj.bias
   text_model.encoder.layers.3.self_attn.out_proj.weight
   text_model.encoder.layers.3.self_attn.out_proj.bias
   text_model.encoder.layers.3.layer_norm1.weight
   text_model.encoder.layers.3.layer_norm1.bias
   text_model.encoder.layers.3.mlp.fc1.weight
   text_model.encoder.layers.3.mlp.fc1.bias
   text_model.encoder.layers.3.mlp.fc2.weight
   text_model.encoder.layers.3.mlp.fc2.bias
   text_model.encoder.layers.3.layer_norm2.weight
   text_model.encoder.layers.3.layer_norm2.bias
   text_model.encoder.layers.4.self_attn.k_proj.weight
   text_model.encoder.layers.4.self_attn.k_proj.bias
   text_model.encoder.layers.4.self_attn.v_proj.weight
   text_model.encoder.layers.4.self_attn.v_proj.bias
   text_model.encoder.layers.4.self_attn.q_proj.weight
   text_model.encoder.layers.4.self_attn.q_proj.bias
   text_model.encoder.layers.4.self_attn.out_proj.weight
   text_model.encoder.layers.4.self_attn.out_proj.bias
   text_model.encoder.layers.4.layer_norm1.weight
   text_model.encoder.layers.4.layer_norm1.bias
   text_model.encoder.layers.4.mlp.fc1.weight
   text_model.encoder.layers.4.mlp.fc1.bias
   text_model.encoder.layers.4.mlp.fc2.weight
   text_model.encoder.layers.4.mlp.fc2.bias
   text_model.encoder.layers.4.layer_norm2.weight
   text_model.encoder.layers.4.layer_norm2.bias
   text_model.encoder.layers.5.self_attn.k_proj.weight
   text_model.encoder.layers.5.self_attn.k_proj.bias
   text_model.encoder.layers.5.self_attn.v_proj.weight
   text_model.encoder.layers.5.self_attn.v_proj.bias
   text_model.encoder.layers.5.self_attn.q_proj.weight
   text_model.encoder.layers.5.self_attn.q_proj.bias
   text_model.encoder.layers.5.self_attn.out_proj.weight
   text_model.encoder.layers.5.self_attn.out_proj.bias
   text_model.encoder.layers.5.layer_norm1.weight
   text_model.encoder.layers.5.layer_norm1.bias
   text_model.encoder.layers.5.mlp.fc1.weight
   text_model.encoder.layers.5.mlp.fc1.bias
   text_model.encoder.layers.5.mlp.fc2.weight
   text_model.encoder.layers.5.mlp.fc2.bias
   text_model.encoder.layers.5.layer_norm2.weight
   text_model.encoder.layers.5.layer_norm2.bias
   text_model.encoder.layers.6.self_attn.k_proj.weight
   text_model.encoder.layers.6.self_attn.k_proj.bias
   text_model.encoder.layers.6.self_attn.v_proj.weight
   text_model.encoder.layers.6.self_attn.v_proj.bias
   text_model.encoder.layers.6.self_attn.q_proj.weight
   text_model.encoder.layers.6.self_attn.q_proj.bias
   text_model.encoder.layers.6.self_attn.out_proj.weight
   text_model.encoder.layers.6.self_attn.out_proj.bias
   text_model.encoder.layers.6.layer_norm1.weight
   text_model.encoder.layers.6.layer_norm1.bias
   text_model.encoder.layers.6.mlp.fc1.weight
   text_model.encoder.layers.6.mlp.fc1.bias
   text_model.encoder.layers.6.mlp.fc2.weight
   text_model.encoder.layers.6.mlp.fc2.bias
   text_model.encoder.layers.6.layer_norm2.weight
   text_model.encoder.layers.6.layer_norm2.bias
   text_model.encoder.layers.7.self_attn.k_proj.weight
   text_model.encoder.layers.7.self_attn.k_proj.bias
   text_model.encoder.layers.7.self_attn.v_proj.weight
   text_model.encoder.layers.7.self_attn.v_proj.bias
   text_model.encoder.layers.7.self_attn.q_proj.weight
   text_model.encoder.layers.7.self_attn.q_proj.bias
   text_model.encoder.layers.7.self_attn.out_proj.weight
   text_model.encoder.layers.7.self_attn.out_proj.bias
   text_model.encoder.layers.7.layer_norm1.weight
   text_model.encoder.layers.7.layer_norm1.bias
   text_model.encoder.layers.7.mlp.fc1.weight
   text_model.encoder.layers.7.mlp.fc1.bias
   text_model.encoder.layers.7.mlp.fc2.weight
   text_model.encoder.layers.7.mlp.fc2.bias
   text_model.encoder.layers.7.layer_norm2.weight
   text_model.encoder.layers.7.layer_norm2.bias
   text_model.encoder.layers.8.self_attn.k_proj.weight
   text_model.encoder.layers.8.self_attn.k_proj.bias
   text_model.encoder.layers.8.self_attn.v_proj.weight
   text_model.encoder.layers.8.self_attn.v_proj.bias
   text_model.encoder.layers.8.self_attn.q_proj.weight
   text_model.encoder.layers.8.self_attn.q_proj.bias
   text_model.encoder.layers.8.self_attn.out_proj.weight
   text_model.encoder.layers.8.self_attn.out_proj.bias
   text_model.encoder.layers.8.layer_norm1.weight
   text_model.encoder.layers.8.layer_norm1.bias
   text_model.encoder.layers.8.mlp.fc1.weight
   text_model.encoder.layers.8.mlp.fc1.bias
   text_model.encoder.layers.8.mlp.fc2.weight
   text_model.encoder.layers.8.mlp.fc2.bias
   text_model.encoder.layers.8.layer_norm2.weight
   text_model.encoder.layers.8.layer_norm2.bias
   text_model.encoder.layers.9.self_attn.k_proj.weight
   text_model.encoder.layers.9.self_attn.k_proj.bias
   text_model.encoder.layers.9.self_attn.v_proj.weight
   text_model.encoder.layers.9.self_attn.v_proj.bias
   text_model.encoder.layers.9.self_attn.q_proj.weight
   text_model.encoder.layers.9.self_attn.q_proj.bias
   text_model.encoder.layers.9.self_attn.out_proj.weight
   text_model.encoder.layers.9.self_attn.out_proj.bias
   text_model.encoder.layers.9.layer_norm1.weight
   text_model.encoder.layers.9.layer_norm1.bias
   text_model.encoder.layers.9.mlp.fc1.weight
   text_model.encoder.layers.9.mlp.fc1.bias
   text_model.encoder.layers.9.mlp.fc2.weight
   text_model.encoder.layers.9.mlp.fc2.bias
   text_model.encoder.layers.9.layer_norm2.weight
   text_model.encoder.layers.9.layer_norm2.bias
   text_model.encoder.layers.10.self_attn.k_proj.weight
   text_model.encoder.layers.10.self_attn.k_proj.bias
   text_model.encoder.layers.10.self_attn.v_proj.weight
   text_model.encoder.layers.10.self_attn.v_proj.bias
   text_model.encoder.layers.10.self_attn.q_proj.weight
   text_model.encoder.layers.10.self_attn.q_proj.bias
   text_model.encoder.layers.10.self_attn.out_proj.weight
   text_model.encoder.layers.10.self_attn.out_proj.bias
   text_model.encoder.layers.10.layer_norm1.weight
   text_model.encoder.layers.10.layer_norm1.bias
   text_model.encoder.layers.10.mlp.fc1.weight
   text_model.encoder.layers.10.mlp.fc1.bias
   text_model.encoder.layers.10.mlp.fc2.weight
   text_model.encoder.layers.10.mlp.fc2.bias
   text_model.encoder.layers.10.layer_norm2.weight
   text_model.encoder.layers.10.layer_norm2.bias
   text_model.encoder.layers.11.self_attn.k_proj.weight
   text_model.encoder.layers.11.self_attn.k_proj.bias
   text_model.encoder.layers.11.self_attn.v_proj.weight
   text_model.encoder.layers.11.self_attn.v_proj.bias
   text_model.encoder.layers.11.self_attn.q_proj.weight
   text_model.encoder.layers.11.self_attn.q_proj.bias
   text_model.encoder.layers.11.self_attn.out_proj.weight
   text_model.encoder.layers.11.self_attn.out_proj.bias
   text_model.encoder.layers.11.layer_norm1.weight
   text_model.encoder.layers.11.layer_norm1.bias
   text_model.encoder.layers.11.mlp.fc1.weight
   text_model.encoder.layers.11.mlp.fc1.bias
   text_model.encoder.layers.11.mlp.fc2.weight
   text_model.encoder.layers.11.mlp.fc2.bias
   text_model.encoder.layers.11.layer_norm2.weight
   text_model.encoder.layers.11.layer_norm2.bias
   text_model.final_layer_norm.weight
   text_model.final_layer_norm.bias
   vision_model.embeddings.class_embedding
   vision_model.embeddings.position_ids
   vision_model.embeddings.patch_embedding.weight
   vision_model.embeddings.position_embedding.weight
   vision_model.pre_layrnorm.weight
   vision_model.pre_layrnorm.bias
   vision_model.encoder.layers.0.self_attn.k_proj.weight
   vision_model.encoder.layers.0.self_attn.k_proj.bias
   vision_model.encoder.layers.0.self_attn.v_proj.weight
   vision_model.encoder.layers.0.self_attn.v_proj.bias
   vision_model.encoder.layers.0.self_attn.q_proj.weight
   vision_model.encoder.layers.0.self_attn.q_proj.bias
   vision_model.encoder.layers.0.self_attn.out_proj.weight
   vision_model.encoder.layers.0.self_attn.out_proj.bias
   vision_model.encoder.layers.0.layer_norm1.weight
   vision_model.encoder.layers.0.layer_norm1.bias
   vision_model.encoder.layers.0.mlp.fc1.weight
   vision_model.encoder.layers.0.mlp.fc1.bias
   vision_model.encoder.layers.0.mlp.fc2.weight
   vision_model.encoder.layers.0.mlp.fc2.bias
   vision_model.encoder.layers.0.layer_norm2.weight
   vision_model.encoder.layers.0.layer_norm2.bias
   vision_model.encoder.layers.1.self_attn.k_proj.weight
   vision_model.encoder.layers.1.self_attn.k_proj.bias
   vision_model.encoder.layers.1.self_attn.v_proj.weight
   vision_model.encoder.layers.1.self_attn.v_proj.bias
   vision_model.encoder.layers.1.self_attn.q_proj.weight
   vision_model.encoder.layers.1.self_attn.q_proj.bias
   vision_model.encoder.layers.1.self_attn.out_proj.weight
   vision_model.encoder.layers.1.self_attn.out_proj.bias
   vision_model.encoder.layers.1.layer_norm1.weight
   vision_model.encoder.layers.1.layer_norm1.bias
   vision_model.encoder.layers.1.mlp.fc1.weight
   vision_model.encoder.layers.1.mlp.fc1.bias
   vision_model.encoder.layers.1.mlp.fc2.weight
   vision_model.encoder.layers.1.mlp.fc2.bias
   vision_model.encoder.layers.1.layer_norm2.weight
   vision_model.encoder.layers.1.layer_norm2.bias
   vision_model.encoder.layers.2.self_attn.k_proj.weight
   vision_model.encoder.layers.2.self_attn.k_proj.bias
   vision_model.encoder.layers.2.self_attn.v_proj.weight
   vision_model.encoder.layers.2.self_attn.v_proj.bias
   vision_model.encoder.layers.2.self_attn.q_proj.weight
   vision_model.encoder.layers.2.self_attn.q_proj.bias
   vision_model.encoder.layers.2.self_attn.out_proj.weight
   vision_model.encoder.layers.2.self_attn.out_proj.bias
   vision_model.encoder.layers.2.layer_norm1.weight
   vision_model.encoder.layers.2.layer_norm1.bias
   vision_model.encoder.layers.2.mlp.fc1.weight
   vision_model.encoder.layers.2.mlp.fc1.bias
   vision_model.encoder.layers.2.mlp.fc2.weight
   vision_model.encoder.layers.2.mlp.fc2.bias
   vision_model.encoder.layers.2.layer_norm2.weight
   vision_model.encoder.layers.2.layer_norm2.bias
   vision_model.encoder.layers.3.self_attn.k_proj.weight
   vision_model.encoder.layers.3.self_attn.k_proj.bias
   vision_model.encoder.layers.3.self_attn.v_proj.weight
   vision_model.encoder.layers.3.self_attn.v_proj.bias
   vision_model.encoder.layers.3.self_attn.q_proj.weight
   vision_model.encoder.layers.3.self_attn.q_proj.bias
   vision_model.encoder.layers.3.self_attn.out_proj.weight
   vision_model.encoder.layers.3.self_attn.out_proj.bias
   vision_model.encoder.layers.3.layer_norm1.weight
   vision_model.encoder.layers.3.layer_norm1.bias
   vision_model.encoder.layers.3.mlp.fc1.weight
   vision_model.encoder.layers.3.mlp.fc1.bias
   vision_model.encoder.layers.3.mlp.fc2.weight
   vision_model.encoder.layers.3.mlp.fc2.bias
   vision_model.encoder.layers.3.layer_norm2.weight
   vision_model.encoder.layers.3.layer_norm2.bias
   vision_model.encoder.layers.4.self_attn.k_proj.weight
   vision_model.encoder.layers.4.self_attn.k_proj.bias
   vision_model.encoder.layers.4.self_attn.v_proj.weight
   vision_model.encoder.layers.4.self_attn.v_proj.bias
   vision_model.encoder.layers.4.self_attn.q_proj.weight
   vision_model.encoder.layers.4.self_attn.q_proj.bias
   vision_model.encoder.layers.4.self_attn.out_proj.weight
   vision_model.encoder.layers.4.self_attn.out_proj.bias
   vision_model.encoder.layers.4.layer_norm1.weight
   vision_model.encoder.layers.4.layer_norm1.bias
   vision_model.encoder.layers.4.mlp.fc1.weight
   vision_model.encoder.layers.4.mlp.fc1.bias
   vision_model.encoder.layers.4.mlp.fc2.weight
   vision_model.encoder.layers.4.mlp.fc2.bias
   vision_model.encoder.layers.4.layer_norm2.weight
   vision_model.encoder.layers.4.layer_norm2.bias
   vision_model.encoder.layers.5.self_attn.k_proj.weight
   vision_model.encoder.layers.5.self_attn.k_proj.bias
   vision_model.encoder.layers.5.self_attn.v_proj.weight
   vision_model.encoder.layers.5.self_attn.v_proj.bias
   vision_model.encoder.layers.5.self_attn.q_proj.weight
   vision_model.encoder.layers.5.self_attn.q_proj.bias
   vision_model.encoder.layers.5.self_attn.out_proj.weight
   vision_model.encoder.layers.5.self_attn.out_proj.bias
   vision_model.encoder.layers.5.layer_norm1.weight
   vision_model.encoder.layers.5.layer_norm1.bias
   vision_model.encoder.layers.5.mlp.fc1.weight
   vision_model.encoder.layers.5.mlp.fc1.bias
   vision_model.encoder.layers.5.mlp.fc2.weight
   vision_model.encoder.layers.5.mlp.fc2.bias
   vision_model.encoder.layers.5.layer_norm2.weight
   vision_model.encoder.layers.5.layer_norm2.bias
   vision_model.encoder.layers.6.self_attn.k_proj.weight
   vision_model.encoder.layers.6.self_attn.k_proj.bias
   vision_model.encoder.layers.6.self_attn.v_proj.weight
   vision_model.encoder.layers.6.self_attn.v_proj.bias
   vision_model.encoder.layers.6.self_attn.q_proj.weight
   vision_model.encoder.layers.6.self_attn.q_proj.bias
   vision_model.encoder.layers.6.self_attn.out_proj.weight
   vision_model.encoder.layers.6.self_attn.out_proj.bias
   vision_model.encoder.layers.6.layer_norm1.weight
   vision_model.encoder.layers.6.layer_norm1.bias
   vision_model.encoder.layers.6.mlp.fc1.weight
   vision_model.encoder.layers.6.mlp.fc1.bias
   vision_model.encoder.layers.6.mlp.fc2.weight
   vision_model.encoder.layers.6.mlp.fc2.bias
   vision_model.encoder.layers.6.layer_norm2.weight
   vision_model.encoder.layers.6.layer_norm2.bias
   vision_model.encoder.layers.7.self_attn.k_proj.weight
   vision_model.encoder.layers.7.self_attn.k_proj.bias
   vision_model.encoder.layers.7.self_attn.v_proj.weight
   vision_model.encoder.layers.7.self_attn.v_proj.bias
   vision_model.encoder.layers.7.self_attn.q_proj.weight
   vision_model.encoder.layers.7.self_attn.q_proj.bias
   vision_model.encoder.layers.7.self_attn.out_proj.weight
   vision_model.encoder.layers.7.self_attn.out_proj.bias
   vision_model.encoder.layers.7.layer_norm1.weight
   vision_model.encoder.layers.7.layer_norm1.bias
   vision_model.encoder.layers.7.mlp.fc1.weight
   vision_model.encoder.layers.7.mlp.fc1.bias
   vision_model.encoder.layers.7.mlp.fc2.weight
   vision_model.encoder.layers.7.mlp.fc2.bias
   vision_model.encoder.layers.7.layer_norm2.weight
   vision_model.encoder.layers.7.layer_norm2.bias
   vision_model.encoder.layers.8.self_attn.k_proj.weight
   vision_model.encoder.layers.8.self_attn.k_proj.bias
   vision_model.encoder.layers.8.self_attn.v_proj.weight
   vision_model.encoder.layers.8.self_attn.v_proj.bias
   vision_model.encoder.layers.8.self_attn.q_proj.weight
   vision_model.encoder.layers.8.self_attn.q_proj.bias
   vision_model.encoder.layers.8.self_attn.out_proj.weight
   vision_model.encoder.layers.8.self_attn.out_proj.bias
   vision_model.encoder.layers.8.layer_norm1.weight
   vision_model.encoder.layers.8.layer_norm1.bias
   vision_model.encoder.layers.8.mlp.fc1.weight
   vision_model.encoder.layers.8.mlp.fc1.bias
   vision_model.encoder.layers.8.mlp.fc2.weight
   vision_model.encoder.layers.8.mlp.fc2.bias
   vision_model.encoder.layers.8.layer_norm2.weight
   vision_model.encoder.layers.8.layer_norm2.bias
   vision_model.encoder.layers.9.self_attn.k_proj.weight
   vision_model.encoder.layers.9.self_attn.k_proj.bias
   vision_model.encoder.layers.9.self_attn.v_proj.weight
   vision_model.encoder.layers.9.self_attn.v_proj.bias
   vision_model.encoder.layers.9.self_attn.q_proj.weight
   vision_model.encoder.layers.9.self_attn.q_proj.bias
   vision_model.encoder.layers.9.self_attn.out_proj.weight
   vision_model.encoder.layers.9.self_attn.out_proj.bias
   vision_model.encoder.layers.9.layer_norm1.weight
   vision_model.encoder.layers.9.layer_norm1.bias
   vision_model.encoder.layers.9.mlp.fc1.weight
   vision_model.encoder.layers.9.mlp.fc1.bias
   vision_model.encoder.layers.9.mlp.fc2.weight
   vision_model.encoder.layers.9.mlp.fc2.bias
   vision_model.encoder.layers.9.layer_norm2.weight
   vision_model.encoder.layers.9.layer_norm2.bias
   vision_model.encoder.layers.10.self_attn.k_proj.weight
   vision_model.encoder.layers.10.self_attn.k_proj.bias
   vision_model.encoder.layers.10.self_attn.v_proj.weight
   vision_model.encoder.layers.10.self_attn.v_proj.bias
   vision_model.encoder.layers.10.self_attn.q_proj.weight
   vision_model.encoder.layers.10.self_attn.q_proj.bias
   vision_model.encoder.layers.10.self_attn.out_proj.weight
   vision_model.encoder.layers.10.self_attn.out_proj.bias
   vision_model.encoder.layers.10.layer_norm1.weight
   vision_model.encoder.layers.10.layer_norm1.bias
   vision_model.encoder.layers.10.mlp.fc1.weight
   vision_model.encoder.layers.10.mlp.fc1.bias
   vision_model.encoder.layers.10.mlp.fc2.weight
   vision_model.encoder.layers.10.mlp.fc2.bias
   vision_model.encoder.layers.10.layer_norm2.weight
   vision_model.encoder.layers.10.layer_norm2.bias
   vision_model.encoder.layers.11.self_attn.k_proj.weight
   vision_model.encoder.layers.11.self_attn.k_proj.bias
   vision_model.encoder.layers.11.self_attn.v_proj.weight
   vision_model.encoder.layers.11.self_attn.v_proj.bias
   vision_model.encoder.layers.11.self_attn.q_proj.weight
   vision_model.encoder.layers.11.self_attn.q_proj.bias
   vision_model.encoder.layers.11.self_attn.out_proj.weight
   vision_model.encoder.layers.11.self_attn.out_proj.bias
   vision_model.encoder.layers.11.layer_norm1.weight
   vision_model.encoder.layers.11.layer_norm1.bias
   vision_model.encoder.layers.11.mlp.fc1.weight
   vision_model.encoder.layers.11.mlp.fc1.bias
   vision_model.encoder.layers.11.mlp.fc2.weight
   vision_model.encoder.layers.11.mlp.fc2.bias
   vision_model.encoder.layers.11.layer_norm2.weight
   vision_model.encoder.layers.11.layer_norm2.bias
   vision_model.post_layernorm.weight
   vision_model.post_layernorm.bias
   visual_projection.weight
   text_projection.weight
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 05:33:45,580:INFO: ***** Running test *****
2024-04-27 05:33:45,581:INFO:   Num examples = 1000
2024-04-27 05:33:45,581:INFO:   Batch size = 16
2024-04-27 05:33:45,581:INFO:   Num steps = 63
2024-04-27 05:33:45,581:INFO: ***** Running val *****
2024-04-27 05:33:45,581:INFO:   Num examples = 1000
2024-04-27 05:35:59,996:INFO: sim matrix size: 1000, 1000
2024-04-27 05:36:00,061:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 05:36:00,061:INFO: Text-to-Video:
2024-04-27 05:36:00,061:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 05:36:00,061:INFO: Video-to-Text:
2024-04-27 05:36:00,061:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 05:38:12,488:INFO: Effective parameters:
2024-04-27 05:38:12,488:INFO:   <<< batch_size: 16
2024-04-27 05:38:12,488:INFO:   <<< batch_size_val: 16
2024-04-27 05:38:12,488:INFO:   <<< cache_dir: 
2024-04-27 05:38:12,488:INFO:   <<< coef_lr: 0.001
2024-04-27 05:38:12,488:INFO:   <<< cross_model: cross-base
2024-04-27 05:38:12,488:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:38:12,488:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:38:12,488:INFO:   <<< datatype: msrvtt
2024-04-27 05:38:12,488:INFO:   <<< do_eval: True
2024-04-27 05:38:12,488:INFO:   <<< do_lower_case: False
2024-04-27 05:38:12,488:INFO:   <<< do_pretrain: False
2024-04-27 05:38:12,488:INFO:   <<< do_train: False
2024-04-27 05:38:12,488:INFO:   <<< epochs: 5
2024-04-27 05:38:12,488:INFO:   <<< eval_frame_order: 0
2024-04-27 05:38:12,488:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:38:12,488:INFO:   <<< feature_framerate: 1
2024-04-27 05:38:12,488:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:38:12,488:INFO:   <<< fp16: False
2024-04-27 05:38:12,488:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:38:12,489:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:38:12,489:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:38:12,489:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:38:12,489:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 05:38:12,489:INFO:   <<< linear_patch: 2d
2024-04-27 05:38:12,489:INFO:   <<< local_rank: 0
2024-04-27 05:38:12,489:INFO:   <<< loose_type: True
2024-04-27 05:38:12,489:INFO:   <<< lr: 0.0001
2024-04-27 05:38:12,489:INFO:   <<< lr_decay: 0.9
2024-04-27 05:38:12,489:INFO:   <<< margin: 0.1
2024-04-27 05:38:12,489:INFO:   <<< max_frames: 12
2024-04-27 05:38:12,489:INFO:   <<< max_words: 32
2024-04-27 05:38:12,489:INFO:   <<< n_display: 100
2024-04-27 05:38:12,489:INFO:   <<< n_gpu: 1
2024-04-27 05:38:12,489:INFO:   <<< n_pair: 1
2024-04-27 05:38:12,489:INFO:   <<< negative_weighting: 1
2024-04-27 05:38:12,489:INFO:   <<< num_thread_reader: 0
2024-04-27 05:38:12,489:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:38:12,489:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:38:12,489:INFO:   <<< rank: 0
2024-04-27 05:38:12,489:INFO:   <<< resume_model: None
2024-04-27 05:38:12,489:INFO:   <<< sampled_use_mil: False
2024-04-27 05:38:12,489:INFO:   <<< seed: 42
2024-04-27 05:38:12,489:INFO:   <<< sim_header: meanP
2024-04-27 05:38:12,489:INFO:   <<< slice_framepos: 2
2024-04-27 05:38:12,489:INFO:   <<< task_type: retrieval
2024-04-27 05:38:12,489:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:38:12,489:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:38:12,489:INFO:   <<< train_frame_order: 0
2024-04-27 05:38:12,489:INFO:   <<< use_mil: False
2024-04-27 05:38:12,489:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:38:12,489:INFO:   <<< video_dim: 1024
2024-04-27 05:38:12,489:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:38:12,489:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:38:12,489:INFO:   <<< world_size: 1
2024-04-27 05:38:12,489:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:38:13,091:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:38:13,091:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 05:38:13,241:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:38:13,242:WARNING: Test retrieval by loose type.
2024-04-27 05:38:13,242:WARNING: 	 embed_dim: 512
2024-04-27 05:38:13,242:WARNING: 	 image_resolution: 224
2024-04-27 05:38:13,242:WARNING: 	 vision_layers: 12
2024-04-27 05:38:13,242:WARNING: 	 vision_width: 768
2024-04-27 05:38:13,242:WARNING: 	 vision_patch_size: 32
2024-04-27 05:38:13,242:WARNING: 	 context_length: 77
2024-04-27 05:38:13,242:WARNING: 	 vocab_size: 49408
2024-04-27 05:38:13,242:WARNING: 	 transformer_width: 512
2024-04-27 05:38:13,242:WARNING: 	 transformer_heads: 8
2024-04-27 05:38:13,242:WARNING: 	 transformer_layers: 12
2024-04-27 05:38:13,242:WARNING: 		 linear_patch: 2d
2024-04-27 05:38:13,242:WARNING: 	 cut_top_layer: 0
2024-04-27 05:38:14,058:WARNING: 	 sim_header: meanP
2024-04-27 05:38:17,660:INFO: --------------------
2024-04-27 05:38:17,660:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 05:38:19,639:INFO: ***** Running test *****
2024-04-27 05:38:19,639:INFO:   Num examples = 1000
2024-04-27 05:38:19,639:INFO:   Batch size = 16
2024-04-27 05:38:19,639:INFO:   Num steps = 63
2024-04-27 05:38:19,639:INFO: ***** Running val *****
2024-04-27 05:38:19,639:INFO:   Num examples = 1000
2024-04-27 05:40:32,278:INFO: sim matrix size: 1000, 1000
2024-04-27 05:40:32,342:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 05:40:32,343:INFO: Text-to-Video:
2024-04-27 05:40:32,343:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 05:40:32,343:INFO: Video-to-Text:
2024-04-27 05:40:32,343:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 05:42:04,485:INFO: Effective parameters:
2024-04-27 05:42:04,485:INFO:   <<< batch_size: 16
2024-04-27 05:42:04,485:INFO:   <<< batch_size_val: 16
2024-04-27 05:42:04,485:INFO:   <<< cache_dir: 
2024-04-27 05:42:04,486:INFO:   <<< coef_lr: 0.001
2024-04-27 05:42:04,486:INFO:   <<< cross_model: cross-base
2024-04-27 05:42:04,486:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:42:04,486:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:42:04,486:INFO:   <<< datatype: msrvtt
2024-04-27 05:42:04,486:INFO:   <<< do_eval: True
2024-04-27 05:42:04,486:INFO:   <<< do_lower_case: False
2024-04-27 05:42:04,486:INFO:   <<< do_pretrain: False
2024-04-27 05:42:04,486:INFO:   <<< do_train: False
2024-04-27 05:42:04,486:INFO:   <<< epochs: 5
2024-04-27 05:42:04,486:INFO:   <<< eval_frame_order: 0
2024-04-27 05:42:04,486:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:42:04,486:INFO:   <<< feature_framerate: 1
2024-04-27 05:42:04,486:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:42:04,486:INFO:   <<< fp16: False
2024-04-27 05:42:04,486:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:42:04,486:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:42:04,486:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:42:04,486:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:42:04,486:INFO:   <<< init_model: None
2024-04-27 05:42:04,486:INFO:   <<< linear_patch: 2d
2024-04-27 05:42:04,486:INFO:   <<< local_rank: 0
2024-04-27 05:42:04,486:INFO:   <<< loose_type: True
2024-04-27 05:42:04,486:INFO:   <<< lr: 0.0001
2024-04-27 05:42:04,486:INFO:   <<< lr_decay: 0.9
2024-04-27 05:42:04,486:INFO:   <<< margin: 0.1
2024-04-27 05:42:04,486:INFO:   <<< max_frames: 12
2024-04-27 05:42:04,486:INFO:   <<< max_words: 32
2024-04-27 05:42:04,486:INFO:   <<< n_display: 100
2024-04-27 05:42:04,486:INFO:   <<< n_gpu: 1
2024-04-27 05:42:04,486:INFO:   <<< n_pair: 1
2024-04-27 05:42:04,486:INFO:   <<< negative_weighting: 1
2024-04-27 05:42:04,486:INFO:   <<< num_thread_reader: 0
2024-04-27 05:42:04,486:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:42:04,486:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:42:04,486:INFO:   <<< rank: 0
2024-04-27 05:42:04,486:INFO:   <<< resume_model: None
2024-04-27 05:42:04,486:INFO:   <<< sampled_use_mil: False
2024-04-27 05:42:04,486:INFO:   <<< seed: 42
2024-04-27 05:42:04,486:INFO:   <<< sim_header: meanP
2024-04-27 05:42:04,486:INFO:   <<< slice_framepos: 2
2024-04-27 05:42:04,486:INFO:   <<< task_type: retrieval
2024-04-27 05:42:04,486:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:42:04,486:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:42:04,486:INFO:   <<< train_frame_order: 0
2024-04-27 05:42:04,486:INFO:   <<< use_mil: False
2024-04-27 05:42:04,486:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:42:04,486:INFO:   <<< video_dim: 1024
2024-04-27 05:42:04,486:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:42:04,486:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:42:04,486:INFO:   <<< world_size: 1
2024-04-27 05:42:04,486:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:42:05,065:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:42:05,065:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 05:42:05,217:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:42:05,217:WARNING: Test retrieval by loose type.
2024-04-27 05:42:05,217:WARNING: 	 embed_dim: 512
2024-04-27 05:42:05,218:WARNING: 	 image_resolution: 224
2024-04-27 05:42:05,218:WARNING: 	 vision_layers: 12
2024-04-27 05:42:05,218:WARNING: 	 vision_width: 768
2024-04-27 05:42:05,218:WARNING: 	 vision_patch_size: 32
2024-04-27 05:42:05,218:WARNING: 	 context_length: 77
2024-04-27 05:42:05,218:WARNING: 	 vocab_size: 49408
2024-04-27 05:42:05,218:WARNING: 	 transformer_width: 512
2024-04-27 05:42:05,218:WARNING: 	 transformer_heads: 8
2024-04-27 05:42:05,218:WARNING: 	 transformer_layers: 12
2024-04-27 05:42:05,218:WARNING: 		 linear_patch: 2d
2024-04-27 05:42:05,218:WARNING: 	 cut_top_layer: 0
2024-04-27 05:42:06,037:WARNING: 	 sim_header: meanP
2024-04-27 05:42:09,634:INFO: --------------------
2024-04-27 05:42:09,634:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 05:42:11,621:INFO: ***** Running test *****
2024-04-27 05:42:11,621:INFO:   Num examples = 1000
2024-04-27 05:42:11,621:INFO:   Batch size = 16
2024-04-27 05:42:11,621:INFO:   Num steps = 63
2024-04-27 05:42:11,621:INFO: ***** Running val *****
2024-04-27 05:42:11,621:INFO:   Num examples = 1000
2024-04-27 05:44:33,026:INFO: sim matrix size: 1000, 1000
2024-04-27 05:44:33,092:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 05:44:33,092:INFO: Text-to-Video:
2024-04-27 05:44:33,092:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 05:44:33,092:INFO: Video-to-Text:
2024-04-27 05:44:33,092:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 05:55:22,403:INFO: Effective parameters:
2024-04-27 05:55:22,404:INFO:   <<< batch_size: 16
2024-04-27 05:55:22,404:INFO:   <<< batch_size_val: 16
2024-04-27 05:55:22,404:INFO:   <<< cache_dir: 
2024-04-27 05:55:22,404:INFO:   <<< coef_lr: 0.001
2024-04-27 05:55:22,404:INFO:   <<< cross_model: cross-base
2024-04-27 05:55:22,404:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:55:22,404:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:55:22,404:INFO:   <<< datatype: msrvtt
2024-04-27 05:55:22,404:INFO:   <<< do_eval: True
2024-04-27 05:55:22,404:INFO:   <<< do_lower_case: False
2024-04-27 05:55:22,404:INFO:   <<< do_pretrain: False
2024-04-27 05:55:22,404:INFO:   <<< do_train: False
2024-04-27 05:55:22,404:INFO:   <<< epochs: 5
2024-04-27 05:55:22,404:INFO:   <<< eval_frame_order: 0
2024-04-27 05:55:22,404:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:55:22,405:INFO:   <<< feature_framerate: 1
2024-04-27 05:55:22,405:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:55:22,405:INFO:   <<< fp16: False
2024-04-27 05:55:22,405:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:55:22,405:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:55:22,405:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:55:22,405:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:55:22,405:INFO:   <<< init_model: None
2024-04-27 05:55:22,405:INFO:   <<< linear_patch: 2d
2024-04-27 05:55:22,405:INFO:   <<< local_rank: 0
2024-04-27 05:55:22,405:INFO:   <<< loose_type: True
2024-04-27 05:55:22,405:INFO:   <<< lr: 0.0001
2024-04-27 05:55:22,405:INFO:   <<< lr_decay: 0.9
2024-04-27 05:55:22,405:INFO:   <<< margin: 0.1
2024-04-27 05:55:22,405:INFO:   <<< max_frames: 12
2024-04-27 05:55:22,405:INFO:   <<< max_words: 32
2024-04-27 05:55:22,405:INFO:   <<< n_display: 100
2024-04-27 05:55:22,406:INFO:   <<< n_gpu: 1
2024-04-27 05:55:22,406:INFO:   <<< n_pair: 1
2024-04-27 05:55:22,406:INFO:   <<< negative_weighting: 1
2024-04-27 05:55:22,406:INFO:   <<< num_thread_reader: 0
2024-04-27 05:55:22,406:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:55:22,406:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:55:22,406:INFO:   <<< rank: 0
2024-04-27 05:55:22,406:INFO:   <<< resume_model: None
2024-04-27 05:55:22,406:INFO:   <<< sampled_use_mil: False
2024-04-27 05:55:22,406:INFO:   <<< seed: 42
2024-04-27 05:55:22,406:INFO:   <<< sim_header: meanP
2024-04-27 05:55:22,406:INFO:   <<< slice_framepos: 2
2024-04-27 05:55:22,406:INFO:   <<< task_type: retrieval
2024-04-27 05:55:22,406:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:55:22,406:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:55:22,406:INFO:   <<< train_frame_order: 0
2024-04-27 05:55:22,407:INFO:   <<< use_mil: False
2024-04-27 05:55:22,407:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:55:22,407:INFO:   <<< video_dim: 1024
2024-04-27 05:55:22,407:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:55:22,407:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:55:22,407:INFO:   <<< world_size: 1
2024-04-27 05:55:22,407:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:55:23,029:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:55:23,032:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 05:55:58,615:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:56:16,703:WARNING: Test retrieval by loose type.
2024-04-27 05:57:01,470:WARNING: 	 embed_dim: 512
2024-04-27 05:57:01,470:WARNING: 	 image_resolution: 224
2024-04-27 05:57:01,470:WARNING: 	 vision_layers: 12
2024-04-27 05:57:01,470:WARNING: 	 vision_width: 768
2024-04-27 05:57:01,471:WARNING: 	 vision_patch_size: 32
2024-04-27 05:57:01,471:WARNING: 	 context_length: 77
2024-04-27 05:57:01,471:WARNING: 	 vocab_size: 49408
2024-04-27 05:57:01,471:WARNING: 	 transformer_width: 512
2024-04-27 05:57:01,471:WARNING: 	 transformer_heads: 8
2024-04-27 05:57:01,472:WARNING: 	 transformer_layers: 12
2024-04-27 05:57:01,472:WARNING: 		 linear_patch: 2d
2024-04-27 05:57:01,472:WARNING: 	 cut_top_layer: 0
2024-04-27 05:57:02,317:WARNING: 	 sim_header: meanP
2024-04-27 05:59:12,040:INFO: Effective parameters:
2024-04-27 05:59:12,041:INFO:   <<< batch_size: 16
2024-04-27 05:59:12,041:INFO:   <<< batch_size_val: 16
2024-04-27 05:59:12,041:INFO:   <<< cache_dir: 
2024-04-27 05:59:12,041:INFO:   <<< coef_lr: 0.001
2024-04-27 05:59:12,041:INFO:   <<< cross_model: cross-base
2024-04-27 05:59:12,041:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 05:59:12,041:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 05:59:12,041:INFO:   <<< datatype: msrvtt
2024-04-27 05:59:12,041:INFO:   <<< do_eval: True
2024-04-27 05:59:12,042:INFO:   <<< do_lower_case: False
2024-04-27 05:59:12,042:INFO:   <<< do_pretrain: False
2024-04-27 05:59:12,042:INFO:   <<< do_train: False
2024-04-27 05:59:12,042:INFO:   <<< epochs: 5
2024-04-27 05:59:12,042:INFO:   <<< eval_frame_order: 0
2024-04-27 05:59:12,042:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 05:59:12,042:INFO:   <<< feature_framerate: 1
2024-04-27 05:59:12,042:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 05:59:12,042:INFO:   <<< fp16: False
2024-04-27 05:59:12,042:INFO:   <<< fp16_opt_level: O1
2024-04-27 05:59:12,042:INFO:   <<< freeze_layer_num: 0
2024-04-27 05:59:12,042:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 05:59:12,042:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 05:59:12,042:INFO:   <<< init_model: None
2024-04-27 05:59:12,042:INFO:   <<< linear_patch: 2d
2024-04-27 05:59:12,043:INFO:   <<< local_rank: 0
2024-04-27 05:59:12,043:INFO:   <<< loose_type: True
2024-04-27 05:59:12,043:INFO:   <<< lr: 0.0001
2024-04-27 05:59:12,043:INFO:   <<< lr_decay: 0.9
2024-04-27 05:59:12,043:INFO:   <<< margin: 0.1
2024-04-27 05:59:12,043:INFO:   <<< max_frames: 12
2024-04-27 05:59:12,043:INFO:   <<< max_words: 32
2024-04-27 05:59:12,043:INFO:   <<< n_display: 100
2024-04-27 05:59:12,043:INFO:   <<< n_gpu: 1
2024-04-27 05:59:12,043:INFO:   <<< n_pair: 1
2024-04-27 05:59:12,043:INFO:   <<< negative_weighting: 1
2024-04-27 05:59:12,043:INFO:   <<< num_thread_reader: 0
2024-04-27 05:59:12,043:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 05:59:12,043:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 05:59:12,044:INFO:   <<< rank: 0
2024-04-27 05:59:12,044:INFO:   <<< resume_model: None
2024-04-27 05:59:12,044:INFO:   <<< sampled_use_mil: False
2024-04-27 05:59:12,044:INFO:   <<< seed: 42
2024-04-27 05:59:12,044:INFO:   <<< sim_header: meanP
2024-04-27 05:59:12,044:INFO:   <<< slice_framepos: 2
2024-04-27 05:59:12,044:INFO:   <<< task_type: retrieval
2024-04-27 05:59:12,044:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 05:59:12,044:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 05:59:12,044:INFO:   <<< train_frame_order: 0
2024-04-27 05:59:12,044:INFO:   <<< use_mil: False
2024-04-27 05:59:12,044:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 05:59:12,044:INFO:   <<< video_dim: 1024
2024-04-27 05:59:12,044:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 05:59:12,045:INFO:   <<< warmup_proportion: 0.1
2024-04-27 05:59:12,045:INFO:   <<< world_size: 1
2024-04-27 05:59:12,045:INFO: device: cuda:0 n_gpu: 1
2024-04-27 05:59:12,654:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 05:59:12,657:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 05:59:25,424:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 05:59:25,424:WARNING: Test retrieval by loose type.
2024-04-27 05:59:25,426:WARNING: 	 embed_dim: 512
2024-04-27 05:59:25,426:WARNING: 	 image_resolution: 224
2024-04-27 05:59:25,426:WARNING: 	 vision_layers: 12
2024-04-27 05:59:25,426:WARNING: 	 vision_width: 768
2024-04-27 05:59:25,427:WARNING: 	 vision_patch_size: 32
2024-04-27 05:59:25,427:WARNING: 	 context_length: 77
2024-04-27 05:59:25,427:WARNING: 	 vocab_size: 49408
2024-04-27 05:59:25,427:WARNING: 	 transformer_width: 512
2024-04-27 05:59:25,427:WARNING: 	 transformer_heads: 8
2024-04-27 05:59:25,427:WARNING: 	 transformer_layers: 12
2024-04-27 05:59:25,428:WARNING: 		 linear_patch: 2d
2024-04-27 05:59:25,428:WARNING: 	 cut_top_layer: 0
2024-04-27 05:59:26,278:WARNING: 	 sim_header: meanP
2024-04-27 06:01:16,374:INFO: Effective parameters:
2024-04-27 06:01:16,374:INFO:   <<< batch_size: 16
2024-04-27 06:01:16,374:INFO:   <<< batch_size_val: 16
2024-04-27 06:01:16,374:INFO:   <<< cache_dir: 
2024-04-27 06:01:16,374:INFO:   <<< coef_lr: 0.001
2024-04-27 06:01:16,374:INFO:   <<< cross_model: cross-base
2024-04-27 06:01:16,374:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:01:16,374:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:01:16,374:INFO:   <<< datatype: msrvtt
2024-04-27 06:01:16,374:INFO:   <<< do_eval: True
2024-04-27 06:01:16,374:INFO:   <<< do_lower_case: False
2024-04-27 06:01:16,374:INFO:   <<< do_pretrain: False
2024-04-27 06:01:16,375:INFO:   <<< do_train: False
2024-04-27 06:01:16,375:INFO:   <<< epochs: 5
2024-04-27 06:01:16,375:INFO:   <<< eval_frame_order: 0
2024-04-27 06:01:16,375:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:01:16,375:INFO:   <<< feature_framerate: 1
2024-04-27 06:01:16,375:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:01:16,375:INFO:   <<< fp16: False
2024-04-27 06:01:16,375:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:01:16,375:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:01:16,375:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:01:16,375:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:01:16,375:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:01:16,375:INFO:   <<< linear_patch: 2d
2024-04-27 06:01:16,375:INFO:   <<< local_rank: 0
2024-04-27 06:01:16,375:INFO:   <<< loose_type: True
2024-04-27 06:01:16,375:INFO:   <<< lr: 0.0001
2024-04-27 06:01:16,375:INFO:   <<< lr_decay: 0.9
2024-04-27 06:01:16,376:INFO:   <<< margin: 0.1
2024-04-27 06:01:16,376:INFO:   <<< max_frames: 12
2024-04-27 06:01:16,376:INFO:   <<< max_words: 32
2024-04-27 06:01:16,376:INFO:   <<< n_display: 100
2024-04-27 06:01:16,376:INFO:   <<< n_gpu: 1
2024-04-27 06:01:16,376:INFO:   <<< n_pair: 1
2024-04-27 06:01:16,376:INFO:   <<< negative_weighting: 1
2024-04-27 06:01:16,376:INFO:   <<< num_thread_reader: 0
2024-04-27 06:01:16,376:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:01:16,376:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:01:16,376:INFO:   <<< rank: 0
2024-04-27 06:01:16,376:INFO:   <<< resume_model: None
2024-04-27 06:01:16,376:INFO:   <<< sampled_use_mil: False
2024-04-27 06:01:16,376:INFO:   <<< seed: 42
2024-04-27 06:01:16,376:INFO:   <<< sim_header: meanP
2024-04-27 06:01:16,376:INFO:   <<< slice_framepos: 2
2024-04-27 06:01:16,376:INFO:   <<< task_type: retrieval
2024-04-27 06:01:16,376:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:01:16,377:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:01:16,377:INFO:   <<< train_frame_order: 0
2024-04-27 06:01:16,377:INFO:   <<< use_mil: False
2024-04-27 06:01:16,377:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:01:16,377:INFO:   <<< video_dim: 1024
2024-04-27 06:01:16,377:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:01:16,377:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:01:16,377:INFO:   <<< world_size: 1
2024-04-27 06:01:16,377:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:02:06,600:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:02:06,603:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:02:19,825:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:02:19,826:WARNING: Test retrieval by loose type.
2024-04-27 06:02:19,827:WARNING: 	 embed_dim: 512
2024-04-27 06:02:19,827:WARNING: 	 image_resolution: 224
2024-04-27 06:02:19,828:WARNING: 	 vision_layers: 12
2024-04-27 06:02:19,828:WARNING: 	 vision_width: 768
2024-04-27 06:02:19,828:WARNING: 	 vision_patch_size: 32
2024-04-27 06:02:19,828:WARNING: 	 context_length: 77
2024-04-27 06:02:19,828:WARNING: 	 vocab_size: 49408
2024-04-27 06:02:19,828:WARNING: 	 transformer_width: 512
2024-04-27 06:02:19,828:WARNING: 	 transformer_heads: 8
2024-04-27 06:02:19,829:WARNING: 	 transformer_layers: 12
2024-04-27 06:02:19,829:WARNING: 		 linear_patch: 2d
2024-04-27 06:02:19,829:WARNING: 	 cut_top_layer: 0
2024-04-27 06:02:20,657:WARNING: 	 sim_header: meanP
2024-04-27 06:03:36,568:INFO: --------------------
2024-04-27 06:03:36,568:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:04:22,445:INFO: ***** Running test *****
2024-04-27 06:04:22,446:INFO:   Num examples = 1000
2024-04-27 06:04:22,446:INFO:   Batch size = 16
2024-04-27 06:04:22,446:INFO:   Num steps = 63
2024-04-27 06:04:22,446:INFO: ***** Running val *****
2024-04-27 06:04:22,446:INFO:   Num examples = 1000
2024-04-27 06:06:51,756:INFO: sim matrix size: 1000, 1000
2024-04-27 06:06:51,825:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 06:06:51,825:INFO: Text-to-Video:
2024-04-27 06:06:51,825:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 06:06:51,825:INFO: Video-to-Text:
2024-04-27 06:06:51,825:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 06:07:34,321:INFO: Effective parameters:
2024-04-27 06:07:34,321:INFO:   <<< batch_size: 16
2024-04-27 06:07:34,321:INFO:   <<< batch_size_val: 16
2024-04-27 06:07:34,322:INFO:   <<< cache_dir: 
2024-04-27 06:07:34,322:INFO:   <<< coef_lr: 0.001
2024-04-27 06:07:34,322:INFO:   <<< cross_model: cross-base
2024-04-27 06:07:34,322:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:07:34,322:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:07:34,322:INFO:   <<< datatype: msrvtt
2024-04-27 06:07:34,322:INFO:   <<< do_eval: True
2024-04-27 06:07:34,322:INFO:   <<< do_lower_case: False
2024-04-27 06:07:34,322:INFO:   <<< do_pretrain: False
2024-04-27 06:07:34,322:INFO:   <<< do_train: False
2024-04-27 06:07:34,322:INFO:   <<< epochs: 5
2024-04-27 06:07:34,322:INFO:   <<< eval_frame_order: 0
2024-04-27 06:07:34,322:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:07:34,322:INFO:   <<< feature_framerate: 1
2024-04-27 06:07:34,323:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:07:34,323:INFO:   <<< fp16: False
2024-04-27 06:07:34,323:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:07:34,323:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:07:34,323:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:07:34,323:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:07:34,323:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:07:34,323:INFO:   <<< linear_patch: 2d
2024-04-27 06:07:34,323:INFO:   <<< local_rank: 0
2024-04-27 06:07:34,323:INFO:   <<< loose_type: True
2024-04-27 06:07:34,323:INFO:   <<< lr: 0.0001
2024-04-27 06:07:34,323:INFO:   <<< lr_decay: 0.9
2024-04-27 06:07:34,323:INFO:   <<< margin: 0.1
2024-04-27 06:07:34,323:INFO:   <<< max_frames: 12
2024-04-27 06:07:34,323:INFO:   <<< max_words: 32
2024-04-27 06:07:34,323:INFO:   <<< n_display: 100
2024-04-27 06:07:34,324:INFO:   <<< n_gpu: 1
2024-04-27 06:07:34,324:INFO:   <<< n_pair: 1
2024-04-27 06:07:34,324:INFO:   <<< negative_weighting: 1
2024-04-27 06:07:34,324:INFO:   <<< num_thread_reader: 0
2024-04-27 06:07:34,324:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:07:34,324:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:07:34,324:INFO:   <<< rank: 0
2024-04-27 06:07:34,324:INFO:   <<< resume_model: None
2024-04-27 06:07:34,324:INFO:   <<< sampled_use_mil: False
2024-04-27 06:07:34,324:INFO:   <<< seed: 42
2024-04-27 06:07:34,324:INFO:   <<< sim_header: meanP
2024-04-27 06:07:34,324:INFO:   <<< slice_framepos: 2
2024-04-27 06:07:34,324:INFO:   <<< task_type: retrieval
2024-04-27 06:07:34,324:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:07:34,324:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:07:34,325:INFO:   <<< train_frame_order: 0
2024-04-27 06:07:34,325:INFO:   <<< use_mil: False
2024-04-27 06:07:34,325:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:07:34,325:INFO:   <<< video_dim: 1024
2024-04-27 06:07:34,325:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:07:34,325:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:07:34,325:INFO:   <<< world_size: 1
2024-04-27 06:07:34,325:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:11:20,414:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:11:20,426:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:17:16,833:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:17:20,302:WARNING: Test retrieval by loose type.
2024-04-27 06:18:00,110:WARNING: 	 embed_dim: 512
2024-04-27 06:18:00,110:WARNING: 	 image_resolution: 224
2024-04-27 06:18:00,110:WARNING: 	 vision_layers: 12
2024-04-27 06:18:00,110:WARNING: 	 vision_width: 768
2024-04-27 06:18:00,111:WARNING: 	 vision_patch_size: 32
2024-04-27 06:18:00,111:WARNING: 	 context_length: 77
2024-04-27 06:18:00,111:WARNING: 	 vocab_size: 49408
2024-04-27 06:18:00,111:WARNING: 	 transformer_width: 512
2024-04-27 06:18:00,112:WARNING: 	 transformer_heads: 8
2024-04-27 06:18:00,112:WARNING: 	 transformer_layers: 12
2024-04-27 06:18:00,112:WARNING: 		 linear_patch: 2d
2024-04-27 06:18:00,112:WARNING: 	 cut_top_layer: 0
2024-04-27 06:18:01,012:WARNING: 	 sim_header: meanP
2024-04-27 06:23:05,525:INFO: Effective parameters:
2024-04-27 06:23:05,525:INFO:   <<< batch_size: 16
2024-04-27 06:23:05,525:INFO:   <<< batch_size_val: 16
2024-04-27 06:23:05,525:INFO:   <<< cache_dir: 
2024-04-27 06:23:05,525:INFO:   <<< coef_lr: 0.001
2024-04-27 06:23:05,525:INFO:   <<< cross_model: cross-base
2024-04-27 06:23:05,526:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:23:05,526:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:23:05,526:INFO:   <<< datatype: msrvtt
2024-04-27 06:23:05,526:INFO:   <<< do_eval: True
2024-04-27 06:23:05,526:INFO:   <<< do_lower_case: False
2024-04-27 06:23:05,526:INFO:   <<< do_pretrain: False
2024-04-27 06:23:05,526:INFO:   <<< do_train: False
2024-04-27 06:23:05,526:INFO:   <<< epochs: 5
2024-04-27 06:23:05,526:INFO:   <<< eval_frame_order: 0
2024-04-27 06:23:05,526:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:23:05,526:INFO:   <<< feature_framerate: 1
2024-04-27 06:23:05,526:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:23:05,526:INFO:   <<< fp16: False
2024-04-27 06:23:05,526:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:23:05,526:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:23:05,526:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:23:05,526:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:23:05,527:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:23:05,527:INFO:   <<< linear_patch: 2d
2024-04-27 06:23:05,527:INFO:   <<< local_rank: 0
2024-04-27 06:23:05,527:INFO:   <<< loose_type: True
2024-04-27 06:23:05,527:INFO:   <<< lr: 0.0001
2024-04-27 06:23:05,527:INFO:   <<< lr_decay: 0.9
2024-04-27 06:23:05,527:INFO:   <<< margin: 0.1
2024-04-27 06:23:05,527:INFO:   <<< max_frames: 12
2024-04-27 06:23:05,527:INFO:   <<< max_words: 32
2024-04-27 06:23:05,527:INFO:   <<< n_display: 100
2024-04-27 06:23:05,527:INFO:   <<< n_gpu: 1
2024-04-27 06:23:05,527:INFO:   <<< n_pair: 1
2024-04-27 06:23:05,527:INFO:   <<< negative_weighting: 1
2024-04-27 06:23:05,527:INFO:   <<< num_thread_reader: 0
2024-04-27 06:23:05,527:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:23:05,527:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:23:05,527:INFO:   <<< rank: 0
2024-04-27 06:23:05,528:INFO:   <<< resume_model: None
2024-04-27 06:23:05,528:INFO:   <<< sampled_use_mil: False
2024-04-27 06:23:05,528:INFO:   <<< seed: 42
2024-04-27 06:23:05,528:INFO:   <<< sim_header: meanP
2024-04-27 06:23:05,528:INFO:   <<< slice_framepos: 2
2024-04-27 06:23:05,528:INFO:   <<< task_type: retrieval
2024-04-27 06:23:05,528:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:23:05,528:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:23:05,528:INFO:   <<< train_frame_order: 0
2024-04-27 06:23:05,528:INFO:   <<< use_mil: False
2024-04-27 06:23:05,528:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:23:05,528:INFO:   <<< video_dim: 1024
2024-04-27 06:23:05,528:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:23:05,528:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:23:05,528:INFO:   <<< world_size: 1
2024-04-27 06:23:05,529:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:23:10,352:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:23:10,366:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:23:12,342:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:24:10,422:WARNING: Test retrieval by loose type.
2024-04-27 06:24:23,815:WARNING: 	 embed_dim: 512
2024-04-27 06:24:23,816:WARNING: 	 image_resolution: 224
2024-04-27 06:24:23,816:WARNING: 	 vision_layers: 12
2024-04-27 06:24:23,817:WARNING: 	 vision_width: 768
2024-04-27 06:24:23,817:WARNING: 	 vision_patch_size: 32
2024-04-27 06:24:23,817:WARNING: 	 context_length: 77
2024-04-27 06:24:23,818:WARNING: 	 vocab_size: 49408
2024-04-27 06:24:23,818:WARNING: 	 transformer_width: 512
2024-04-27 06:24:23,818:WARNING: 	 transformer_heads: 8
2024-04-27 06:24:23,819:WARNING: 	 transformer_layers: 12
2024-04-27 06:24:23,819:WARNING: 		 linear_patch: 2d
2024-04-27 06:24:23,820:WARNING: 	 cut_top_layer: 0
2024-04-27 06:24:24,688:WARNING: 	 sim_header: meanP
2024-04-27 06:24:34,549:INFO: --------------------
2024-04-27 06:24:34,550:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:25:29,893:INFO: Effective parameters:
2024-04-27 06:25:29,893:INFO:   <<< batch_size: 16
2024-04-27 06:25:29,893:INFO:   <<< batch_size_val: 16
2024-04-27 06:25:29,893:INFO:   <<< cache_dir: 
2024-04-27 06:25:29,893:INFO:   <<< coef_lr: 0.001
2024-04-27 06:25:29,893:INFO:   <<< cross_model: cross-base
2024-04-27 06:25:29,894:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:25:29,894:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:25:29,894:INFO:   <<< datatype: msrvtt
2024-04-27 06:25:29,894:INFO:   <<< do_eval: True
2024-04-27 06:25:29,894:INFO:   <<< do_lower_case: False
2024-04-27 06:25:29,894:INFO:   <<< do_pretrain: False
2024-04-27 06:25:29,894:INFO:   <<< do_train: False
2024-04-27 06:25:29,894:INFO:   <<< epochs: 5
2024-04-27 06:25:29,894:INFO:   <<< eval_frame_order: 0
2024-04-27 06:25:29,894:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:25:29,894:INFO:   <<< feature_framerate: 1
2024-04-27 06:25:29,894:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:25:29,894:INFO:   <<< fp16: False
2024-04-27 06:25:29,894:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:25:29,894:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:25:29,894:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:25:29,894:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:25:29,895:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:25:29,895:INFO:   <<< linear_patch: 2d
2024-04-27 06:25:29,895:INFO:   <<< local_rank: 0
2024-04-27 06:25:29,895:INFO:   <<< loose_type: True
2024-04-27 06:25:29,895:INFO:   <<< lr: 0.0001
2024-04-27 06:25:29,895:INFO:   <<< lr_decay: 0.9
2024-04-27 06:25:29,895:INFO:   <<< margin: 0.1
2024-04-27 06:25:29,895:INFO:   <<< max_frames: 12
2024-04-27 06:25:29,895:INFO:   <<< max_words: 32
2024-04-27 06:25:29,895:INFO:   <<< n_display: 100
2024-04-27 06:25:29,895:INFO:   <<< n_gpu: 1
2024-04-27 06:25:29,895:INFO:   <<< n_pair: 1
2024-04-27 06:25:29,895:INFO:   <<< negative_weighting: 1
2024-04-27 06:25:29,895:INFO:   <<< num_thread_reader: 0
2024-04-27 06:25:29,895:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:25:29,895:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:25:29,895:INFO:   <<< rank: 0
2024-04-27 06:25:29,896:INFO:   <<< resume_model: None
2024-04-27 06:25:29,896:INFO:   <<< sampled_use_mil: False
2024-04-27 06:25:29,896:INFO:   <<< seed: 42
2024-04-27 06:25:29,896:INFO:   <<< sim_header: meanP
2024-04-27 06:25:29,896:INFO:   <<< slice_framepos: 2
2024-04-27 06:25:29,896:INFO:   <<< task_type: retrieval
2024-04-27 06:25:29,896:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:25:29,896:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:25:29,896:INFO:   <<< train_frame_order: 0
2024-04-27 06:25:29,896:INFO:   <<< use_mil: False
2024-04-27 06:25:29,896:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:25:29,896:INFO:   <<< video_dim: 1024
2024-04-27 06:25:29,896:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:25:29,896:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:25:29,896:INFO:   <<< world_size: 1
2024-04-27 06:25:29,896:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:25:36,524:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:25:36,539:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:25:40,193:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:25:40,194:WARNING: Test retrieval by loose type.
2024-04-27 06:25:40,195:WARNING: 	 embed_dim: 512
2024-04-27 06:25:40,195:WARNING: 	 image_resolution: 224
2024-04-27 06:25:40,195:WARNING: 	 vision_layers: 12
2024-04-27 06:25:40,196:WARNING: 	 vision_width: 768
2024-04-27 06:25:40,196:WARNING: 	 vision_patch_size: 32
2024-04-27 06:25:40,196:WARNING: 	 context_length: 77
2024-04-27 06:25:40,196:WARNING: 	 vocab_size: 49408
2024-04-27 06:25:40,197:WARNING: 	 transformer_width: 512
2024-04-27 06:25:40,197:WARNING: 	 transformer_heads: 8
2024-04-27 06:25:40,197:WARNING: 	 transformer_layers: 12
2024-04-27 06:25:40,198:WARNING: 		 linear_patch: 2d
2024-04-27 06:25:40,198:WARNING: 	 cut_top_layer: 0
2024-04-27 06:25:41,075:WARNING: 	 sim_header: meanP
2024-04-27 06:25:49,578:INFO: --------------------
2024-04-27 06:25:49,579:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:26:59,758:INFO: Effective parameters:
2024-04-27 06:26:59,758:INFO:   <<< batch_size: 16
2024-04-27 06:26:59,758:INFO:   <<< batch_size_val: 16
2024-04-27 06:26:59,758:INFO:   <<< cache_dir: 
2024-04-27 06:26:59,758:INFO:   <<< coef_lr: 0.001
2024-04-27 06:26:59,758:INFO:   <<< cross_model: cross-base
2024-04-27 06:26:59,758:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:26:59,758:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:26:59,758:INFO:   <<< datatype: msrvtt
2024-04-27 06:26:59,758:INFO:   <<< do_eval: True
2024-04-27 06:26:59,758:INFO:   <<< do_lower_case: False
2024-04-27 06:26:59,758:INFO:   <<< do_pretrain: False
2024-04-27 06:26:59,758:INFO:   <<< do_train: False
2024-04-27 06:26:59,759:INFO:   <<< epochs: 5
2024-04-27 06:26:59,759:INFO:   <<< eval_frame_order: 0
2024-04-27 06:26:59,759:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:26:59,759:INFO:   <<< feature_framerate: 1
2024-04-27 06:26:59,759:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:26:59,759:INFO:   <<< fp16: False
2024-04-27 06:26:59,759:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:26:59,759:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:26:59,759:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:26:59,759:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:26:59,759:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:26:59,759:INFO:   <<< linear_patch: 2d
2024-04-27 06:26:59,759:INFO:   <<< local_rank: 0
2024-04-27 06:26:59,759:INFO:   <<< loose_type: True
2024-04-27 06:26:59,759:INFO:   <<< lr: 0.0001
2024-04-27 06:26:59,759:INFO:   <<< lr_decay: 0.9
2024-04-27 06:26:59,759:INFO:   <<< margin: 0.1
2024-04-27 06:26:59,760:INFO:   <<< max_frames: 12
2024-04-27 06:26:59,760:INFO:   <<< max_words: 32
2024-04-27 06:26:59,760:INFO:   <<< n_display: 100
2024-04-27 06:26:59,760:INFO:   <<< n_gpu: 1
2024-04-27 06:26:59,760:INFO:   <<< n_pair: 1
2024-04-27 06:26:59,760:INFO:   <<< negative_weighting: 1
2024-04-27 06:26:59,760:INFO:   <<< num_thread_reader: 0
2024-04-27 06:26:59,760:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:26:59,760:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:26:59,760:INFO:   <<< rank: 0
2024-04-27 06:26:59,760:INFO:   <<< resume_model: None
2024-04-27 06:26:59,760:INFO:   <<< sampled_use_mil: False
2024-04-27 06:26:59,760:INFO:   <<< seed: 42
2024-04-27 06:26:59,760:INFO:   <<< sim_header: meanP
2024-04-27 06:26:59,760:INFO:   <<< slice_framepos: 2
2024-04-27 06:26:59,760:INFO:   <<< task_type: retrieval
2024-04-27 06:26:59,760:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:26:59,760:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:26:59,761:INFO:   <<< train_frame_order: 0
2024-04-27 06:26:59,761:INFO:   <<< use_mil: False
2024-04-27 06:26:59,761:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:26:59,761:INFO:   <<< video_dim: 1024
2024-04-27 06:26:59,761:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:26:59,761:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:26:59,761:INFO:   <<< world_size: 1
2024-04-27 06:26:59,761:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:27:00,448:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:27:00,459:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:27:31,579:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:27:38,757:WARNING: Test retrieval by loose type.
2024-04-27 06:27:40,202:WARNING: 	 embed_dim: 512
2024-04-27 06:27:40,202:WARNING: 	 image_resolution: 224
2024-04-27 06:27:40,202:WARNING: 	 vision_layers: 12
2024-04-27 06:27:40,203:WARNING: 	 vision_width: 768
2024-04-27 06:27:40,203:WARNING: 	 vision_patch_size: 32
2024-04-27 06:27:40,204:WARNING: 	 context_length: 77
2024-04-27 06:27:40,204:WARNING: 	 vocab_size: 49408
2024-04-27 06:27:40,204:WARNING: 	 transformer_width: 512
2024-04-27 06:27:40,205:WARNING: 	 transformer_heads: 8
2024-04-27 06:27:40,205:WARNING: 	 transformer_layers: 12
2024-04-27 06:27:40,205:WARNING: 		 linear_patch: 2d
2024-04-27 06:27:40,206:WARNING: 	 cut_top_layer: 0
2024-04-27 06:27:41,093:WARNING: 	 sim_header: meanP
2024-04-27 06:28:43,478:INFO: --------------------
2024-04-27 06:28:43,479:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:28:52,145:INFO: Effective parameters:
2024-04-27 06:28:52,145:INFO:   <<< batch_size: 16
2024-04-27 06:28:52,146:INFO:   <<< batch_size_val: 16
2024-04-27 06:28:52,146:INFO:   <<< cache_dir: 
2024-04-27 06:28:52,146:INFO:   <<< coef_lr: 0.001
2024-04-27 06:28:52,146:INFO:   <<< cross_model: cross-base
2024-04-27 06:28:52,146:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:28:52,146:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:28:52,146:INFO:   <<< datatype: msrvtt
2024-04-27 06:28:52,146:INFO:   <<< do_eval: True
2024-04-27 06:28:52,146:INFO:   <<< do_lower_case: False
2024-04-27 06:28:52,146:INFO:   <<< do_pretrain: False
2024-04-27 06:28:52,146:INFO:   <<< do_train: False
2024-04-27 06:28:52,146:INFO:   <<< epochs: 5
2024-04-27 06:28:52,146:INFO:   <<< eval_frame_order: 0
2024-04-27 06:28:52,146:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:28:52,146:INFO:   <<< feature_framerate: 1
2024-04-27 06:28:52,146:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:28:52,147:INFO:   <<< fp16: False
2024-04-27 06:28:52,147:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:28:52,147:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:28:52,147:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:28:52,147:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:28:52,147:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 06:28:52,147:INFO:   <<< linear_patch: 2d
2024-04-27 06:28:52,147:INFO:   <<< local_rank: 0
2024-04-27 06:28:52,147:INFO:   <<< loose_type: False
2024-04-27 06:28:52,147:INFO:   <<< lr: 0.0001
2024-04-27 06:28:52,147:INFO:   <<< lr_decay: 0.9
2024-04-27 06:28:52,147:INFO:   <<< margin: 0.1
2024-04-27 06:28:52,147:INFO:   <<< max_frames: 12
2024-04-27 06:28:52,147:INFO:   <<< max_words: 32
2024-04-27 06:28:52,147:INFO:   <<< n_display: 100
2024-04-27 06:28:52,147:INFO:   <<< n_gpu: 1
2024-04-27 06:28:52,147:INFO:   <<< n_pair: 1
2024-04-27 06:28:52,148:INFO:   <<< negative_weighting: 1
2024-04-27 06:28:52,148:INFO:   <<< num_thread_reader: 0
2024-04-27 06:28:52,148:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:28:52,148:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:28:52,148:INFO:   <<< rank: 0
2024-04-27 06:28:52,148:INFO:   <<< resume_model: None
2024-04-27 06:28:52,148:INFO:   <<< sampled_use_mil: False
2024-04-27 06:28:52,148:INFO:   <<< seed: 42
2024-04-27 06:28:52,148:INFO:   <<< sim_header: meanP
2024-04-27 06:28:52,148:INFO:   <<< slice_framepos: 2
2024-04-27 06:28:52,148:INFO:   <<< task_type: retrieval
2024-04-27 06:28:52,148:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:28:52,148:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:28:52,148:INFO:   <<< train_frame_order: 0
2024-04-27 06:28:52,148:INFO:   <<< use_mil: False
2024-04-27 06:28:52,148:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:28:52,148:INFO:   <<< video_dim: 1024
2024-04-27 06:28:52,149:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:28:52,149:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:28:52,149:INFO:   <<< world_size: 1
2024-04-27 06:28:52,149:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:28:52,828:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:28:52,841:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:28:55,687:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:28:55,689:WARNING: 	 embed_dim: 512
2024-04-27 06:28:55,689:WARNING: 	 image_resolution: 224
2024-04-27 06:28:55,690:WARNING: 	 vision_layers: 12
2024-04-27 06:28:55,690:WARNING: 	 vision_width: 768
2024-04-27 06:28:55,690:WARNING: 	 vision_patch_size: 32
2024-04-27 06:28:55,691:WARNING: 	 context_length: 77
2024-04-27 06:28:55,691:WARNING: 	 vocab_size: 49408
2024-04-27 06:28:55,692:WARNING: 	 transformer_width: 512
2024-04-27 06:28:55,692:WARNING: 	 transformer_heads: 8
2024-04-27 06:28:55,692:WARNING: 	 transformer_layers: 12
2024-04-27 06:28:55,693:WARNING: 		 linear_patch: 2d
2024-04-27 06:28:55,693:WARNING: 	 cut_top_layer: 0
2024-04-27 06:28:56,565:WARNING: 	 sim_header: meanP
2024-04-27 06:29:14,861:WARNING: Set cross_config.num_hidden_layers: 4.
2024-04-27 06:30:35,486:INFO: --------------------
2024-04-27 06:30:35,486:INFO: Weights of CLIP4Clip not initialized from pretrained model: 
   cross.embeddings.position_embeddings.weight
   cross.transformer.resblocks.0.attn.in_proj_weight
   cross.transformer.resblocks.0.attn.in_proj_bias
   cross.transformer.resblocks.0.attn.out_proj.weight
   cross.transformer.resblocks.0.attn.out_proj.bias
   cross.transformer.resblocks.0.ln_1.weight
   cross.transformer.resblocks.0.ln_1.bias
   cross.transformer.resblocks.0.mlp.c_fc.weight
   cross.transformer.resblocks.0.mlp.c_fc.bias
   cross.transformer.resblocks.0.mlp.c_proj.weight
   cross.transformer.resblocks.0.mlp.c_proj.bias
   cross.transformer.resblocks.0.ln_2.weight
   cross.transformer.resblocks.0.ln_2.bias
   cross.transformer.resblocks.1.attn.in_proj_weight
   cross.transformer.resblocks.1.attn.in_proj_bias
   cross.transformer.resblocks.1.attn.out_proj.weight
   cross.transformer.resblocks.1.attn.out_proj.bias
   cross.transformer.resblocks.1.ln_1.weight
   cross.transformer.resblocks.1.ln_1.bias
   cross.transformer.resblocks.1.mlp.c_fc.weight
   cross.transformer.resblocks.1.mlp.c_fc.bias
   cross.transformer.resblocks.1.mlp.c_proj.weight
   cross.transformer.resblocks.1.mlp.c_proj.bias
   cross.transformer.resblocks.1.ln_2.weight
   cross.transformer.resblocks.1.ln_2.bias
   cross.transformer.resblocks.2.attn.in_proj_weight
   cross.transformer.resblocks.2.attn.in_proj_bias
   cross.transformer.resblocks.2.attn.out_proj.weight
   cross.transformer.resblocks.2.attn.out_proj.bias
   cross.transformer.resblocks.2.ln_1.weight
   cross.transformer.resblocks.2.ln_1.bias
   cross.transformer.resblocks.2.mlp.c_fc.weight
   cross.transformer.resblocks.2.mlp.c_fc.bias
   cross.transformer.resblocks.2.mlp.c_proj.weight
   cross.transformer.resblocks.2.mlp.c_proj.bias
   cross.transformer.resblocks.2.ln_2.weight
   cross.transformer.resblocks.2.ln_2.bias
   cross.transformer.resblocks.3.attn.in_proj_weight
   cross.transformer.resblocks.3.attn.in_proj_bias
   cross.transformer.resblocks.3.attn.out_proj.weight
   cross.transformer.resblocks.3.attn.out_proj.bias
   cross.transformer.resblocks.3.ln_1.weight
   cross.transformer.resblocks.3.ln_1.bias
   cross.transformer.resblocks.3.mlp.c_fc.weight
   cross.transformer.resblocks.3.mlp.c_fc.bias
   cross.transformer.resblocks.3.mlp.c_proj.weight
   cross.transformer.resblocks.3.mlp.c_proj.bias
   cross.transformer.resblocks.3.ln_2.weight
   cross.transformer.resblocks.3.ln_2.bias
   cross.pooler.ln_pool.weight
   cross.pooler.ln_pool.bias
   cross.pooler.dense.weight
   cross.pooler.dense.bias
   similarity_dense.weight
   similarity_dense.bias
2024-04-27 06:30:35,487:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:30:41,934:INFO: ***** Running test *****
2024-04-27 06:30:41,934:INFO:   Num examples = 1000
2024-04-27 06:30:41,935:INFO:   Batch size = 16
2024-04-27 06:30:41,935:INFO:   Num steps = 63
2024-04-27 06:30:41,935:INFO: ***** Running val *****
2024-04-27 06:30:41,935:INFO:   Num examples = 1000
2024-04-27 06:35:43,836:INFO: Effective parameters:
2024-04-27 06:35:43,836:INFO:   <<< batch_size: 16
2024-04-27 06:35:43,836:INFO:   <<< batch_size_val: 16
2024-04-27 06:35:43,836:INFO:   <<< cache_dir: 
2024-04-27 06:35:43,836:INFO:   <<< coef_lr: 0.001
2024-04-27 06:35:43,836:INFO:   <<< cross_model: /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:35:43,836:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:35:43,836:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:35:43,836:INFO:   <<< datatype: msrvtt
2024-04-27 06:35:43,836:INFO:   <<< do_eval: True
2024-04-27 06:35:43,836:INFO:   <<< do_lower_case: False
2024-04-27 06:35:43,836:INFO:   <<< do_pretrain: False
2024-04-27 06:35:43,836:INFO:   <<< do_train: False
2024-04-27 06:35:43,836:INFO:   <<< epochs: 5
2024-04-27 06:35:43,836:INFO:   <<< eval_frame_order: 0
2024-04-27 06:35:43,836:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:35:43,836:INFO:   <<< feature_framerate: 1
2024-04-27 06:35:43,836:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:35:43,836:INFO:   <<< fp16: False
2024-04-27 06:35:43,836:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:35:43,836:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:35:43,836:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:35:43,836:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:35:43,836:INFO:   <<< init_model: None
2024-04-27 06:35:43,836:INFO:   <<< linear_patch: 2d
2024-04-27 06:35:43,836:INFO:   <<< local_rank: 0
2024-04-27 06:35:43,836:INFO:   <<< loose_type: False
2024-04-27 06:35:43,836:INFO:   <<< lr: 0.0001
2024-04-27 06:35:43,837:INFO:   <<< lr_decay: 0.9
2024-04-27 06:35:43,837:INFO:   <<< margin: 0.1
2024-04-27 06:35:43,837:INFO:   <<< max_frames: 12
2024-04-27 06:35:43,837:INFO:   <<< max_words: 32
2024-04-27 06:35:43,837:INFO:   <<< n_display: 100
2024-04-27 06:35:43,837:INFO:   <<< n_gpu: 1
2024-04-27 06:35:43,837:INFO:   <<< n_pair: 1
2024-04-27 06:35:43,837:INFO:   <<< negative_weighting: 1
2024-04-27 06:35:43,837:INFO:   <<< num_thread_reader: 0
2024-04-27 06:35:43,837:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:35:43,837:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:35:43,837:INFO:   <<< rank: 0
2024-04-27 06:35:43,837:INFO:   <<< resume_model: None
2024-04-27 06:35:43,837:INFO:   <<< sampled_use_mil: False
2024-04-27 06:35:43,837:INFO:   <<< seed: 42
2024-04-27 06:35:43,837:INFO:   <<< sim_header: meanP
2024-04-27 06:35:43,837:INFO:   <<< slice_framepos: 2
2024-04-27 06:35:43,837:INFO:   <<< task_type: retrieval
2024-04-27 06:35:43,837:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:35:43,837:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:35:43,837:INFO:   <<< train_frame_order: 0
2024-04-27 06:35:43,837:INFO:   <<< use_mil: False
2024-04-27 06:35:43,837:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:35:43,837:INFO:   <<< video_dim: 1024
2024-04-27 06:35:43,837:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:35:43,837:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:35:43,837:INFO:   <<< world_size: 1
2024-04-27 06:35:43,837:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:35:44,408:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:35:44,408:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:35:44,562:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:35:44,562:WARNING: 	 embed_dim: 512
2024-04-27 06:35:44,562:WARNING: 	 image_resolution: 224
2024-04-27 06:35:44,562:WARNING: 	 vision_layers: 12
2024-04-27 06:35:44,562:WARNING: 	 vision_width: 768
2024-04-27 06:35:44,562:WARNING: 	 vision_patch_size: 32
2024-04-27 06:35:44,562:WARNING: 	 context_length: 77
2024-04-27 06:35:44,562:WARNING: 	 vocab_size: 49408
2024-04-27 06:35:44,562:WARNING: 	 transformer_width: 512
2024-04-27 06:35:44,562:WARNING: 	 transformer_heads: 8
2024-04-27 06:35:44,562:WARNING: 	 transformer_layers: 12
2024-04-27 06:35:44,562:WARNING: 		 linear_patch: 2d
2024-04-27 06:35:44,562:WARNING: 	 cut_top_layer: 0
2024-04-27 06:35:45,380:WARNING: 	 sim_header: meanP
2024-04-27 06:35:45,380:WARNING: Set cross_config.num_hidden_layers: 4.
2024-04-27 06:35:49,202:INFO: --------------------
2024-04-27 06:35:49,202:INFO: Weights of CLIP4Clip not initialized from pretrained model: 
   cross.embeddings.position_embeddings.weight
   cross.transformer.resblocks.0.attn.in_proj_weight
   cross.transformer.resblocks.0.attn.in_proj_bias
   cross.transformer.resblocks.0.attn.out_proj.weight
   cross.transformer.resblocks.0.attn.out_proj.bias
   cross.transformer.resblocks.0.ln_1.weight
   cross.transformer.resblocks.0.ln_1.bias
   cross.transformer.resblocks.0.mlp.c_fc.weight
   cross.transformer.resblocks.0.mlp.c_fc.bias
   cross.transformer.resblocks.0.mlp.c_proj.weight
   cross.transformer.resblocks.0.mlp.c_proj.bias
   cross.transformer.resblocks.0.ln_2.weight
   cross.transformer.resblocks.0.ln_2.bias
   cross.transformer.resblocks.1.attn.in_proj_weight
   cross.transformer.resblocks.1.attn.in_proj_bias
   cross.transformer.resblocks.1.attn.out_proj.weight
   cross.transformer.resblocks.1.attn.out_proj.bias
   cross.transformer.resblocks.1.ln_1.weight
   cross.transformer.resblocks.1.ln_1.bias
   cross.transformer.resblocks.1.mlp.c_fc.weight
   cross.transformer.resblocks.1.mlp.c_fc.bias
   cross.transformer.resblocks.1.mlp.c_proj.weight
   cross.transformer.resblocks.1.mlp.c_proj.bias
   cross.transformer.resblocks.1.ln_2.weight
   cross.transformer.resblocks.1.ln_2.bias
   cross.transformer.resblocks.2.attn.in_proj_weight
   cross.transformer.resblocks.2.attn.in_proj_bias
   cross.transformer.resblocks.2.attn.out_proj.weight
   cross.transformer.resblocks.2.attn.out_proj.bias
   cross.transformer.resblocks.2.ln_1.weight
   cross.transformer.resblocks.2.ln_1.bias
   cross.transformer.resblocks.2.mlp.c_fc.weight
   cross.transformer.resblocks.2.mlp.c_fc.bias
   cross.transformer.resblocks.2.mlp.c_proj.weight
   cross.transformer.resblocks.2.mlp.c_proj.bias
   cross.transformer.resblocks.2.ln_2.weight
   cross.transformer.resblocks.2.ln_2.bias
   cross.transformer.resblocks.3.attn.in_proj_weight
   cross.transformer.resblocks.3.attn.in_proj_bias
   cross.transformer.resblocks.3.attn.out_proj.weight
   cross.transformer.resblocks.3.attn.out_proj.bias
   cross.transformer.resblocks.3.ln_1.weight
   cross.transformer.resblocks.3.ln_1.bias
   cross.transformer.resblocks.3.mlp.c_fc.weight
   cross.transformer.resblocks.3.mlp.c_fc.bias
   cross.transformer.resblocks.3.mlp.c_proj.weight
   cross.transformer.resblocks.3.mlp.c_proj.bias
   cross.transformer.resblocks.3.ln_2.weight
   cross.transformer.resblocks.3.ln_2.bias
   cross.pooler.ln_pool.weight
   cross.pooler.ln_pool.bias
   cross.pooler.dense.weight
   cross.pooler.dense.bias
   similarity_dense.weight
   similarity_dense.bias
2024-04-27 06:35:49,202:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:35:51,236:INFO: ***** Running test *****
2024-04-27 06:35:51,236:INFO:   Num examples = 1000
2024-04-27 06:35:51,236:INFO:   Batch size = 16
2024-04-27 06:35:51,236:INFO:   Num steps = 63
2024-04-27 06:35:51,236:INFO: ***** Running val *****
2024-04-27 06:35:51,237:INFO:   Num examples = 1000
2024-04-27 06:38:20,773:INFO: Effective parameters:
2024-04-27 06:38:20,773:INFO:   <<< batch_size: 16
2024-04-27 06:38:20,773:INFO:   <<< batch_size_val: 16
2024-04-27 06:38:20,773:INFO:   <<< cache_dir: 
2024-04-27 06:38:20,773:INFO:   <<< coef_lr: 0.001
2024-04-27 06:38:20,773:INFO:   <<< cross_model: /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:38:20,773:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 06:38:20,773:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 06:38:20,773:INFO:   <<< datatype: msrvtt
2024-04-27 06:38:20,773:INFO:   <<< do_eval: True
2024-04-27 06:38:20,773:INFO:   <<< do_lower_case: False
2024-04-27 06:38:20,773:INFO:   <<< do_pretrain: False
2024-04-27 06:38:20,773:INFO:   <<< do_train: False
2024-04-27 06:38:20,773:INFO:   <<< epochs: 5
2024-04-27 06:38:20,773:INFO:   <<< eval_frame_order: 0
2024-04-27 06:38:20,773:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 06:38:20,773:INFO:   <<< feature_framerate: 1
2024-04-27 06:38:20,773:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 06:38:20,773:INFO:   <<< fp16: False
2024-04-27 06:38:20,773:INFO:   <<< fp16_opt_level: O1
2024-04-27 06:38:20,773:INFO:   <<< freeze_layer_num: 0
2024-04-27 06:38:20,773:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 06:38:20,773:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 06:38:20,773:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 06:38:20,773:INFO:   <<< linear_patch: 2d
2024-04-27 06:38:20,774:INFO:   <<< local_rank: 0
2024-04-27 06:38:20,774:INFO:   <<< loose_type: False
2024-04-27 06:38:20,774:INFO:   <<< lr: 0.0001
2024-04-27 06:38:20,774:INFO:   <<< lr_decay: 0.9
2024-04-27 06:38:20,774:INFO:   <<< margin: 0.1
2024-04-27 06:38:20,774:INFO:   <<< max_frames: 12
2024-04-27 06:38:20,774:INFO:   <<< max_words: 32
2024-04-27 06:38:20,774:INFO:   <<< n_display: 100
2024-04-27 06:38:20,774:INFO:   <<< n_gpu: 1
2024-04-27 06:38:20,774:INFO:   <<< n_pair: 1
2024-04-27 06:38:20,774:INFO:   <<< negative_weighting: 1
2024-04-27 06:38:20,774:INFO:   <<< num_thread_reader: 0
2024-04-27 06:38:20,774:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 06:38:20,774:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 06:38:20,774:INFO:   <<< rank: 0
2024-04-27 06:38:20,774:INFO:   <<< resume_model: None
2024-04-27 06:38:20,774:INFO:   <<< sampled_use_mil: False
2024-04-27 06:38:20,774:INFO:   <<< seed: 42
2024-04-27 06:38:20,774:INFO:   <<< sim_header: meanP
2024-04-27 06:38:20,774:INFO:   <<< slice_framepos: 2
2024-04-27 06:38:20,774:INFO:   <<< task_type: retrieval
2024-04-27 06:38:20,774:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 06:38:20,774:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 06:38:20,774:INFO:   <<< train_frame_order: 0
2024-04-27 06:38:20,774:INFO:   <<< use_mil: False
2024-04-27 06:38:20,774:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 06:38:20,774:INFO:   <<< video_dim: 1024
2024-04-27 06:38:20,774:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 06:38:20,774:INFO:   <<< warmup_proportion: 0.1
2024-04-27 06:38:20,774:INFO:   <<< world_size: 1
2024-04-27 06:38:20,774:INFO: device: cuda:0 n_gpu: 1
2024-04-27 06:38:21,509:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 06:38:21,510:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 06:38:21,663:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 06:38:21,663:WARNING: 	 embed_dim: 512
2024-04-27 06:38:21,663:WARNING: 	 image_resolution: 224
2024-04-27 06:38:21,663:WARNING: 	 vision_layers: 12
2024-04-27 06:38:21,663:WARNING: 	 vision_width: 768
2024-04-27 06:38:21,663:WARNING: 	 vision_patch_size: 32
2024-04-27 06:38:21,663:WARNING: 	 context_length: 77
2024-04-27 06:38:21,663:WARNING: 	 vocab_size: 49408
2024-04-27 06:38:21,663:WARNING: 	 transformer_width: 512
2024-04-27 06:38:21,663:WARNING: 	 transformer_heads: 8
2024-04-27 06:38:21,664:WARNING: 	 transformer_layers: 12
2024-04-27 06:38:21,664:WARNING: 		 linear_patch: 2d
2024-04-27 06:38:21,664:WARNING: 	 cut_top_layer: 0
2024-04-27 06:38:22,477:WARNING: 	 sim_header: meanP
2024-04-27 06:38:22,477:WARNING: Set cross_config.num_hidden_layers: 4.
2024-04-27 06:38:26,354:INFO: --------------------
2024-04-27 06:38:26,354:INFO: Weights of CLIP4Clip not initialized from pretrained model: 
   cross.embeddings.position_embeddings.weight
   cross.transformer.resblocks.0.attn.in_proj_weight
   cross.transformer.resblocks.0.attn.in_proj_bias
   cross.transformer.resblocks.0.attn.out_proj.weight
   cross.transformer.resblocks.0.attn.out_proj.bias
   cross.transformer.resblocks.0.ln_1.weight
   cross.transformer.resblocks.0.ln_1.bias
   cross.transformer.resblocks.0.mlp.c_fc.weight
   cross.transformer.resblocks.0.mlp.c_fc.bias
   cross.transformer.resblocks.0.mlp.c_proj.weight
   cross.transformer.resblocks.0.mlp.c_proj.bias
   cross.transformer.resblocks.0.ln_2.weight
   cross.transformer.resblocks.0.ln_2.bias
   cross.transformer.resblocks.1.attn.in_proj_weight
   cross.transformer.resblocks.1.attn.in_proj_bias
   cross.transformer.resblocks.1.attn.out_proj.weight
   cross.transformer.resblocks.1.attn.out_proj.bias
   cross.transformer.resblocks.1.ln_1.weight
   cross.transformer.resblocks.1.ln_1.bias
   cross.transformer.resblocks.1.mlp.c_fc.weight
   cross.transformer.resblocks.1.mlp.c_fc.bias
   cross.transformer.resblocks.1.mlp.c_proj.weight
   cross.transformer.resblocks.1.mlp.c_proj.bias
   cross.transformer.resblocks.1.ln_2.weight
   cross.transformer.resblocks.1.ln_2.bias
   cross.transformer.resblocks.2.attn.in_proj_weight
   cross.transformer.resblocks.2.attn.in_proj_bias
   cross.transformer.resblocks.2.attn.out_proj.weight
   cross.transformer.resblocks.2.attn.out_proj.bias
   cross.transformer.resblocks.2.ln_1.weight
   cross.transformer.resblocks.2.ln_1.bias
   cross.transformer.resblocks.2.mlp.c_fc.weight
   cross.transformer.resblocks.2.mlp.c_fc.bias
   cross.transformer.resblocks.2.mlp.c_proj.weight
   cross.transformer.resblocks.2.mlp.c_proj.bias
   cross.transformer.resblocks.2.ln_2.weight
   cross.transformer.resblocks.2.ln_2.bias
   cross.transformer.resblocks.3.attn.in_proj_weight
   cross.transformer.resblocks.3.attn.in_proj_bias
   cross.transformer.resblocks.3.attn.out_proj.weight
   cross.transformer.resblocks.3.attn.out_proj.bias
   cross.transformer.resblocks.3.ln_1.weight
   cross.transformer.resblocks.3.ln_1.bias
   cross.transformer.resblocks.3.mlp.c_fc.weight
   cross.transformer.resblocks.3.mlp.c_fc.bias
   cross.transformer.resblocks.3.mlp.c_proj.weight
   cross.transformer.resblocks.3.mlp.c_proj.bias
   cross.transformer.resblocks.3.ln_2.weight
   cross.transformer.resblocks.3.ln_2.bias
   cross.pooler.ln_pool.weight
   cross.pooler.ln_pool.bias
   cross.pooler.dense.weight
   cross.pooler.dense.bias
   similarity_dense.weight
   similarity_dense.bias
2024-04-27 06:38:26,354:INFO: Weights from pretrained model not used in CLIP4Clip: 
   logit_scale
   text_model.embeddings.position_ids
   text_model.embeddings.token_embedding.weight
   text_model.embeddings.position_embedding.weight
   text_model.encoder.layers.0.self_attn.k_proj.weight
   text_model.encoder.layers.0.self_attn.k_proj.bias
   text_model.encoder.layers.0.self_attn.v_proj.weight
   text_model.encoder.layers.0.self_attn.v_proj.bias
   text_model.encoder.layers.0.self_attn.q_proj.weight
   text_model.encoder.layers.0.self_attn.q_proj.bias
   text_model.encoder.layers.0.self_attn.out_proj.weight
   text_model.encoder.layers.0.self_attn.out_proj.bias
   text_model.encoder.layers.0.layer_norm1.weight
   text_model.encoder.layers.0.layer_norm1.bias
   text_model.encoder.layers.0.mlp.fc1.weight
   text_model.encoder.layers.0.mlp.fc1.bias
   text_model.encoder.layers.0.mlp.fc2.weight
   text_model.encoder.layers.0.mlp.fc2.bias
   text_model.encoder.layers.0.layer_norm2.weight
   text_model.encoder.layers.0.layer_norm2.bias
   text_model.encoder.layers.1.self_attn.k_proj.weight
   text_model.encoder.layers.1.self_attn.k_proj.bias
   text_model.encoder.layers.1.self_attn.v_proj.weight
   text_model.encoder.layers.1.self_attn.v_proj.bias
   text_model.encoder.layers.1.self_attn.q_proj.weight
   text_model.encoder.layers.1.self_attn.q_proj.bias
   text_model.encoder.layers.1.self_attn.out_proj.weight
   text_model.encoder.layers.1.self_attn.out_proj.bias
   text_model.encoder.layers.1.layer_norm1.weight
   text_model.encoder.layers.1.layer_norm1.bias
   text_model.encoder.layers.1.mlp.fc1.weight
   text_model.encoder.layers.1.mlp.fc1.bias
   text_model.encoder.layers.1.mlp.fc2.weight
   text_model.encoder.layers.1.mlp.fc2.bias
   text_model.encoder.layers.1.layer_norm2.weight
   text_model.encoder.layers.1.layer_norm2.bias
   text_model.encoder.layers.2.self_attn.k_proj.weight
   text_model.encoder.layers.2.self_attn.k_proj.bias
   text_model.encoder.layers.2.self_attn.v_proj.weight
   text_model.encoder.layers.2.self_attn.v_proj.bias
   text_model.encoder.layers.2.self_attn.q_proj.weight
   text_model.encoder.layers.2.self_attn.q_proj.bias
   text_model.encoder.layers.2.self_attn.out_proj.weight
   text_model.encoder.layers.2.self_attn.out_proj.bias
   text_model.encoder.layers.2.layer_norm1.weight
   text_model.encoder.layers.2.layer_norm1.bias
   text_model.encoder.layers.2.mlp.fc1.weight
   text_model.encoder.layers.2.mlp.fc1.bias
   text_model.encoder.layers.2.mlp.fc2.weight
   text_model.encoder.layers.2.mlp.fc2.bias
   text_model.encoder.layers.2.layer_norm2.weight
   text_model.encoder.layers.2.layer_norm2.bias
   text_model.encoder.layers.3.self_attn.k_proj.weight
   text_model.encoder.layers.3.self_attn.k_proj.bias
   text_model.encoder.layers.3.self_attn.v_proj.weight
   text_model.encoder.layers.3.self_attn.v_proj.bias
   text_model.encoder.layers.3.self_attn.q_proj.weight
   text_model.encoder.layers.3.self_attn.q_proj.bias
   text_model.encoder.layers.3.self_attn.out_proj.weight
   text_model.encoder.layers.3.self_attn.out_proj.bias
   text_model.encoder.layers.3.layer_norm1.weight
   text_model.encoder.layers.3.layer_norm1.bias
   text_model.encoder.layers.3.mlp.fc1.weight
   text_model.encoder.layers.3.mlp.fc1.bias
   text_model.encoder.layers.3.mlp.fc2.weight
   text_model.encoder.layers.3.mlp.fc2.bias
   text_model.encoder.layers.3.layer_norm2.weight
   text_model.encoder.layers.3.layer_norm2.bias
   text_model.encoder.layers.4.self_attn.k_proj.weight
   text_model.encoder.layers.4.self_attn.k_proj.bias
   text_model.encoder.layers.4.self_attn.v_proj.weight
   text_model.encoder.layers.4.self_attn.v_proj.bias
   text_model.encoder.layers.4.self_attn.q_proj.weight
   text_model.encoder.layers.4.self_attn.q_proj.bias
   text_model.encoder.layers.4.self_attn.out_proj.weight
   text_model.encoder.layers.4.self_attn.out_proj.bias
   text_model.encoder.layers.4.layer_norm1.weight
   text_model.encoder.layers.4.layer_norm1.bias
   text_model.encoder.layers.4.mlp.fc1.weight
   text_model.encoder.layers.4.mlp.fc1.bias
   text_model.encoder.layers.4.mlp.fc2.weight
   text_model.encoder.layers.4.mlp.fc2.bias
   text_model.encoder.layers.4.layer_norm2.weight
   text_model.encoder.layers.4.layer_norm2.bias
   text_model.encoder.layers.5.self_attn.k_proj.weight
   text_model.encoder.layers.5.self_attn.k_proj.bias
   text_model.encoder.layers.5.self_attn.v_proj.weight
   text_model.encoder.layers.5.self_attn.v_proj.bias
   text_model.encoder.layers.5.self_attn.q_proj.weight
   text_model.encoder.layers.5.self_attn.q_proj.bias
   text_model.encoder.layers.5.self_attn.out_proj.weight
   text_model.encoder.layers.5.self_attn.out_proj.bias
   text_model.encoder.layers.5.layer_norm1.weight
   text_model.encoder.layers.5.layer_norm1.bias
   text_model.encoder.layers.5.mlp.fc1.weight
   text_model.encoder.layers.5.mlp.fc1.bias
   text_model.encoder.layers.5.mlp.fc2.weight
   text_model.encoder.layers.5.mlp.fc2.bias
   text_model.encoder.layers.5.layer_norm2.weight
   text_model.encoder.layers.5.layer_norm2.bias
   text_model.encoder.layers.6.self_attn.k_proj.weight
   text_model.encoder.layers.6.self_attn.k_proj.bias
   text_model.encoder.layers.6.self_attn.v_proj.weight
   text_model.encoder.layers.6.self_attn.v_proj.bias
   text_model.encoder.layers.6.self_attn.q_proj.weight
   text_model.encoder.layers.6.self_attn.q_proj.bias
   text_model.encoder.layers.6.self_attn.out_proj.weight
   text_model.encoder.layers.6.self_attn.out_proj.bias
   text_model.encoder.layers.6.layer_norm1.weight
   text_model.encoder.layers.6.layer_norm1.bias
   text_model.encoder.layers.6.mlp.fc1.weight
   text_model.encoder.layers.6.mlp.fc1.bias
   text_model.encoder.layers.6.mlp.fc2.weight
   text_model.encoder.layers.6.mlp.fc2.bias
   text_model.encoder.layers.6.layer_norm2.weight
   text_model.encoder.layers.6.layer_norm2.bias
   text_model.encoder.layers.7.self_attn.k_proj.weight
   text_model.encoder.layers.7.self_attn.k_proj.bias
   text_model.encoder.layers.7.self_attn.v_proj.weight
   text_model.encoder.layers.7.self_attn.v_proj.bias
   text_model.encoder.layers.7.self_attn.q_proj.weight
   text_model.encoder.layers.7.self_attn.q_proj.bias
   text_model.encoder.layers.7.self_attn.out_proj.weight
   text_model.encoder.layers.7.self_attn.out_proj.bias
   text_model.encoder.layers.7.layer_norm1.weight
   text_model.encoder.layers.7.layer_norm1.bias
   text_model.encoder.layers.7.mlp.fc1.weight
   text_model.encoder.layers.7.mlp.fc1.bias
   text_model.encoder.layers.7.mlp.fc2.weight
   text_model.encoder.layers.7.mlp.fc2.bias
   text_model.encoder.layers.7.layer_norm2.weight
   text_model.encoder.layers.7.layer_norm2.bias
   text_model.encoder.layers.8.self_attn.k_proj.weight
   text_model.encoder.layers.8.self_attn.k_proj.bias
   text_model.encoder.layers.8.self_attn.v_proj.weight
   text_model.encoder.layers.8.self_attn.v_proj.bias
   text_model.encoder.layers.8.self_attn.q_proj.weight
   text_model.encoder.layers.8.self_attn.q_proj.bias
   text_model.encoder.layers.8.self_attn.out_proj.weight
   text_model.encoder.layers.8.self_attn.out_proj.bias
   text_model.encoder.layers.8.layer_norm1.weight
   text_model.encoder.layers.8.layer_norm1.bias
   text_model.encoder.layers.8.mlp.fc1.weight
   text_model.encoder.layers.8.mlp.fc1.bias
   text_model.encoder.layers.8.mlp.fc2.weight
   text_model.encoder.layers.8.mlp.fc2.bias
   text_model.encoder.layers.8.layer_norm2.weight
   text_model.encoder.layers.8.layer_norm2.bias
   text_model.encoder.layers.9.self_attn.k_proj.weight
   text_model.encoder.layers.9.self_attn.k_proj.bias
   text_model.encoder.layers.9.self_attn.v_proj.weight
   text_model.encoder.layers.9.self_attn.v_proj.bias
   text_model.encoder.layers.9.self_attn.q_proj.weight
   text_model.encoder.layers.9.self_attn.q_proj.bias
   text_model.encoder.layers.9.self_attn.out_proj.weight
   text_model.encoder.layers.9.self_attn.out_proj.bias
   text_model.encoder.layers.9.layer_norm1.weight
   text_model.encoder.layers.9.layer_norm1.bias
   text_model.encoder.layers.9.mlp.fc1.weight
   text_model.encoder.layers.9.mlp.fc1.bias
   text_model.encoder.layers.9.mlp.fc2.weight
   text_model.encoder.layers.9.mlp.fc2.bias
   text_model.encoder.layers.9.layer_norm2.weight
   text_model.encoder.layers.9.layer_norm2.bias
   text_model.encoder.layers.10.self_attn.k_proj.weight
   text_model.encoder.layers.10.self_attn.k_proj.bias
   text_model.encoder.layers.10.self_attn.v_proj.weight
   text_model.encoder.layers.10.self_attn.v_proj.bias
   text_model.encoder.layers.10.self_attn.q_proj.weight
   text_model.encoder.layers.10.self_attn.q_proj.bias
   text_model.encoder.layers.10.self_attn.out_proj.weight
   text_model.encoder.layers.10.self_attn.out_proj.bias
   text_model.encoder.layers.10.layer_norm1.weight
   text_model.encoder.layers.10.layer_norm1.bias
   text_model.encoder.layers.10.mlp.fc1.weight
   text_model.encoder.layers.10.mlp.fc1.bias
   text_model.encoder.layers.10.mlp.fc2.weight
   text_model.encoder.layers.10.mlp.fc2.bias
   text_model.encoder.layers.10.layer_norm2.weight
   text_model.encoder.layers.10.layer_norm2.bias
   text_model.encoder.layers.11.self_attn.k_proj.weight
   text_model.encoder.layers.11.self_attn.k_proj.bias
   text_model.encoder.layers.11.self_attn.v_proj.weight
   text_model.encoder.layers.11.self_attn.v_proj.bias
   text_model.encoder.layers.11.self_attn.q_proj.weight
   text_model.encoder.layers.11.self_attn.q_proj.bias
   text_model.encoder.layers.11.self_attn.out_proj.weight
   text_model.encoder.layers.11.self_attn.out_proj.bias
   text_model.encoder.layers.11.layer_norm1.weight
   text_model.encoder.layers.11.layer_norm1.bias
   text_model.encoder.layers.11.mlp.fc1.weight
   text_model.encoder.layers.11.mlp.fc1.bias
   text_model.encoder.layers.11.mlp.fc2.weight
   text_model.encoder.layers.11.mlp.fc2.bias
   text_model.encoder.layers.11.layer_norm2.weight
   text_model.encoder.layers.11.layer_norm2.bias
   text_model.final_layer_norm.weight
   text_model.final_layer_norm.bias
   vision_model.embeddings.class_embedding
   vision_model.embeddings.position_ids
   vision_model.embeddings.patch_embedding.weight
   vision_model.embeddings.position_embedding.weight
   vision_model.pre_layrnorm.weight
   vision_model.pre_layrnorm.bias
   vision_model.encoder.layers.0.self_attn.k_proj.weight
   vision_model.encoder.layers.0.self_attn.k_proj.bias
   vision_model.encoder.layers.0.self_attn.v_proj.weight
   vision_model.encoder.layers.0.self_attn.v_proj.bias
   vision_model.encoder.layers.0.self_attn.q_proj.weight
   vision_model.encoder.layers.0.self_attn.q_proj.bias
   vision_model.encoder.layers.0.self_attn.out_proj.weight
   vision_model.encoder.layers.0.self_attn.out_proj.bias
   vision_model.encoder.layers.0.layer_norm1.weight
   vision_model.encoder.layers.0.layer_norm1.bias
   vision_model.encoder.layers.0.mlp.fc1.weight
   vision_model.encoder.layers.0.mlp.fc1.bias
   vision_model.encoder.layers.0.mlp.fc2.weight
   vision_model.encoder.layers.0.mlp.fc2.bias
   vision_model.encoder.layers.0.layer_norm2.weight
   vision_model.encoder.layers.0.layer_norm2.bias
   vision_model.encoder.layers.1.self_attn.k_proj.weight
   vision_model.encoder.layers.1.self_attn.k_proj.bias
   vision_model.encoder.layers.1.self_attn.v_proj.weight
   vision_model.encoder.layers.1.self_attn.v_proj.bias
   vision_model.encoder.layers.1.self_attn.q_proj.weight
   vision_model.encoder.layers.1.self_attn.q_proj.bias
   vision_model.encoder.layers.1.self_attn.out_proj.weight
   vision_model.encoder.layers.1.self_attn.out_proj.bias
   vision_model.encoder.layers.1.layer_norm1.weight
   vision_model.encoder.layers.1.layer_norm1.bias
   vision_model.encoder.layers.1.mlp.fc1.weight
   vision_model.encoder.layers.1.mlp.fc1.bias
   vision_model.encoder.layers.1.mlp.fc2.weight
   vision_model.encoder.layers.1.mlp.fc2.bias
   vision_model.encoder.layers.1.layer_norm2.weight
   vision_model.encoder.layers.1.layer_norm2.bias
   vision_model.encoder.layers.2.self_attn.k_proj.weight
   vision_model.encoder.layers.2.self_attn.k_proj.bias
   vision_model.encoder.layers.2.self_attn.v_proj.weight
   vision_model.encoder.layers.2.self_attn.v_proj.bias
   vision_model.encoder.layers.2.self_attn.q_proj.weight
   vision_model.encoder.layers.2.self_attn.q_proj.bias
   vision_model.encoder.layers.2.self_attn.out_proj.weight
   vision_model.encoder.layers.2.self_attn.out_proj.bias
   vision_model.encoder.layers.2.layer_norm1.weight
   vision_model.encoder.layers.2.layer_norm1.bias
   vision_model.encoder.layers.2.mlp.fc1.weight
   vision_model.encoder.layers.2.mlp.fc1.bias
   vision_model.encoder.layers.2.mlp.fc2.weight
   vision_model.encoder.layers.2.mlp.fc2.bias
   vision_model.encoder.layers.2.layer_norm2.weight
   vision_model.encoder.layers.2.layer_norm2.bias
   vision_model.encoder.layers.3.self_attn.k_proj.weight
   vision_model.encoder.layers.3.self_attn.k_proj.bias
   vision_model.encoder.layers.3.self_attn.v_proj.weight
   vision_model.encoder.layers.3.self_attn.v_proj.bias
   vision_model.encoder.layers.3.self_attn.q_proj.weight
   vision_model.encoder.layers.3.self_attn.q_proj.bias
   vision_model.encoder.layers.3.self_attn.out_proj.weight
   vision_model.encoder.layers.3.self_attn.out_proj.bias
   vision_model.encoder.layers.3.layer_norm1.weight
   vision_model.encoder.layers.3.layer_norm1.bias
   vision_model.encoder.layers.3.mlp.fc1.weight
   vision_model.encoder.layers.3.mlp.fc1.bias
   vision_model.encoder.layers.3.mlp.fc2.weight
   vision_model.encoder.layers.3.mlp.fc2.bias
   vision_model.encoder.layers.3.layer_norm2.weight
   vision_model.encoder.layers.3.layer_norm2.bias
   vision_model.encoder.layers.4.self_attn.k_proj.weight
   vision_model.encoder.layers.4.self_attn.k_proj.bias
   vision_model.encoder.layers.4.self_attn.v_proj.weight
   vision_model.encoder.layers.4.self_attn.v_proj.bias
   vision_model.encoder.layers.4.self_attn.q_proj.weight
   vision_model.encoder.layers.4.self_attn.q_proj.bias
   vision_model.encoder.layers.4.self_attn.out_proj.weight
   vision_model.encoder.layers.4.self_attn.out_proj.bias
   vision_model.encoder.layers.4.layer_norm1.weight
   vision_model.encoder.layers.4.layer_norm1.bias
   vision_model.encoder.layers.4.mlp.fc1.weight
   vision_model.encoder.layers.4.mlp.fc1.bias
   vision_model.encoder.layers.4.mlp.fc2.weight
   vision_model.encoder.layers.4.mlp.fc2.bias
   vision_model.encoder.layers.4.layer_norm2.weight
   vision_model.encoder.layers.4.layer_norm2.bias
   vision_model.encoder.layers.5.self_attn.k_proj.weight
   vision_model.encoder.layers.5.self_attn.k_proj.bias
   vision_model.encoder.layers.5.self_attn.v_proj.weight
   vision_model.encoder.layers.5.self_attn.v_proj.bias
   vision_model.encoder.layers.5.self_attn.q_proj.weight
   vision_model.encoder.layers.5.self_attn.q_proj.bias
   vision_model.encoder.layers.5.self_attn.out_proj.weight
   vision_model.encoder.layers.5.self_attn.out_proj.bias
   vision_model.encoder.layers.5.layer_norm1.weight
   vision_model.encoder.layers.5.layer_norm1.bias
   vision_model.encoder.layers.5.mlp.fc1.weight
   vision_model.encoder.layers.5.mlp.fc1.bias
   vision_model.encoder.layers.5.mlp.fc2.weight
   vision_model.encoder.layers.5.mlp.fc2.bias
   vision_model.encoder.layers.5.layer_norm2.weight
   vision_model.encoder.layers.5.layer_norm2.bias
   vision_model.encoder.layers.6.self_attn.k_proj.weight
   vision_model.encoder.layers.6.self_attn.k_proj.bias
   vision_model.encoder.layers.6.self_attn.v_proj.weight
   vision_model.encoder.layers.6.self_attn.v_proj.bias
   vision_model.encoder.layers.6.self_attn.q_proj.weight
   vision_model.encoder.layers.6.self_attn.q_proj.bias
   vision_model.encoder.layers.6.self_attn.out_proj.weight
   vision_model.encoder.layers.6.self_attn.out_proj.bias
   vision_model.encoder.layers.6.layer_norm1.weight
   vision_model.encoder.layers.6.layer_norm1.bias
   vision_model.encoder.layers.6.mlp.fc1.weight
   vision_model.encoder.layers.6.mlp.fc1.bias
   vision_model.encoder.layers.6.mlp.fc2.weight
   vision_model.encoder.layers.6.mlp.fc2.bias
   vision_model.encoder.layers.6.layer_norm2.weight
   vision_model.encoder.layers.6.layer_norm2.bias
   vision_model.encoder.layers.7.self_attn.k_proj.weight
   vision_model.encoder.layers.7.self_attn.k_proj.bias
   vision_model.encoder.layers.7.self_attn.v_proj.weight
   vision_model.encoder.layers.7.self_attn.v_proj.bias
   vision_model.encoder.layers.7.self_attn.q_proj.weight
   vision_model.encoder.layers.7.self_attn.q_proj.bias
   vision_model.encoder.layers.7.self_attn.out_proj.weight
   vision_model.encoder.layers.7.self_attn.out_proj.bias
   vision_model.encoder.layers.7.layer_norm1.weight
   vision_model.encoder.layers.7.layer_norm1.bias
   vision_model.encoder.layers.7.mlp.fc1.weight
   vision_model.encoder.layers.7.mlp.fc1.bias
   vision_model.encoder.layers.7.mlp.fc2.weight
   vision_model.encoder.layers.7.mlp.fc2.bias
   vision_model.encoder.layers.7.layer_norm2.weight
   vision_model.encoder.layers.7.layer_norm2.bias
   vision_model.encoder.layers.8.self_attn.k_proj.weight
   vision_model.encoder.layers.8.self_attn.k_proj.bias
   vision_model.encoder.layers.8.self_attn.v_proj.weight
   vision_model.encoder.layers.8.self_attn.v_proj.bias
   vision_model.encoder.layers.8.self_attn.q_proj.weight
   vision_model.encoder.layers.8.self_attn.q_proj.bias
   vision_model.encoder.layers.8.self_attn.out_proj.weight
   vision_model.encoder.layers.8.self_attn.out_proj.bias
   vision_model.encoder.layers.8.layer_norm1.weight
   vision_model.encoder.layers.8.layer_norm1.bias
   vision_model.encoder.layers.8.mlp.fc1.weight
   vision_model.encoder.layers.8.mlp.fc1.bias
   vision_model.encoder.layers.8.mlp.fc2.weight
   vision_model.encoder.layers.8.mlp.fc2.bias
   vision_model.encoder.layers.8.layer_norm2.weight
   vision_model.encoder.layers.8.layer_norm2.bias
   vision_model.encoder.layers.9.self_attn.k_proj.weight
   vision_model.encoder.layers.9.self_attn.k_proj.bias
   vision_model.encoder.layers.9.self_attn.v_proj.weight
   vision_model.encoder.layers.9.self_attn.v_proj.bias
   vision_model.encoder.layers.9.self_attn.q_proj.weight
   vision_model.encoder.layers.9.self_attn.q_proj.bias
   vision_model.encoder.layers.9.self_attn.out_proj.weight
   vision_model.encoder.layers.9.self_attn.out_proj.bias
   vision_model.encoder.layers.9.layer_norm1.weight
   vision_model.encoder.layers.9.layer_norm1.bias
   vision_model.encoder.layers.9.mlp.fc1.weight
   vision_model.encoder.layers.9.mlp.fc1.bias
   vision_model.encoder.layers.9.mlp.fc2.weight
   vision_model.encoder.layers.9.mlp.fc2.bias
   vision_model.encoder.layers.9.layer_norm2.weight
   vision_model.encoder.layers.9.layer_norm2.bias
   vision_model.encoder.layers.10.self_attn.k_proj.weight
   vision_model.encoder.layers.10.self_attn.k_proj.bias
   vision_model.encoder.layers.10.self_attn.v_proj.weight
   vision_model.encoder.layers.10.self_attn.v_proj.bias
   vision_model.encoder.layers.10.self_attn.q_proj.weight
   vision_model.encoder.layers.10.self_attn.q_proj.bias
   vision_model.encoder.layers.10.self_attn.out_proj.weight
   vision_model.encoder.layers.10.self_attn.out_proj.bias
   vision_model.encoder.layers.10.layer_norm1.weight
   vision_model.encoder.layers.10.layer_norm1.bias
   vision_model.encoder.layers.10.mlp.fc1.weight
   vision_model.encoder.layers.10.mlp.fc1.bias
   vision_model.encoder.layers.10.mlp.fc2.weight
   vision_model.encoder.layers.10.mlp.fc2.bias
   vision_model.encoder.layers.10.layer_norm2.weight
   vision_model.encoder.layers.10.layer_norm2.bias
   vision_model.encoder.layers.11.self_attn.k_proj.weight
   vision_model.encoder.layers.11.self_attn.k_proj.bias
   vision_model.encoder.layers.11.self_attn.v_proj.weight
   vision_model.encoder.layers.11.self_attn.v_proj.bias
   vision_model.encoder.layers.11.self_attn.q_proj.weight
   vision_model.encoder.layers.11.self_attn.q_proj.bias
   vision_model.encoder.layers.11.self_attn.out_proj.weight
   vision_model.encoder.layers.11.self_attn.out_proj.bias
   vision_model.encoder.layers.11.layer_norm1.weight
   vision_model.encoder.layers.11.layer_norm1.bias
   vision_model.encoder.layers.11.mlp.fc1.weight
   vision_model.encoder.layers.11.mlp.fc1.bias
   vision_model.encoder.layers.11.mlp.fc2.weight
   vision_model.encoder.layers.11.mlp.fc2.bias
   vision_model.encoder.layers.11.layer_norm2.weight
   vision_model.encoder.layers.11.layer_norm2.bias
   vision_model.post_layernorm.weight
   vision_model.post_layernorm.bias
   visual_projection.weight
   text_projection.weight
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 06:38:28,359:INFO: ***** Running test *****
2024-04-27 06:38:28,359:INFO:   Num examples = 1000
2024-04-27 06:38:28,359:INFO:   Batch size = 16
2024-04-27 06:38:28,359:INFO:   Num steps = 63
2024-04-27 06:38:28,359:INFO: ***** Running val *****
2024-04-27 06:38:28,359:INFO:   Num examples = 1000
2024-04-27 07:25:15,329:INFO: Effective parameters:
2024-04-27 07:25:15,329:INFO:   <<< batch_size: 16
2024-04-27 07:25:15,329:INFO:   <<< batch_size_val: 16
2024-04-27 07:25:15,329:INFO:   <<< cache_dir: 
2024-04-27 07:25:15,329:INFO:   <<< coef_lr: 0.001
2024-04-27 07:25:15,329:INFO:   <<< cross_model: cross-base
2024-04-27 07:25:15,329:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 07:25:15,329:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 07:25:15,330:INFO:   <<< datatype: msrvtt
2024-04-27 07:25:15,330:INFO:   <<< do_eval: True
2024-04-27 07:25:15,330:INFO:   <<< do_lower_case: False
2024-04-27 07:25:15,330:INFO:   <<< do_pretrain: False
2024-04-27 07:25:15,330:INFO:   <<< do_train: False
2024-04-27 07:25:15,330:INFO:   <<< epochs: 5
2024-04-27 07:25:15,330:INFO:   <<< eval_frame_order: 0
2024-04-27 07:25:15,330:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 07:25:15,330:INFO:   <<< feature_framerate: 1
2024-04-27 07:25:15,330:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 07:25:15,330:INFO:   <<< fp16: False
2024-04-27 07:25:15,330:INFO:   <<< fp16_opt_level: O1
2024-04-27 07:25:15,330:INFO:   <<< freeze_layer_num: 0
2024-04-27 07:25:15,330:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 07:25:15,330:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 07:25:15,330:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 07:25:15,330:INFO:   <<< linear_patch: 2d
2024-04-27 07:25:15,330:INFO:   <<< local_rank: 0
2024-04-27 07:25:15,331:INFO:   <<< loose_type: False
2024-04-27 07:25:15,331:INFO:   <<< lr: 0.0001
2024-04-27 07:25:15,331:INFO:   <<< lr_decay: 0.9
2024-04-27 07:25:15,331:INFO:   <<< margin: 0.1
2024-04-27 07:25:15,331:INFO:   <<< max_frames: 12
2024-04-27 07:25:15,331:INFO:   <<< max_words: 32
2024-04-27 07:25:15,331:INFO:   <<< n_display: 100
2024-04-27 07:25:15,331:INFO:   <<< n_gpu: 1
2024-04-27 07:25:15,331:INFO:   <<< n_pair: 1
2024-04-27 07:25:15,331:INFO:   <<< negative_weighting: 1
2024-04-27 07:25:15,331:INFO:   <<< num_thread_reader: 0
2024-04-27 07:25:15,331:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 07:25:15,331:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 07:25:15,331:INFO:   <<< rank: 0
2024-04-27 07:25:15,331:INFO:   <<< resume_model: None
2024-04-27 07:25:15,331:INFO:   <<< sampled_use_mil: False
2024-04-27 07:25:15,332:INFO:   <<< seed: 42
2024-04-27 07:25:15,332:INFO:   <<< sim_header: meanP
2024-04-27 07:25:15,332:INFO:   <<< slice_framepos: 2
2024-04-27 07:25:15,332:INFO:   <<< task_type: retrieval
2024-04-27 07:25:15,332:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 07:25:15,332:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 07:25:15,332:INFO:   <<< train_frame_order: 0
2024-04-27 07:25:15,332:INFO:   <<< use_mil: False
2024-04-27 07:25:15,332:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 07:25:15,332:INFO:   <<< video_dim: 1024
2024-04-27 07:25:15,332:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 07:25:15,332:INFO:   <<< warmup_proportion: 0.1
2024-04-27 07:25:15,332:INFO:   <<< world_size: 1
2024-04-27 07:25:15,332:INFO: device: cuda:0 n_gpu: 1
2024-04-27 07:25:22,883:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 07:25:22,897:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 07:25:23,065:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 07:25:23,066:WARNING: 	 embed_dim: 512
2024-04-27 07:25:23,066:WARNING: 	 image_resolution: 224
2024-04-27 07:25:23,066:WARNING: 	 vision_layers: 12
2024-04-27 07:25:23,067:WARNING: 	 vision_width: 768
2024-04-27 07:25:23,067:WARNING: 	 vision_patch_size: 32
2024-04-27 07:25:23,067:WARNING: 	 context_length: 77
2024-04-27 07:25:23,067:WARNING: 	 vocab_size: 49408
2024-04-27 07:25:23,068:WARNING: 	 transformer_width: 512
2024-04-27 07:25:23,068:WARNING: 	 transformer_heads: 8
2024-04-27 07:25:23,068:WARNING: 	 transformer_layers: 12
2024-04-27 07:25:23,068:WARNING: 		 linear_patch: 2d
2024-04-27 07:25:23,068:WARNING: 	 cut_top_layer: 0
2024-04-27 07:25:23,943:WARNING: 	 sim_header: meanP
2024-04-27 07:25:32,736:WARNING: Set cross_config.num_hidden_layers: 4.
2024-04-27 07:26:24,764:INFO: Effective parameters:
2024-04-27 07:26:24,765:INFO:   <<< batch_size: 16
2024-04-27 07:26:24,765:INFO:   <<< batch_size_val: 16
2024-04-27 07:26:24,765:INFO:   <<< cache_dir: 
2024-04-27 07:26:24,765:INFO:   <<< coef_lr: 0.001
2024-04-27 07:26:24,765:INFO:   <<< cross_model: cross-base
2024-04-27 07:26:24,765:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 07:26:24,765:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 07:26:24,765:INFO:   <<< datatype: msrvtt
2024-04-27 07:26:24,765:INFO:   <<< do_eval: True
2024-04-27 07:26:24,765:INFO:   <<< do_lower_case: False
2024-04-27 07:26:24,765:INFO:   <<< do_pretrain: False
2024-04-27 07:26:24,765:INFO:   <<< do_train: False
2024-04-27 07:26:24,765:INFO:   <<< epochs: 5
2024-04-27 07:26:24,766:INFO:   <<< eval_frame_order: 0
2024-04-27 07:26:24,766:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 07:26:24,766:INFO:   <<< feature_framerate: 1
2024-04-27 07:26:24,766:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 07:26:24,766:INFO:   <<< fp16: False
2024-04-27 07:26:24,766:INFO:   <<< fp16_opt_level: O1
2024-04-27 07:26:24,766:INFO:   <<< freeze_layer_num: 0
2024-04-27 07:26:24,766:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 07:26:24,766:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 07:26:24,766:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 07:26:24,766:INFO:   <<< linear_patch: 2d
2024-04-27 07:26:24,766:INFO:   <<< local_rank: 0
2024-04-27 07:26:24,766:INFO:   <<< loose_type: False
2024-04-27 07:26:24,766:INFO:   <<< lr: 0.0001
2024-04-27 07:26:24,766:INFO:   <<< lr_decay: 0.9
2024-04-27 07:26:24,766:INFO:   <<< margin: 0.1
2024-04-27 07:26:24,766:INFO:   <<< max_frames: 12
2024-04-27 07:26:24,766:INFO:   <<< max_words: 32
2024-04-27 07:26:24,767:INFO:   <<< n_display: 100
2024-04-27 07:26:24,767:INFO:   <<< n_gpu: 1
2024-04-27 07:26:24,767:INFO:   <<< n_pair: 1
2024-04-27 07:26:24,767:INFO:   <<< negative_weighting: 1
2024-04-27 07:26:24,767:INFO:   <<< num_thread_reader: 0
2024-04-27 07:26:24,767:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 07:26:24,767:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 07:26:24,767:INFO:   <<< rank: 0
2024-04-27 07:26:24,767:INFO:   <<< resume_model: None
2024-04-27 07:26:24,767:INFO:   <<< sampled_use_mil: False
2024-04-27 07:26:24,767:INFO:   <<< seed: 42
2024-04-27 07:26:24,767:INFO:   <<< sim_header: meanP
2024-04-27 07:26:24,767:INFO:   <<< slice_framepos: 2
2024-04-27 07:26:24,767:INFO:   <<< task_type: retrieval
2024-04-27 07:26:24,767:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 07:26:24,767:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 07:26:24,767:INFO:   <<< train_frame_order: 0
2024-04-27 07:26:24,768:INFO:   <<< use_mil: False
2024-04-27 07:26:24,768:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 07:26:24,768:INFO:   <<< video_dim: 1024
2024-04-27 07:26:24,768:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 07:26:24,768:INFO:   <<< warmup_proportion: 0.1
2024-04-27 07:26:24,768:INFO:   <<< world_size: 1
2024-04-27 07:26:24,768:INFO: device: cuda:0 n_gpu: 1
2024-04-27 07:26:25,536:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 07:26:25,545:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 07:26:25,699:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 07:26:25,700:WARNING: 	 embed_dim: 512
2024-04-27 07:26:25,700:WARNING: 	 image_resolution: 224
2024-04-27 07:26:25,700:WARNING: 	 vision_layers: 12
2024-04-27 07:26:25,700:WARNING: 	 vision_width: 768
2024-04-27 07:26:25,700:WARNING: 	 vision_patch_size: 32
2024-04-27 07:26:25,701:WARNING: 	 context_length: 77
2024-04-27 07:26:25,701:WARNING: 	 vocab_size: 49408
2024-04-27 07:26:25,701:WARNING: 	 transformer_width: 512
2024-04-27 07:26:25,701:WARNING: 	 transformer_heads: 8
2024-04-27 07:26:25,701:WARNING: 	 transformer_layers: 12
2024-04-27 07:26:25,701:WARNING: 		 linear_patch: 2d
2024-04-27 07:26:25,701:WARNING: 	 cut_top_layer: 0
2024-04-27 07:26:26,541:WARNING: 	 sim_header: meanP
2024-04-27 07:26:30,200:WARNING: Set cross_config.num_hidden_layers: 4.
2024-04-27 07:32:17,862:INFO: Effective parameters:
2024-04-27 07:32:17,862:INFO:   <<< batch_size: 16
2024-04-27 07:32:17,862:INFO:   <<< batch_size_val: 16
2024-04-27 07:32:17,862:INFO:   <<< cache_dir: 
2024-04-27 07:32:17,862:INFO:   <<< coef_lr: 0.001
2024-04-27 07:32:17,862:INFO:   <<< cross_model: cross-base
2024-04-27 07:32:17,862:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 07:32:17,862:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 07:32:17,862:INFO:   <<< datatype: msrvtt
2024-04-27 07:32:17,862:INFO:   <<< do_eval: True
2024-04-27 07:32:17,863:INFO:   <<< do_lower_case: False
2024-04-27 07:32:17,863:INFO:   <<< do_pretrain: False
2024-04-27 07:32:17,863:INFO:   <<< do_train: False
2024-04-27 07:32:17,863:INFO:   <<< epochs: 5
2024-04-27 07:32:17,863:INFO:   <<< eval_frame_order: 0
2024-04-27 07:32:17,863:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 07:32:17,863:INFO:   <<< feature_framerate: 1
2024-04-27 07:32:17,863:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 07:32:17,863:INFO:   <<< fp16: False
2024-04-27 07:32:17,863:INFO:   <<< fp16_opt_level: O1
2024-04-27 07:32:17,863:INFO:   <<< freeze_layer_num: 0
2024-04-27 07:32:17,863:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 07:32:17,863:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 07:32:17,863:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 07:32:17,863:INFO:   <<< linear_patch: 2d
2024-04-27 07:32:17,863:INFO:   <<< local_rank: 0
2024-04-27 07:32:17,863:INFO:   <<< loose_type: False
2024-04-27 07:32:17,863:INFO:   <<< lr: 0.0001
2024-04-27 07:32:17,863:INFO:   <<< lr_decay: 0.9
2024-04-27 07:32:17,863:INFO:   <<< margin: 0.1
2024-04-27 07:32:17,863:INFO:   <<< max_frames: 12
2024-04-27 07:32:17,863:INFO:   <<< max_words: 32
2024-04-27 07:32:17,863:INFO:   <<< n_display: 100
2024-04-27 07:32:17,863:INFO:   <<< n_gpu: 1
2024-04-27 07:32:17,863:INFO:   <<< n_pair: 1
2024-04-27 07:32:17,863:INFO:   <<< negative_weighting: 1
2024-04-27 07:32:17,863:INFO:   <<< num_thread_reader: 0
2024-04-27 07:32:17,863:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 07:32:17,863:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 07:32:17,863:INFO:   <<< rank: 0
2024-04-27 07:32:17,863:INFO:   <<< resume_model: None
2024-04-27 07:32:17,863:INFO:   <<< sampled_use_mil: False
2024-04-27 07:32:17,863:INFO:   <<< seed: 42
2024-04-27 07:32:17,863:INFO:   <<< sim_header: meanP
2024-04-27 07:32:17,863:INFO:   <<< slice_framepos: 2
2024-04-27 07:32:17,863:INFO:   <<< task_type: retrieval
2024-04-27 07:32:17,863:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 07:32:17,863:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 07:32:17,863:INFO:   <<< train_frame_order: 0
2024-04-27 07:32:17,863:INFO:   <<< use_mil: False
2024-04-27 07:32:17,863:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 07:32:17,863:INFO:   <<< video_dim: 1024
2024-04-27 07:32:17,863:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 07:32:17,863:INFO:   <<< warmup_proportion: 0.1
2024-04-27 07:32:17,863:INFO:   <<< world_size: 1
2024-04-27 07:32:17,863:INFO: device: cuda:0 n_gpu: 1
2024-04-27 07:32:18,078:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 07:32:18,078:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 07:32:18,232:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 07:32:25,694:INFO: Effective parameters:
2024-04-27 07:32:25,694:INFO:   <<< batch_size: 16
2024-04-27 07:32:25,694:INFO:   <<< batch_size_val: 16
2024-04-27 07:32:25,694:INFO:   <<< cache_dir: 
2024-04-27 07:32:25,694:INFO:   <<< coef_lr: 0.001
2024-04-27 07:32:25,694:INFO:   <<< cross_model: cross-base
2024-04-27 07:32:25,694:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 07:32:25,694:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 07:32:25,694:INFO:   <<< datatype: msrvtt
2024-04-27 07:32:25,694:INFO:   <<< do_eval: True
2024-04-27 07:32:25,694:INFO:   <<< do_lower_case: False
2024-04-27 07:32:25,694:INFO:   <<< do_pretrain: False
2024-04-27 07:32:25,694:INFO:   <<< do_train: False
2024-04-27 07:32:25,694:INFO:   <<< epochs: 5
2024-04-27 07:32:25,694:INFO:   <<< eval_frame_order: 0
2024-04-27 07:32:25,694:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 07:32:25,694:INFO:   <<< feature_framerate: 1
2024-04-27 07:32:25,694:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 07:32:25,694:INFO:   <<< fp16: False
2024-04-27 07:32:25,694:INFO:   <<< fp16_opt_level: O1
2024-04-27 07:32:25,694:INFO:   <<< freeze_layer_num: 0
2024-04-27 07:32:25,694:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 07:32:25,694:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 07:32:25,694:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 07:32:25,694:INFO:   <<< linear_patch: 2d
2024-04-27 07:32:25,694:INFO:   <<< local_rank: 0
2024-04-27 07:32:25,694:INFO:   <<< loose_type: False
2024-04-27 07:32:25,694:INFO:   <<< lr: 0.0001
2024-04-27 07:32:25,694:INFO:   <<< lr_decay: 0.9
2024-04-27 07:32:25,694:INFO:   <<< margin: 0.1
2024-04-27 07:32:25,694:INFO:   <<< max_frames: 12
2024-04-27 07:32:25,694:INFO:   <<< max_words: 32
2024-04-27 07:32:25,694:INFO:   <<< n_display: 100
2024-04-27 07:32:25,694:INFO:   <<< n_gpu: 1
2024-04-27 07:32:25,694:INFO:   <<< n_pair: 1
2024-04-27 07:32:25,694:INFO:   <<< negative_weighting: 1
2024-04-27 07:32:25,694:INFO:   <<< num_thread_reader: 0
2024-04-27 07:32:25,694:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 07:32:25,694:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 07:32:25,694:INFO:   <<< rank: 0
2024-04-27 07:32:25,694:INFO:   <<< resume_model: None
2024-04-27 07:32:25,695:INFO:   <<< sampled_use_mil: False
2024-04-27 07:32:25,695:INFO:   <<< seed: 42
2024-04-27 07:32:25,695:INFO:   <<< sim_header: meanP
2024-04-27 07:32:25,695:INFO:   <<< slice_framepos: 2
2024-04-27 07:32:25,695:INFO:   <<< task_type: retrieval
2024-04-27 07:32:25,695:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 07:32:25,695:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 07:32:25,695:INFO:   <<< train_frame_order: 0
2024-04-27 07:32:25,695:INFO:   <<< use_mil: False
2024-04-27 07:32:25,695:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 07:32:25,695:INFO:   <<< video_dim: 1024
2024-04-27 07:32:25,695:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 07:32:25,695:INFO:   <<< warmup_proportion: 0.1
2024-04-27 07:32:25,695:INFO:   <<< world_size: 1
2024-04-27 07:32:25,695:INFO: device: cuda:0 n_gpu: 1
2024-04-27 07:32:25,918:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 07:32:25,919:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 07:32:26,074:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 07:33:07,462:INFO: Effective parameters:
2024-04-27 07:33:07,462:INFO:   <<< batch_size: 16
2024-04-27 07:33:07,462:INFO:   <<< batch_size_val: 16
2024-04-27 07:33:07,462:INFO:   <<< cache_dir: 
2024-04-27 07:33:07,462:INFO:   <<< coef_lr: 0.001
2024-04-27 07:33:07,462:INFO:   <<< cross_model: cross-base
2024-04-27 07:33:07,462:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 07:33:07,462:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 07:33:07,462:INFO:   <<< datatype: msrvtt
2024-04-27 07:33:07,462:INFO:   <<< do_eval: True
2024-04-27 07:33:07,462:INFO:   <<< do_lower_case: False
2024-04-27 07:33:07,462:INFO:   <<< do_pretrain: False
2024-04-27 07:33:07,462:INFO:   <<< do_train: False
2024-04-27 07:33:07,462:INFO:   <<< epochs: 5
2024-04-27 07:33:07,462:INFO:   <<< eval_frame_order: 0
2024-04-27 07:33:07,462:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 07:33:07,462:INFO:   <<< feature_framerate: 1
2024-04-27 07:33:07,462:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 07:33:07,462:INFO:   <<< fp16: False
2024-04-27 07:33:07,462:INFO:   <<< fp16_opt_level: O1
2024-04-27 07:33:07,462:INFO:   <<< freeze_layer_num: 0
2024-04-27 07:33:07,462:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 07:33:07,462:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 07:33:07,462:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 07:33:07,462:INFO:   <<< linear_patch: 2d
2024-04-27 07:33:07,462:INFO:   <<< local_rank: 0
2024-04-27 07:33:07,462:INFO:   <<< loose_type: False
2024-04-27 07:33:07,462:INFO:   <<< lr: 0.0001
2024-04-27 07:33:07,462:INFO:   <<< lr_decay: 0.9
2024-04-27 07:33:07,462:INFO:   <<< margin: 0.1
2024-04-27 07:33:07,462:INFO:   <<< max_frames: 12
2024-04-27 07:33:07,462:INFO:   <<< max_words: 32
2024-04-27 07:33:07,462:INFO:   <<< n_display: 100
2024-04-27 07:33:07,462:INFO:   <<< n_gpu: 1
2024-04-27 07:33:07,462:INFO:   <<< n_pair: 1
2024-04-27 07:33:07,462:INFO:   <<< negative_weighting: 1
2024-04-27 07:33:07,462:INFO:   <<< num_thread_reader: 0
2024-04-27 07:33:07,462:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 07:33:07,462:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 07:33:07,462:INFO:   <<< rank: 0
2024-04-27 07:33:07,462:INFO:   <<< resume_model: None
2024-04-27 07:33:07,462:INFO:   <<< sampled_use_mil: False
2024-04-27 07:33:07,462:INFO:   <<< seed: 42
2024-04-27 07:33:07,462:INFO:   <<< sim_header: meanP
2024-04-27 07:33:07,462:INFO:   <<< slice_framepos: 2
2024-04-27 07:33:07,462:INFO:   <<< task_type: retrieval
2024-04-27 07:33:07,462:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 07:33:07,462:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 07:33:07,463:INFO:   <<< train_frame_order: 0
2024-04-27 07:33:07,463:INFO:   <<< use_mil: False
2024-04-27 07:33:07,463:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 07:33:07,463:INFO:   <<< video_dim: 1024
2024-04-27 07:33:07,463:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 07:33:07,463:INFO:   <<< warmup_proportion: 0.1
2024-04-27 07:33:07,463:INFO:   <<< world_size: 1
2024-04-27 07:33:07,463:INFO: device: cuda:0 n_gpu: 1
2024-04-27 07:33:07,678:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 07:33:07,678:INFO: Model config {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "logit_scale_init_value": 2.6592,
  "max_position_embeddings": 512,
  "model_type": "clip",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "projection_dim": 512,
  "text_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": 0,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": 2,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 512,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 2048,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "max_position_embeddings": 77,
    "min_length": 0,
    "model_type": "clip_text_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 8,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": 1,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false,
    "vocab_size": 49408
  },
  "text_config_dict": null,
  "transformers_version": null,
  "type_vocab_size": 2,
  "vision_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "attention_dropout": 0.0,
    "bad_words_ids": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "diversity_penalty": 0.0,
    "do_sample": false,
    "dropout": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "hidden_act": "quick_gelu",
    "hidden_size": 768,
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "image_size": 224,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_norm_eps": 1e-05,
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "clip_vision_model",
    "no_repeat_ngram_size": 0,
    "num_attention_heads": 12,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_hidden_layers": 12,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "pad_token_id": null,
    "patch_size": 32,
    "prefix": null,
    "problem_type": null,
    "projection_dim": 512,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.16.0.dev0",
    "use_bfloat16": false
  },
  "vision_config_dict": null,
  "vocab_size": -1
}

2024-04-27 07:33:07,834:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 08:03:54,669:INFO: Effective parameters:
2024-04-27 08:03:54,669:INFO:   <<< batch_size: 16
2024-04-27 08:03:54,669:INFO:   <<< batch_size_val: 16
2024-04-27 08:03:54,669:INFO:   <<< cache_dir: 
2024-04-27 08:03:54,669:INFO:   <<< coef_lr: 0.001
2024-04-27 08:03:54,669:INFO:   <<< cross_model: cross-base
2024-04-27 08:03:54,669:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:03:54,669:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:03:54,670:INFO:   <<< datatype: msrvtt
2024-04-27 08:03:54,670:INFO:   <<< do_eval: True
2024-04-27 08:03:54,670:INFO:   <<< do_lower_case: False
2024-04-27 08:03:54,670:INFO:   <<< do_pretrain: False
2024-04-27 08:03:54,670:INFO:   <<< do_train: False
2024-04-27 08:03:54,670:INFO:   <<< epochs: 2
2024-04-27 08:03:54,670:INFO:   <<< eval_frame_order: 0
2024-04-27 08:03:54,670:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:03:54,670:INFO:   <<< feature_framerate: 1
2024-04-27 08:03:54,670:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:03:54,670:INFO:   <<< fp16: False
2024-04-27 08:03:54,670:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:03:54,670:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:03:54,670:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:03:54,670:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:03:54,670:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin4
2024-04-27 08:03:54,670:INFO:   <<< linear_patch: 2d
2024-04-27 08:03:54,670:INFO:   <<< local_rank: 0
2024-04-27 08:03:54,670:INFO:   <<< loose_type: True
2024-04-27 08:03:54,670:INFO:   <<< lr: 0.0001
2024-04-27 08:03:54,670:INFO:   <<< lr_decay: 0.9
2024-04-27 08:03:54,670:INFO:   <<< margin: 0.1
2024-04-27 08:03:54,670:INFO:   <<< max_frames: 12
2024-04-27 08:03:54,670:INFO:   <<< max_words: 32
2024-04-27 08:03:54,670:INFO:   <<< n_display: 100
2024-04-27 08:03:54,670:INFO:   <<< n_gpu: 1
2024-04-27 08:03:54,670:INFO:   <<< n_pair: 1
2024-04-27 08:03:54,670:INFO:   <<< negative_weighting: 1
2024-04-27 08:03:54,670:INFO:   <<< num_thread_reader: 0
2024-04-27 08:03:54,670:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:03:54,670:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:03:54,670:INFO:   <<< rank: 0
2024-04-27 08:03:54,670:INFO:   <<< resume_model: None
2024-04-27 08:03:54,670:INFO:   <<< sampled_use_mil: False
2024-04-27 08:03:54,670:INFO:   <<< seed: 42
2024-04-27 08:03:54,670:INFO:   <<< sim_header: meanP
2024-04-27 08:03:54,670:INFO:   <<< slice_framepos: 2
2024-04-27 08:03:54,670:INFO:   <<< task_type: retrieval
2024-04-27 08:03:54,670:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:03:54,670:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:03:54,670:INFO:   <<< train_frame_order: 0
2024-04-27 08:03:54,670:INFO:   <<< use_mil: False
2024-04-27 08:03:54,670:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:03:54,670:INFO:   <<< video_dim: 1024
2024-04-27 08:03:54,670:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:03:54,670:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:03:54,671:INFO:   <<< world_size: 1
2024-04-27 08:03:54,671:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:04:12,341:INFO: Effective parameters:
2024-04-27 08:04:12,341:INFO:   <<< batch_size: 16
2024-04-27 08:04:12,341:INFO:   <<< batch_size_val: 16
2024-04-27 08:04:12,341:INFO:   <<< cache_dir: 
2024-04-27 08:04:12,341:INFO:   <<< coef_lr: 0.001
2024-04-27 08:04:12,341:INFO:   <<< cross_model: cross-base
2024-04-27 08:04:12,341:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:04:12,341:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:04:12,341:INFO:   <<< datatype: msrvtt
2024-04-27 08:04:12,341:INFO:   <<< do_eval: True
2024-04-27 08:04:12,341:INFO:   <<< do_lower_case: False
2024-04-27 08:04:12,341:INFO:   <<< do_pretrain: False
2024-04-27 08:04:12,341:INFO:   <<< do_train: False
2024-04-27 08:04:12,341:INFO:   <<< epochs: 2
2024-04-27 08:04:12,341:INFO:   <<< eval_frame_order: 0
2024-04-27 08:04:12,341:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:04:12,341:INFO:   <<< feature_framerate: 1
2024-04-27 08:04:12,341:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:04:12,341:INFO:   <<< fp16: False
2024-04-27 08:04:12,341:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:04:12,341:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:04:12,341:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:04:12,341:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:04:12,341:INFO:   <<< init_model: /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin4
2024-04-27 08:04:12,341:INFO:   <<< linear_patch: 2d
2024-04-27 08:04:12,341:INFO:   <<< local_rank: 0
2024-04-27 08:04:12,341:INFO:   <<< loose_type: True
2024-04-27 08:04:12,341:INFO:   <<< lr: 0.0001
2024-04-27 08:04:12,341:INFO:   <<< lr_decay: 0.9
2024-04-27 08:04:12,341:INFO:   <<< margin: 0.1
2024-04-27 08:04:12,341:INFO:   <<< max_frames: 12
2024-04-27 08:04:12,341:INFO:   <<< max_words: 32
2024-04-27 08:04:12,341:INFO:   <<< n_display: 100
2024-04-27 08:04:12,341:INFO:   <<< n_gpu: 1
2024-04-27 08:04:12,341:INFO:   <<< n_pair: 1
2024-04-27 08:04:12,341:INFO:   <<< negative_weighting: 1
2024-04-27 08:04:12,341:INFO:   <<< num_thread_reader: 0
2024-04-27 08:04:12,341:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:04:12,341:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:04:12,341:INFO:   <<< rank: 0
2024-04-27 08:04:12,341:INFO:   <<< resume_model: None
2024-04-27 08:04:12,341:INFO:   <<< sampled_use_mil: False
2024-04-27 08:04:12,342:INFO:   <<< seed: 42
2024-04-27 08:04:12,342:INFO:   <<< sim_header: meanP
2024-04-27 08:04:12,342:INFO:   <<< slice_framepos: 2
2024-04-27 08:04:12,342:INFO:   <<< task_type: retrieval
2024-04-27 08:04:12,342:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:04:12,342:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:04:12,342:INFO:   <<< train_frame_order: 0
2024-04-27 08:04:12,342:INFO:   <<< use_mil: False
2024-04-27 08:04:12,342:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:04:12,342:INFO:   <<< video_dim: 1024
2024-04-27 08:04:12,342:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:04:12,342:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:04:12,342:INFO:   <<< world_size: 1
2024-04-27 08:04:12,342:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:05:03,238:INFO: Effective parameters:
2024-04-27 08:05:03,238:INFO:   <<< batch_size: 16
2024-04-27 08:05:03,238:INFO:   <<< batch_size_val: 16
2024-04-27 08:05:03,238:INFO:   <<< cache_dir: 
2024-04-27 08:05:03,238:INFO:   <<< coef_lr: 0.001
2024-04-27 08:05:03,238:INFO:   <<< cross_model: cross-base
2024-04-27 08:05:03,238:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:05:03,238:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:05:03,238:INFO:   <<< datatype: msrvtt
2024-04-27 08:05:03,238:INFO:   <<< do_eval: True
2024-04-27 08:05:03,238:INFO:   <<< do_lower_case: False
2024-04-27 08:05:03,239:INFO:   <<< do_pretrain: False
2024-04-27 08:05:03,239:INFO:   <<< do_train: False
2024-04-27 08:05:03,239:INFO:   <<< epochs: 2
2024-04-27 08:05:03,239:INFO:   <<< eval_frame_order: 0
2024-04-27 08:05:03,239:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:05:03,239:INFO:   <<< feature_framerate: 1
2024-04-27 08:05:03,239:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:05:03,239:INFO:   <<< fp16: False
2024-04-27 08:05:03,239:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:05:03,239:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:05:03,239:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:05:03,239:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:05:03,239:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 08:05:03,239:INFO:   <<< linear_patch: 2d
2024-04-27 08:05:03,239:INFO:   <<< local_rank: 0
2024-04-27 08:05:03,239:INFO:   <<< loose_type: True
2024-04-27 08:05:03,239:INFO:   <<< lr: 0.0001
2024-04-27 08:05:03,239:INFO:   <<< lr_decay: 0.9
2024-04-27 08:05:03,240:INFO:   <<< margin: 0.1
2024-04-27 08:05:03,240:INFO:   <<< max_frames: 12
2024-04-27 08:05:03,240:INFO:   <<< max_words: 32
2024-04-27 08:05:03,240:INFO:   <<< n_display: 100
2024-04-27 08:05:03,240:INFO:   <<< n_gpu: 1
2024-04-27 08:05:03,240:INFO:   <<< n_pair: 1
2024-04-27 08:05:03,240:INFO:   <<< negative_weighting: 1
2024-04-27 08:05:03,240:INFO:   <<< num_thread_reader: 0
2024-04-27 08:05:03,240:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:05:03,240:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:05:03,240:INFO:   <<< rank: 0
2024-04-27 08:05:03,240:INFO:   <<< resume_model: None
2024-04-27 08:05:03,240:INFO:   <<< sampled_use_mil: False
2024-04-27 08:05:03,240:INFO:   <<< seed: 42
2024-04-27 08:05:03,240:INFO:   <<< sim_header: meanP
2024-04-27 08:05:03,240:INFO:   <<< slice_framepos: 2
2024-04-27 08:05:03,240:INFO:   <<< task_type: retrieval
2024-04-27 08:05:03,241:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:05:03,241:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:05:03,241:INFO:   <<< train_frame_order: 0
2024-04-27 08:05:03,241:INFO:   <<< use_mil: False
2024-04-27 08:05:03,241:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:05:03,241:INFO:   <<< video_dim: 1024
2024-04-27 08:05:03,241:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:05:03,241:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:05:03,241:INFO:   <<< world_size: 1
2024-04-27 08:05:03,241:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:05:03,931:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 08:05:50,304:INFO: Effective parameters:
2024-04-27 08:05:50,304:INFO:   <<< batch_size: 16
2024-04-27 08:05:50,305:INFO:   <<< batch_size_val: 16
2024-04-27 08:05:50,305:INFO:   <<< cache_dir: 
2024-04-27 08:05:50,305:INFO:   <<< coef_lr: 0.001
2024-04-27 08:05:50,305:INFO:   <<< cross_model: cross-base
2024-04-27 08:05:50,305:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:05:50,305:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:05:50,305:INFO:   <<< datatype: msrvtt
2024-04-27 08:05:50,305:INFO:   <<< do_eval: True
2024-04-27 08:05:50,305:INFO:   <<< do_lower_case: False
2024-04-27 08:05:50,305:INFO:   <<< do_pretrain: False
2024-04-27 08:05:50,305:INFO:   <<< do_train: False
2024-04-27 08:05:50,305:INFO:   <<< epochs: 2
2024-04-27 08:05:50,305:INFO:   <<< eval_frame_order: 0
2024-04-27 08:05:50,305:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:05:50,305:INFO:   <<< feature_framerate: 1
2024-04-27 08:05:50,305:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:05:50,306:INFO:   <<< fp16: False
2024-04-27 08:05:50,306:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:05:50,306:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:05:50,306:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:05:50,306:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:05:50,306:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 08:05:50,306:INFO:   <<< linear_patch: 2d
2024-04-27 08:05:50,306:INFO:   <<< local_rank: 0
2024-04-27 08:05:50,306:INFO:   <<< loose_type: True
2024-04-27 08:05:50,306:INFO:   <<< lr: 0.0001
2024-04-27 08:05:50,306:INFO:   <<< lr_decay: 0.9
2024-04-27 08:05:50,306:INFO:   <<< margin: 0.1
2024-04-27 08:05:50,306:INFO:   <<< max_frames: 12
2024-04-27 08:05:50,306:INFO:   <<< max_words: 32
2024-04-27 08:05:50,306:INFO:   <<< n_display: 100
2024-04-27 08:05:50,306:INFO:   <<< n_gpu: 1
2024-04-27 08:05:50,306:INFO:   <<< n_pair: 1
2024-04-27 08:05:50,307:INFO:   <<< negative_weighting: 1
2024-04-27 08:05:50,307:INFO:   <<< num_thread_reader: 0
2024-04-27 08:05:50,307:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:05:50,307:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:05:50,307:INFO:   <<< rank: 0
2024-04-27 08:05:50,307:INFO:   <<< resume_model: None
2024-04-27 08:05:50,307:INFO:   <<< sampled_use_mil: False
2024-04-27 08:05:50,307:INFO:   <<< seed: 42
2024-04-27 08:05:50,307:INFO:   <<< sim_header: meanP
2024-04-27 08:05:50,307:INFO:   <<< slice_framepos: 2
2024-04-27 08:05:50,307:INFO:   <<< task_type: retrieval
2024-04-27 08:05:50,307:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:05:50,307:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:05:50,307:INFO:   <<< train_frame_order: 0
2024-04-27 08:05:50,307:INFO:   <<< use_mil: False
2024-04-27 08:05:50,307:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:05:50,307:INFO:   <<< video_dim: 1024
2024-04-27 08:05:50,308:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:05:50,308:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:05:50,308:INFO:   <<< world_size: 1
2024-04-27 08:05:50,308:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:05:51,016:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 08:05:51,018:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 08:05:51,018:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 08:05:51,018:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 08:05:51,018:WARNING: Test retrieval by loose type.
2024-04-27 08:05:51,019:WARNING: 	 embed_dim: 512
2024-04-27 08:05:51,019:WARNING: 	 image_resolution: 224
2024-04-27 08:05:51,019:WARNING: 	 vision_layers: 12
2024-04-27 08:05:51,019:WARNING: 	 vision_width: 768
2024-04-27 08:05:51,019:WARNING: 	 vision_patch_size: 32
2024-04-27 08:05:51,019:WARNING: 	 context_length: 77
2024-04-27 08:05:51,019:WARNING: 	 vocab_size: 49408
2024-04-27 08:05:51,019:WARNING: 	 transformer_width: 512
2024-04-27 08:05:51,019:WARNING: 	 transformer_heads: 8
2024-04-27 08:05:51,019:WARNING: 	 transformer_layers: 12
2024-04-27 08:05:51,019:WARNING: 		 linear_patch: 2d
2024-04-27 08:05:51,019:WARNING: 	 cut_top_layer: 0
2024-04-27 08:05:51,872:WARNING: 	 sim_header: meanP
2024-04-27 08:06:07,488:INFO: Effective parameters:
2024-04-27 08:06:07,489:INFO:   <<< batch_size: 16
2024-04-27 08:06:07,489:INFO:   <<< batch_size_val: 16
2024-04-27 08:06:07,489:INFO:   <<< cache_dir: 
2024-04-27 08:06:07,489:INFO:   <<< coef_lr: 0.001
2024-04-27 08:06:07,489:INFO:   <<< cross_model: cross-base
2024-04-27 08:06:07,489:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:06:07,489:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:06:07,489:INFO:   <<< datatype: msrvtt
2024-04-27 08:06:07,489:INFO:   <<< do_eval: True
2024-04-27 08:06:07,489:INFO:   <<< do_lower_case: False
2024-04-27 08:06:07,489:INFO:   <<< do_pretrain: False
2024-04-27 08:06:07,489:INFO:   <<< do_train: False
2024-04-27 08:06:07,489:INFO:   <<< epochs: 2
2024-04-27 08:06:07,489:INFO:   <<< eval_frame_order: 0
2024-04-27 08:06:07,489:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:06:07,490:INFO:   <<< feature_framerate: 1
2024-04-27 08:06:07,490:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:06:07,490:INFO:   <<< fp16: False
2024-04-27 08:06:07,490:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:06:07,490:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:06:07,490:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:06:07,490:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:06:07,490:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 08:06:07,490:INFO:   <<< linear_patch: 2d
2024-04-27 08:06:07,490:INFO:   <<< local_rank: 0
2024-04-27 08:06:07,490:INFO:   <<< loose_type: True
2024-04-27 08:06:07,490:INFO:   <<< lr: 0.0001
2024-04-27 08:06:07,490:INFO:   <<< lr_decay: 0.9
2024-04-27 08:06:07,490:INFO:   <<< margin: 0.1
2024-04-27 08:06:07,490:INFO:   <<< max_frames: 12
2024-04-27 08:06:07,490:INFO:   <<< max_words: 32
2024-04-27 08:06:07,491:INFO:   <<< n_display: 100
2024-04-27 08:06:07,491:INFO:   <<< n_gpu: 1
2024-04-27 08:06:07,491:INFO:   <<< n_pair: 1
2024-04-27 08:06:07,491:INFO:   <<< negative_weighting: 1
2024-04-27 08:06:07,491:INFO:   <<< num_thread_reader: 0
2024-04-27 08:06:07,491:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:06:07,491:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:06:07,491:INFO:   <<< rank: 0
2024-04-27 08:06:07,491:INFO:   <<< resume_model: None
2024-04-27 08:06:07,491:INFO:   <<< sampled_use_mil: False
2024-04-27 08:06:07,491:INFO:   <<< seed: 42
2024-04-27 08:06:07,491:INFO:   <<< sim_header: meanP
2024-04-27 08:06:07,491:INFO:   <<< slice_framepos: 2
2024-04-27 08:06:07,491:INFO:   <<< task_type: retrieval
2024-04-27 08:06:07,491:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:06:07,491:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:06:07,492:INFO:   <<< train_frame_order: 0
2024-04-27 08:06:07,492:INFO:   <<< use_mil: False
2024-04-27 08:06:07,492:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:06:07,492:INFO:   <<< video_dim: 1024
2024-04-27 08:06:07,492:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:06:07,492:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:06:07,492:INFO:   <<< world_size: 1
2024-04-27 08:06:07,492:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:07:01,796:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 08:07:01,799:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 08:07:01,799:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 08:07:01,800:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 08:07:01,801:WARNING: Test retrieval by loose type.
2024-04-27 08:07:01,802:WARNING: 	 embed_dim: 512
2024-04-27 08:07:01,802:WARNING: 	 image_resolution: 224
2024-04-27 08:07:01,802:WARNING: 	 vision_layers: 12
2024-04-27 08:07:01,802:WARNING: 	 vision_width: 768
2024-04-27 08:07:01,803:WARNING: 	 vision_patch_size: 32
2024-04-27 08:07:01,803:WARNING: 	 context_length: 77
2024-04-27 08:07:01,803:WARNING: 	 vocab_size: 49408
2024-04-27 08:07:01,803:WARNING: 	 transformer_width: 512
2024-04-27 08:07:01,803:WARNING: 	 transformer_heads: 8
2024-04-27 08:07:01,804:WARNING: 	 transformer_layers: 12
2024-04-27 08:07:01,804:WARNING: 		 linear_patch: 2d
2024-04-27 08:07:01,804:WARNING: 	 cut_top_layer: 0
2024-04-27 08:07:02,682:WARNING: 	 sim_header: meanP
2024-04-27 08:07:06,299:INFO: --------------------
2024-04-27 08:07:06,299:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 08:10:01,661:INFO: Effective parameters:
2024-04-27 08:10:01,661:INFO:   <<< batch_size: 16
2024-04-27 08:10:01,661:INFO:   <<< batch_size_val: 16
2024-04-27 08:10:01,661:INFO:   <<< cache_dir: 
2024-04-27 08:10:01,661:INFO:   <<< coef_lr: 0.001
2024-04-27 08:10:01,661:INFO:   <<< cross_model: cross-base
2024-04-27 08:10:01,661:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:10:01,661:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:10:01,661:INFO:   <<< datatype: msrvtt
2024-04-27 08:10:01,661:INFO:   <<< do_eval: True
2024-04-27 08:10:01,661:INFO:   <<< do_lower_case: False
2024-04-27 08:10:01,661:INFO:   <<< do_pretrain: False
2024-04-27 08:10:01,661:INFO:   <<< do_train: False
2024-04-27 08:10:01,661:INFO:   <<< epochs: 2
2024-04-27 08:10:01,661:INFO:   <<< eval_frame_order: 0
2024-04-27 08:10:01,661:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:10:01,661:INFO:   <<< feature_framerate: 1
2024-04-27 08:10:01,661:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:10:01,661:INFO:   <<< fp16: False
2024-04-27 08:10:01,662:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:10:01,662:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:10:01,662:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:10:01,662:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:10:01,662:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 08:10:01,662:INFO:   <<< linear_patch: 2d
2024-04-27 08:10:01,662:INFO:   <<< local_rank: 0
2024-04-27 08:10:01,662:INFO:   <<< loose_type: True
2024-04-27 08:10:01,662:INFO:   <<< lr: 0.0001
2024-04-27 08:10:01,662:INFO:   <<< lr_decay: 0.9
2024-04-27 08:10:01,662:INFO:   <<< margin: 0.1
2024-04-27 08:10:01,662:INFO:   <<< max_frames: 12
2024-04-27 08:10:01,662:INFO:   <<< max_words: 32
2024-04-27 08:10:01,662:INFO:   <<< n_display: 100
2024-04-27 08:10:01,662:INFO:   <<< n_gpu: 1
2024-04-27 08:10:01,662:INFO:   <<< n_pair: 1
2024-04-27 08:10:01,662:INFO:   <<< negative_weighting: 1
2024-04-27 08:10:01,662:INFO:   <<< num_thread_reader: 0
2024-04-27 08:10:01,662:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:10:01,662:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:10:01,662:INFO:   <<< rank: 0
2024-04-27 08:10:01,662:INFO:   <<< resume_model: None
2024-04-27 08:10:01,662:INFO:   <<< sampled_use_mil: False
2024-04-27 08:10:01,662:INFO:   <<< seed: 42
2024-04-27 08:10:01,662:INFO:   <<< sim_header: meanP
2024-04-27 08:10:01,662:INFO:   <<< slice_framepos: 2
2024-04-27 08:10:01,662:INFO:   <<< task_type: retrieval
2024-04-27 08:10:01,662:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:10:01,662:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:10:01,662:INFO:   <<< train_frame_order: 0
2024-04-27 08:10:01,662:INFO:   <<< use_mil: False
2024-04-27 08:10:01,662:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:10:01,662:INFO:   <<< video_dim: 1024
2024-04-27 08:10:01,662:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:10:01,662:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:10:01,662:INFO:   <<< world_size: 1
2024-04-27 08:10:01,662:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:10:02,392:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 08:10:02,392:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 08:10:02,393:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 08:10:02,393:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 08:10:02,393:WARNING: Test retrieval by loose type.
2024-04-27 08:10:02,393:WARNING: 	 embed_dim: 512
2024-04-27 08:10:02,393:WARNING: 	 image_resolution: 224
2024-04-27 08:10:02,393:WARNING: 	 vision_layers: 12
2024-04-27 08:10:02,393:WARNING: 	 vision_width: 768
2024-04-27 08:10:02,393:WARNING: 	 vision_patch_size: 32
2024-04-27 08:10:02,393:WARNING: 	 context_length: 77
2024-04-27 08:10:02,393:WARNING: 	 vocab_size: 49408
2024-04-27 08:10:02,393:WARNING: 	 transformer_width: 512
2024-04-27 08:10:02,393:WARNING: 	 transformer_heads: 8
2024-04-27 08:10:02,393:WARNING: 	 transformer_layers: 12
2024-04-27 08:10:02,393:WARNING: 		 linear_patch: 2d
2024-04-27 08:10:02,393:WARNING: 	 cut_top_layer: 0
2024-04-27 08:10:03,216:WARNING: 	 sim_header: meanP
2024-04-27 08:10:06,830:INFO: --------------------
2024-04-27 08:10:06,831:INFO: Weights from pretrained model not used in CLIP4Clip: 
   meta
   state_dict
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 08:10:08,841:INFO: ***** Running test *****
2024-04-27 08:10:08,841:INFO:   Num examples = 1000
2024-04-27 08:10:08,841:INFO:   Batch size = 16
2024-04-27 08:10:08,841:INFO:   Num steps = 63
2024-04-27 08:10:08,841:INFO: ***** Running val *****
2024-04-27 08:10:08,841:INFO:   Num examples = 1000
2024-04-27 08:12:19,098:INFO: sim matrix size: 1000, 1000
2024-04-27 08:12:19,162:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 08:12:19,162:INFO: Text-to-Video:
2024-04-27 08:12:19,162:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 08:12:19,162:INFO: Video-to-Text:
2024-04-27 08:12:19,162:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 08:12:50,964:INFO: Effective parameters:
2024-04-27 08:12:50,964:INFO:   <<< batch_size: 16
2024-04-27 08:12:50,964:INFO:   <<< batch_size_val: 16
2024-04-27 08:12:50,964:INFO:   <<< cache_dir: 
2024-04-27 08:12:50,964:INFO:   <<< coef_lr: 0.001
2024-04-27 08:12:50,964:INFO:   <<< cross_model: cross-base
2024-04-27 08:12:50,964:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 08:12:50,964:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 08:12:50,964:INFO:   <<< datatype: msrvtt
2024-04-27 08:12:50,965:INFO:   <<< do_eval: True
2024-04-27 08:12:50,965:INFO:   <<< do_lower_case: False
2024-04-27 08:12:50,965:INFO:   <<< do_pretrain: False
2024-04-27 08:12:50,965:INFO:   <<< do_train: False
2024-04-27 08:12:50,965:INFO:   <<< epochs: 2
2024-04-27 08:12:50,965:INFO:   <<< eval_frame_order: 0
2024-04-27 08:12:50,965:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 08:12:50,965:INFO:   <<< feature_framerate: 1
2024-04-27 08:12:50,965:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 08:12:50,965:INFO:   <<< fp16: False
2024-04-27 08:12:50,965:INFO:   <<< fp16_opt_level: O1
2024-04-27 08:12:50,965:INFO:   <<< freeze_layer_num: 0
2024-04-27 08:12:50,965:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 08:12:50,965:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 08:12:50,966:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 08:12:50,966:INFO:   <<< linear_patch: 2d
2024-04-27 08:12:50,966:INFO:   <<< local_rank: 0
2024-04-27 08:12:50,966:INFO:   <<< loose_type: True
2024-04-27 08:12:50,966:INFO:   <<< lr: 0.0001
2024-04-27 08:12:50,966:INFO:   <<< lr_decay: 0.9
2024-04-27 08:12:50,966:INFO:   <<< margin: 0.1
2024-04-27 08:12:50,966:INFO:   <<< max_frames: 12
2024-04-27 08:12:50,966:INFO:   <<< max_words: 32
2024-04-27 08:12:50,966:INFO:   <<< n_display: 100
2024-04-27 08:12:50,966:INFO:   <<< n_gpu: 1
2024-04-27 08:12:50,966:INFO:   <<< n_pair: 1
2024-04-27 08:12:50,966:INFO:   <<< negative_weighting: 1
2024-04-27 08:12:50,966:INFO:   <<< num_thread_reader: 0
2024-04-27 08:12:50,967:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 08:12:50,967:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 08:12:50,967:INFO:   <<< rank: 0
2024-04-27 08:12:50,967:INFO:   <<< resume_model: None
2024-04-27 08:12:50,967:INFO:   <<< sampled_use_mil: False
2024-04-27 08:12:50,967:INFO:   <<< seed: 42
2024-04-27 08:12:50,967:INFO:   <<< sim_header: meanP
2024-04-27 08:12:50,967:INFO:   <<< slice_framepos: 2
2024-04-27 08:12:50,967:INFO:   <<< task_type: retrieval
2024-04-27 08:12:50,967:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 08:12:50,967:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 08:12:50,967:INFO:   <<< train_frame_order: 0
2024-04-27 08:12:50,967:INFO:   <<< use_mil: False
2024-04-27 08:12:50,967:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 08:12:50,967:INFO:   <<< video_dim: 1024
2024-04-27 08:12:50,968:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 08:12:50,968:INFO:   <<< warmup_proportion: 0.1
2024-04-27 08:12:50,968:INFO:   <<< world_size: 1
2024-04-27 08:12:50,968:INFO: device: cuda:0 n_gpu: 1
2024-04-27 08:17:36,949:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 08:17:36,951:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 08:17:36,951:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 08:17:44,529:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 08:17:44,530:WARNING: Test retrieval by loose type.
2024-04-27 08:17:44,532:WARNING: 	 embed_dim: 512
2024-04-27 08:17:44,532:WARNING: 	 image_resolution: 224
2024-04-27 08:17:44,533:WARNING: 	 vision_layers: 12
2024-04-27 08:17:44,533:WARNING: 	 vision_width: 768
2024-04-27 08:17:44,533:WARNING: 	 vision_patch_size: 32
2024-04-27 08:17:44,534:WARNING: 	 context_length: 77
2024-04-27 08:17:44,534:WARNING: 	 vocab_size: 49408
2024-04-27 08:17:44,535:WARNING: 	 transformer_width: 512
2024-04-27 08:17:44,535:WARNING: 	 transformer_heads: 8
2024-04-27 08:17:44,535:WARNING: 	 transformer_layers: 12
2024-04-27 08:17:44,536:WARNING: 		 linear_patch: 2d
2024-04-27 08:17:44,536:WARNING: 	 cut_top_layer: 0
2024-04-27 08:17:45,419:WARNING: 	 sim_header: meanP
2024-04-27 08:18:18,709:INFO: --------------------
2024-04-27 08:18:18,709:INFO: Weights from pretrained model not used in CLIP4Clip: 
   meta
   state_dict
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 08:18:58,209:INFO: ***** Running test *****
2024-04-27 08:18:58,210:INFO:   Num examples = 1000
2024-04-27 08:18:58,210:INFO:   Batch size = 16
2024-04-27 08:18:58,210:INFO:   Num steps = 63
2024-04-27 08:18:58,210:INFO: ***** Running val *****
2024-04-27 08:18:58,211:INFO:   Num examples = 1000
2024-04-27 08:22:16,521:INFO: sim matrix size: 1000, 1000
2024-04-27 08:22:16,586:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 08:22:16,586:INFO: Text-to-Video:
2024-04-27 08:22:16,587:INFO: 	>>>  R@1: 30.9 - R@5: 54.2 - R@10: 63.3 - Median R: 4.0 - Mean R: 38.8
2024-04-27 08:22:16,587:INFO: Video-to-Text:
2024-04-27 08:22:16,587:INFO: 	>>>  V2T$R@1: 26.4 - V2T$R@5: 52.0 - V2T$R@10: 62.3 - V2T$Median R: 5.0 - V2T$Mean R: 37.6
2024-04-27 11:47:40,578:INFO: Effective parameters:
2024-04-27 11:47:40,578:INFO:   <<< batch_size: 16
2024-04-27 11:47:40,578:INFO:   <<< batch_size_val: 16
2024-04-27 11:47:40,578:INFO:   <<< cache_dir: 
2024-04-27 11:47:40,578:INFO:   <<< coef_lr: 0.001
2024-04-27 11:47:40,578:INFO:   <<< cross_model: cross-base
2024-04-27 11:47:40,578:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 11:47:40,578:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 11:47:40,578:INFO:   <<< datatype: msrvtt
2024-04-27 11:47:40,578:INFO:   <<< do_eval: True
2024-04-27 11:47:40,578:INFO:   <<< do_lower_case: False
2024-04-27 11:47:40,578:INFO:   <<< do_pretrain: False
2024-04-27 11:47:40,578:INFO:   <<< do_train: False
2024-04-27 11:47:40,579:INFO:   <<< epochs: 2
2024-04-27 11:47:40,579:INFO:   <<< eval_frame_order: 0
2024-04-27 11:47:40,579:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 11:47:40,579:INFO:   <<< feature_framerate: 1
2024-04-27 11:47:40,579:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 11:47:40,579:INFO:   <<< fp16: False
2024-04-27 11:47:40,579:INFO:   <<< fp16_opt_level: O1
2024-04-27 11:47:40,579:INFO:   <<< freeze_layer_num: 0
2024-04-27 11:47:40,579:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 11:47:40,579:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 11:47:40,579:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 11:47:40,579:INFO:   <<< linear_patch: 2d
2024-04-27 11:47:40,579:INFO:   <<< local_rank: 0
2024-04-27 11:47:40,579:INFO:   <<< loose_type: True
2024-04-27 11:47:40,579:INFO:   <<< lr: 0.0001
2024-04-27 11:47:40,579:INFO:   <<< lr_decay: 0.9
2024-04-27 11:47:40,579:INFO:   <<< margin: 0.1
2024-04-27 11:47:40,579:INFO:   <<< max_frames: 12
2024-04-27 11:47:40,579:INFO:   <<< max_words: 32
2024-04-27 11:47:40,579:INFO:   <<< n_display: 100
2024-04-27 11:47:40,579:INFO:   <<< n_gpu: 1
2024-04-27 11:47:40,579:INFO:   <<< n_pair: 1
2024-04-27 11:47:40,579:INFO:   <<< negative_weighting: 1
2024-04-27 11:47:40,579:INFO:   <<< num_thread_reader: 0
2024-04-27 11:47:40,579:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 11:47:40,579:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 11:47:40,579:INFO:   <<< rank: 0
2024-04-27 11:47:40,579:INFO:   <<< resume_model: None
2024-04-27 11:47:40,579:INFO:   <<< sampled_use_mil: False
2024-04-27 11:47:40,579:INFO:   <<< seed: 42
2024-04-27 11:47:40,579:INFO:   <<< sim_header: meanP
2024-04-27 11:47:40,579:INFO:   <<< slice_framepos: 2
2024-04-27 11:47:40,579:INFO:   <<< task_type: retrieval
2024-04-27 11:47:40,579:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 11:47:40,579:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 11:47:40,579:INFO:   <<< train_frame_order: 0
2024-04-27 11:47:40,579:INFO:   <<< use_mil: False
2024-04-27 11:47:40,579:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 11:47:40,579:INFO:   <<< video_dim: 1024
2024-04-27 11:47:40,579:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 11:47:40,579:INFO:   <<< warmup_proportion: 0.1
2024-04-27 11:47:40,579:INFO:   <<< world_size: 1
2024-04-27 11:47:40,579:INFO: device: cuda:0 n_gpu: 1
2024-04-27 11:47:42,630:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 11:47:42,631:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 11:47:42,631:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 11:47:42,631:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 11:47:42,631:WARNING: Test retrieval by loose type.
2024-04-27 11:47:42,631:WARNING: 	 embed_dim: 512
2024-04-27 11:47:42,632:WARNING: 	 image_resolution: 224
2024-04-27 11:47:42,632:WARNING: 	 vision_layers: 12
2024-04-27 11:47:42,632:WARNING: 	 vision_width: 768
2024-04-27 11:47:42,632:WARNING: 	 vision_patch_size: 32
2024-04-27 11:47:42,632:WARNING: 	 context_length: 77
2024-04-27 11:47:42,632:WARNING: 	 vocab_size: 49408
2024-04-27 11:47:42,632:WARNING: 	 transformer_width: 512
2024-04-27 11:47:42,632:WARNING: 	 transformer_heads: 8
2024-04-27 11:47:42,632:WARNING: 	 transformer_layers: 12
2024-04-27 11:47:42,632:WARNING: 		 linear_patch: 2d
2024-04-27 11:47:42,632:WARNING: 	 cut_top_layer: 0
2024-04-27 11:47:43,479:WARNING: 	 sim_header: meanP
2024-04-27 11:47:47,075:INFO: --------------------
2024-04-27 11:47:47,075:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 11:47:51,387:INFO: ***** Running test *****
2024-04-27 11:47:51,387:INFO:   Num examples = 1000
2024-04-27 11:47:51,387:INFO:   Batch size = 16
2024-04-27 11:47:51,387:INFO:   Num steps = 63
2024-04-27 11:47:51,388:INFO: ***** Running val *****
2024-04-27 11:47:51,388:INFO:   Num examples = 1000
2024-04-27 11:50:10,576:INFO: sim matrix size: 1000, 1000
2024-04-27 11:50:10,643:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 11:50:10,644:INFO: Text-to-Video:
2024-04-27 11:50:10,644:INFO: 	>>>  R@1: 35.6 - R@5: 61.2 - R@10: 70.5 - Median R: 3.0 - Mean R: 22.2
2024-04-27 11:50:10,644:INFO: Video-to-Text:
2024-04-27 11:50:10,644:INFO: 	>>>  V2T$R@1: 35.4 - V2T$R@5: 61.9 - V2T$R@10: 73.2 - V2T$Median R: 3.0 - V2T$Mean R: 19.4
2024-04-27 11:50:46,340:INFO: Effective parameters:
2024-04-27 11:50:46,341:INFO:   <<< batch_size: 16
2024-04-27 11:50:46,341:INFO:   <<< batch_size_val: 16
2024-04-27 11:50:46,341:INFO:   <<< cache_dir: 
2024-04-27 11:50:46,341:INFO:   <<< coef_lr: 0.001
2024-04-27 11:50:46,341:INFO:   <<< cross_model: cross-base
2024-04-27 11:50:46,341:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 11:50:46,341:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 11:50:46,341:INFO:   <<< datatype: msrvtt
2024-04-27 11:50:46,341:INFO:   <<< do_eval: True
2024-04-27 11:50:46,341:INFO:   <<< do_lower_case: False
2024-04-27 11:50:46,341:INFO:   <<< do_pretrain: False
2024-04-27 11:50:46,342:INFO:   <<< do_train: False
2024-04-27 11:50:46,342:INFO:   <<< epochs: 2
2024-04-27 11:50:46,342:INFO:   <<< eval_frame_order: 0
2024-04-27 11:50:46,342:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 11:50:46,342:INFO:   <<< feature_framerate: 1
2024-04-27 11:50:46,342:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 11:50:46,342:INFO:   <<< fp16: False
2024-04-27 11:50:46,342:INFO:   <<< fp16_opt_level: O1
2024-04-27 11:50:46,342:INFO:   <<< freeze_layer_num: 0
2024-04-27 11:50:46,342:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 11:50:46,342:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 11:50:46,342:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 11:50:46,342:INFO:   <<< linear_patch: 2d
2024-04-27 11:50:46,342:INFO:   <<< local_rank: 0
2024-04-27 11:50:46,342:INFO:   <<< loose_type: True
2024-04-27 11:50:46,343:INFO:   <<< lr: 0.0001
2024-04-27 11:50:46,343:INFO:   <<< lr_decay: 0.9
2024-04-27 11:50:46,343:INFO:   <<< margin: 0.1
2024-04-27 11:50:46,343:INFO:   <<< max_frames: 12
2024-04-27 11:50:46,343:INFO:   <<< max_words: 32
2024-04-27 11:50:46,343:INFO:   <<< n_display: 100
2024-04-27 11:50:46,343:INFO:   <<< n_gpu: 1
2024-04-27 11:50:46,343:INFO:   <<< n_pair: 1
2024-04-27 11:50:46,343:INFO:   <<< negative_weighting: 1
2024-04-27 11:50:46,343:INFO:   <<< num_thread_reader: 0
2024-04-27 11:50:46,343:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 11:50:46,343:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 11:50:46,343:INFO:   <<< rank: 0
2024-04-27 11:50:46,343:INFO:   <<< resume_model: None
2024-04-27 11:50:46,343:INFO:   <<< sampled_use_mil: False
2024-04-27 11:50:46,343:INFO:   <<< seed: 42
2024-04-27 11:50:46,344:INFO:   <<< sim_header: meanP
2024-04-27 11:50:46,344:INFO:   <<< slice_framepos: 2
2024-04-27 11:50:46,344:INFO:   <<< task_type: retrieval
2024-04-27 11:50:46,344:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 11:50:46,344:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 11:50:46,344:INFO:   <<< train_frame_order: 0
2024-04-27 11:50:46,344:INFO:   <<< use_mil: False
2024-04-27 11:50:46,344:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 11:50:46,344:INFO:   <<< video_dim: 1024
2024-04-27 11:50:46,344:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 11:50:46,344:INFO:   <<< warmup_proportion: 0.1
2024-04-27 11:50:46,344:INFO:   <<< world_size: 1
2024-04-27 11:50:46,344:INFO: device: cuda:0 n_gpu: 1
2024-04-27 11:53:08,854:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 11:53:08,856:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 11:53:08,857:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 11:53:28,614:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 11:53:28,615:WARNING: Test retrieval by loose type.
2024-04-27 11:53:28,617:WARNING: 	 embed_dim: 512
2024-04-27 11:53:28,617:WARNING: 	 image_resolution: 224
2024-04-27 11:53:28,618:WARNING: 	 vision_layers: 12
2024-04-27 11:53:28,618:WARNING: 	 vision_width: 768
2024-04-27 11:53:28,619:WARNING: 	 vision_patch_size: 32
2024-04-27 11:53:28,619:WARNING: 	 context_length: 77
2024-04-27 11:53:28,619:WARNING: 	 vocab_size: 49408
2024-04-27 11:53:28,620:WARNING: 	 transformer_width: 512
2024-04-27 11:53:28,620:WARNING: 	 transformer_heads: 8
2024-04-27 11:53:28,621:WARNING: 	 transformer_layers: 12
2024-04-27 11:53:28,621:WARNING: 		 linear_patch: 2d
2024-04-27 11:53:28,622:WARNING: 	 cut_top_layer: 0
2024-04-27 11:53:29,504:WARNING: 	 sim_header: meanP
2024-04-27 11:54:24,322:INFO: --------------------
2024-04-27 11:54:24,322:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 11:56:31,597:INFO: ***** Running test *****
2024-04-27 11:56:31,597:INFO:   Num examples = 1000
2024-04-27 11:56:31,597:INFO:   Batch size = 16
2024-04-27 11:56:31,598:INFO:   Num steps = 63
2024-04-27 11:56:31,598:INFO: ***** Running val *****
2024-04-27 11:56:31,598:INFO:   Num examples = 1000
2024-04-27 11:57:21,294:INFO: Effective parameters:
2024-04-27 11:57:21,294:INFO:   <<< batch_size: 16
2024-04-27 11:57:21,294:INFO:   <<< batch_size_val: 16
2024-04-27 11:57:21,294:INFO:   <<< cache_dir: 
2024-04-27 11:57:21,294:INFO:   <<< coef_lr: 0.001
2024-04-27 11:57:21,294:INFO:   <<< cross_model: cross-base
2024-04-27 11:57:21,294:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 11:57:21,294:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 11:57:21,294:INFO:   <<< datatype: msrvtt
2024-04-27 11:57:21,294:INFO:   <<< do_eval: True
2024-04-27 11:57:21,294:INFO:   <<< do_lower_case: False
2024-04-27 11:57:21,295:INFO:   <<< do_pretrain: False
2024-04-27 11:57:21,295:INFO:   <<< do_train: False
2024-04-27 11:57:21,295:INFO:   <<< epochs: 2
2024-04-27 11:57:21,295:INFO:   <<< eval_frame_order: 0
2024-04-27 11:57:21,295:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 11:57:21,295:INFO:   <<< feature_framerate: 1
2024-04-27 11:57:21,295:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 11:57:21,295:INFO:   <<< fp16: False
2024-04-27 11:57:21,295:INFO:   <<< fp16_opt_level: O1
2024-04-27 11:57:21,295:INFO:   <<< freeze_layer_num: 0
2024-04-27 11:57:21,295:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 11:57:21,295:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 11:57:21,295:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 11:57:21,295:INFO:   <<< linear_patch: 2d
2024-04-27 11:57:21,295:INFO:   <<< local_rank: 0
2024-04-27 11:57:21,295:INFO:   <<< loose_type: True
2024-04-27 11:57:21,295:INFO:   <<< lr: 0.0001
2024-04-27 11:57:21,296:INFO:   <<< lr_decay: 0.9
2024-04-27 11:57:21,296:INFO:   <<< margin: 0.1
2024-04-27 11:57:21,296:INFO:   <<< max_frames: 12
2024-04-27 11:57:21,296:INFO:   <<< max_words: 32
2024-04-27 11:57:21,296:INFO:   <<< n_display: 100
2024-04-27 11:57:21,296:INFO:   <<< n_gpu: 1
2024-04-27 11:57:21,296:INFO:   <<< n_pair: 1
2024-04-27 11:57:21,296:INFO:   <<< negative_weighting: 1
2024-04-27 11:57:21,296:INFO:   <<< num_thread_reader: 0
2024-04-27 11:57:21,296:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 11:57:21,296:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 11:57:21,296:INFO:   <<< rank: 0
2024-04-27 11:57:21,296:INFO:   <<< resume_model: None
2024-04-27 11:57:21,296:INFO:   <<< sampled_use_mil: False
2024-04-27 11:57:21,296:INFO:   <<< seed: 42
2024-04-27 11:57:21,296:INFO:   <<< sim_header: meanP
2024-04-27 11:57:21,296:INFO:   <<< slice_framepos: 2
2024-04-27 11:57:21,296:INFO:   <<< task_type: retrieval
2024-04-27 11:57:21,297:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 11:57:21,297:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 11:57:21,297:INFO:   <<< train_frame_order: 0
2024-04-27 11:57:21,297:INFO:   <<< use_mil: False
2024-04-27 11:57:21,297:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 11:57:21,297:INFO:   <<< video_dim: 1024
2024-04-27 11:57:21,297:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 11:57:21,297:INFO:   <<< warmup_proportion: 0.1
2024-04-27 11:57:21,297:INFO:   <<< world_size: 1
2024-04-27 11:57:21,297:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:00:49,332:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:00:49,334:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:00:49,334:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:00:58,362:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:00:58,363:WARNING: Test retrieval by loose type.
2024-04-27 12:00:58,364:WARNING: 	 embed_dim: 512
2024-04-27 12:00:58,364:WARNING: 	 image_resolution: 224
2024-04-27 12:00:58,364:WARNING: 	 vision_layers: 12
2024-04-27 12:00:58,365:WARNING: 	 vision_width: 768
2024-04-27 12:00:58,365:WARNING: 	 vision_patch_size: 32
2024-04-27 12:00:58,365:WARNING: 	 context_length: 77
2024-04-27 12:00:58,365:WARNING: 	 vocab_size: 49408
2024-04-27 12:00:58,366:WARNING: 	 transformer_width: 512
2024-04-27 12:00:58,366:WARNING: 	 transformer_heads: 8
2024-04-27 12:00:58,366:WARNING: 	 transformer_layers: 12
2024-04-27 12:00:58,367:WARNING: 		 linear_patch: 2d
2024-04-27 12:00:58,367:WARNING: 	 cut_top_layer: 0
2024-04-27 12:00:59,245:WARNING: 	 sim_header: meanP
2024-04-27 12:05:33,095:INFO: --------------------
2024-04-27 12:05:33,096:INFO: Weights from pretrained model not used in CLIP4Clip: 
   meta
   state_dict
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 12:06:07,555:INFO: Effective parameters:
2024-04-27 12:06:07,555:INFO:   <<< batch_size: 16
2024-04-27 12:06:07,555:INFO:   <<< batch_size_val: 16
2024-04-27 12:06:07,556:INFO:   <<< cache_dir: 
2024-04-27 12:06:07,556:INFO:   <<< coef_lr: 0.001
2024-04-27 12:06:07,556:INFO:   <<< cross_model: cross-base
2024-04-27 12:06:07,556:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:06:07,556:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:06:07,556:INFO:   <<< datatype: msrvtt
2024-04-27 12:06:07,556:INFO:   <<< do_eval: True
2024-04-27 12:06:07,556:INFO:   <<< do_lower_case: False
2024-04-27 12:06:07,556:INFO:   <<< do_pretrain: False
2024-04-27 12:06:07,556:INFO:   <<< do_train: False
2024-04-27 12:06:07,556:INFO:   <<< epochs: 2
2024-04-27 12:06:07,556:INFO:   <<< eval_frame_order: 0
2024-04-27 12:06:07,556:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:06:07,556:INFO:   <<< feature_framerate: 1
2024-04-27 12:06:07,556:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:06:07,556:INFO:   <<< fp16: False
2024-04-27 12:06:07,556:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:06:07,557:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:06:07,557:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:06:07,557:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:06:07,557:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 12:06:07,557:INFO:   <<< linear_patch: 2d
2024-04-27 12:06:07,557:INFO:   <<< local_rank: 0
2024-04-27 12:06:07,557:INFO:   <<< loose_type: True
2024-04-27 12:06:07,557:INFO:   <<< lr: 0.0001
2024-04-27 12:06:07,557:INFO:   <<< lr_decay: 0.9
2024-04-27 12:06:07,557:INFO:   <<< margin: 0.1
2024-04-27 12:06:07,557:INFO:   <<< max_frames: 12
2024-04-27 12:06:07,557:INFO:   <<< max_words: 32
2024-04-27 12:06:07,557:INFO:   <<< n_display: 100
2024-04-27 12:06:07,557:INFO:   <<< n_gpu: 1
2024-04-27 12:06:07,557:INFO:   <<< n_pair: 1
2024-04-27 12:06:07,557:INFO:   <<< negative_weighting: 1
2024-04-27 12:06:07,558:INFO:   <<< num_thread_reader: 0
2024-04-27 12:06:07,558:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:06:07,558:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:06:07,558:INFO:   <<< rank: 0
2024-04-27 12:06:07,558:INFO:   <<< resume_model: None
2024-04-27 12:06:07,558:INFO:   <<< sampled_use_mil: False
2024-04-27 12:06:07,558:INFO:   <<< seed: 42
2024-04-27 12:06:07,558:INFO:   <<< sim_header: meanP
2024-04-27 12:06:07,558:INFO:   <<< slice_framepos: 2
2024-04-27 12:06:07,558:INFO:   <<< task_type: retrieval
2024-04-27 12:06:07,558:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:06:07,558:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:06:07,558:INFO:   <<< train_frame_order: 0
2024-04-27 12:06:07,558:INFO:   <<< use_mil: False
2024-04-27 12:06:07,558:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:06:07,558:INFO:   <<< video_dim: 1024
2024-04-27 12:06:07,558:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:06:07,559:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:06:07,559:INFO:   <<< world_size: 1
2024-04-27 12:06:07,559:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:06:10,327:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:06:10,329:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:06:10,329:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:06:13,556:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:06:13,556:WARNING: Test retrieval by loose type.
2024-04-27 12:06:13,558:WARNING: 	 embed_dim: 512
2024-04-27 12:06:13,558:WARNING: 	 image_resolution: 224
2024-04-27 12:06:13,559:WARNING: 	 vision_layers: 12
2024-04-27 12:06:13,559:WARNING: 	 vision_width: 768
2024-04-27 12:06:13,560:WARNING: 	 vision_patch_size: 32
2024-04-27 12:06:13,560:WARNING: 	 context_length: 77
2024-04-27 12:06:13,560:WARNING: 	 vocab_size: 49408
2024-04-27 12:06:13,561:WARNING: 	 transformer_width: 512
2024-04-27 12:06:13,561:WARNING: 	 transformer_heads: 8
2024-04-27 12:06:13,561:WARNING: 	 transformer_layers: 12
2024-04-27 12:06:13,561:WARNING: 		 linear_patch: 2d
2024-04-27 12:06:13,562:WARNING: 	 cut_top_layer: 0
2024-04-27 12:06:14,450:WARNING: 	 sim_header: meanP
2024-04-27 12:06:18,061:INFO: --------------------
2024-04-27 12:06:18,061:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 12:07:00,805:INFO: Effective parameters:
2024-04-27 12:07:00,805:INFO:   <<< batch_size: 16
2024-04-27 12:07:00,805:INFO:   <<< batch_size_val: 16
2024-04-27 12:07:00,805:INFO:   <<< cache_dir: 
2024-04-27 12:07:00,805:INFO:   <<< coef_lr: 0.001
2024-04-27 12:07:00,805:INFO:   <<< cross_model: cross-base
2024-04-27 12:07:00,805:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:07:00,805:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:07:00,806:INFO:   <<< datatype: msrvtt
2024-04-27 12:07:00,806:INFO:   <<< do_eval: True
2024-04-27 12:07:00,806:INFO:   <<< do_lower_case: False
2024-04-27 12:07:00,806:INFO:   <<< do_pretrain: False
2024-04-27 12:07:00,806:INFO:   <<< do_train: False
2024-04-27 12:07:00,806:INFO:   <<< epochs: 2
2024-04-27 12:07:00,806:INFO:   <<< eval_frame_order: 0
2024-04-27 12:07:00,806:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:07:00,806:INFO:   <<< feature_framerate: 1
2024-04-27 12:07:00,806:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:07:00,806:INFO:   <<< fp16: False
2024-04-27 12:07:00,806:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:07:00,806:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:07:00,806:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:07:00,806:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:07:00,806:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 12:07:00,806:INFO:   <<< linear_patch: 2d
2024-04-27 12:07:00,807:INFO:   <<< local_rank: 0
2024-04-27 12:07:00,807:INFO:   <<< loose_type: True
2024-04-27 12:07:00,807:INFO:   <<< lr: 0.0001
2024-04-27 12:07:00,807:INFO:   <<< lr_decay: 0.9
2024-04-27 12:07:00,807:INFO:   <<< margin: 0.1
2024-04-27 12:07:00,807:INFO:   <<< max_frames: 12
2024-04-27 12:07:00,807:INFO:   <<< max_words: 32
2024-04-27 12:07:00,807:INFO:   <<< n_display: 100
2024-04-27 12:07:00,807:INFO:   <<< n_gpu: 1
2024-04-27 12:07:00,807:INFO:   <<< n_pair: 1
2024-04-27 12:07:00,807:INFO:   <<< negative_weighting: 1
2024-04-27 12:07:00,807:INFO:   <<< num_thread_reader: 0
2024-04-27 12:07:00,807:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:07:00,807:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:07:00,807:INFO:   <<< rank: 0
2024-04-27 12:07:00,807:INFO:   <<< resume_model: None
2024-04-27 12:07:00,807:INFO:   <<< sampled_use_mil: False
2024-04-27 12:07:00,808:INFO:   <<< seed: 42
2024-04-27 12:07:00,808:INFO:   <<< sim_header: meanP
2024-04-27 12:07:00,808:INFO:   <<< slice_framepos: 2
2024-04-27 12:07:00,808:INFO:   <<< task_type: retrieval
2024-04-27 12:07:00,808:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:07:00,808:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:07:00,808:INFO:   <<< train_frame_order: 0
2024-04-27 12:07:00,808:INFO:   <<< use_mil: False
2024-04-27 12:07:00,808:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:07:00,808:INFO:   <<< video_dim: 1024
2024-04-27 12:07:00,808:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:07:00,808:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:07:00,808:INFO:   <<< world_size: 1
2024-04-27 12:07:00,808:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:07:03,884:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:07:03,887:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:07:03,887:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:07:06,394:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:07:06,394:WARNING: Test retrieval by loose type.
2024-04-27 12:07:06,395:WARNING: 	 embed_dim: 512
2024-04-27 12:07:06,396:WARNING: 	 image_resolution: 224
2024-04-27 12:07:06,396:WARNING: 	 vision_layers: 12
2024-04-27 12:07:06,396:WARNING: 	 vision_width: 768
2024-04-27 12:07:06,396:WARNING: 	 vision_patch_size: 32
2024-04-27 12:07:06,396:WARNING: 	 context_length: 77
2024-04-27 12:07:06,397:WARNING: 	 vocab_size: 49408
2024-04-27 12:07:06,397:WARNING: 	 transformer_width: 512
2024-04-27 12:07:06,397:WARNING: 	 transformer_heads: 8
2024-04-27 12:07:06,397:WARNING: 	 transformer_layers: 12
2024-04-27 12:07:06,398:WARNING: 		 linear_patch: 2d
2024-04-27 12:07:06,398:WARNING: 	 cut_top_layer: 0
2024-04-27 12:07:07,291:WARNING: 	 sim_header: meanP
2024-04-27 12:08:15,050:INFO: --------------------
2024-04-27 12:11:01,929:INFO: Effective parameters:
2024-04-27 12:11:01,930:INFO:   <<< batch_size: 16
2024-04-27 12:11:01,930:INFO:   <<< batch_size_val: 16
2024-04-27 12:11:01,930:INFO:   <<< cache_dir: 
2024-04-27 12:11:01,930:INFO:   <<< coef_lr: 0.001
2024-04-27 12:11:01,930:INFO:   <<< cross_model: cross-base
2024-04-27 12:11:01,930:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:11:01,930:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:11:01,930:INFO:   <<< datatype: msrvtt
2024-04-27 12:11:01,930:INFO:   <<< do_eval: True
2024-04-27 12:11:01,930:INFO:   <<< do_lower_case: False
2024-04-27 12:11:01,930:INFO:   <<< do_pretrain: False
2024-04-27 12:11:01,930:INFO:   <<< do_train: False
2024-04-27 12:11:01,930:INFO:   <<< epochs: 2
2024-04-27 12:11:01,930:INFO:   <<< eval_frame_order: 0
2024-04-27 12:11:01,930:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:11:01,930:INFO:   <<< feature_framerate: 1
2024-04-27 12:11:01,931:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:11:01,931:INFO:   <<< fp16: False
2024-04-27 12:11:01,931:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:11:01,931:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:11:01,931:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:11:01,931:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:11:01,931:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/pytorch_model.bin.4
2024-04-27 12:11:01,931:INFO:   <<< linear_patch: 2d
2024-04-27 12:11:01,931:INFO:   <<< local_rank: 0
2024-04-27 12:11:01,931:INFO:   <<< loose_type: True
2024-04-27 12:11:01,931:INFO:   <<< lr: 0.0001
2024-04-27 12:11:01,931:INFO:   <<< lr_decay: 0.9
2024-04-27 12:11:01,931:INFO:   <<< margin: 0.1
2024-04-27 12:11:01,931:INFO:   <<< max_frames: 12
2024-04-27 12:11:01,931:INFO:   <<< max_words: 32
2024-04-27 12:11:01,931:INFO:   <<< n_display: 100
2024-04-27 12:11:01,931:INFO:   <<< n_gpu: 1
2024-04-27 12:11:01,931:INFO:   <<< n_pair: 1
2024-04-27 12:11:01,932:INFO:   <<< negative_weighting: 1
2024-04-27 12:11:01,932:INFO:   <<< num_thread_reader: 0
2024-04-27 12:11:01,932:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:11:01,932:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:11:01,932:INFO:   <<< rank: 0
2024-04-27 12:11:01,932:INFO:   <<< resume_model: None
2024-04-27 12:11:01,932:INFO:   <<< sampled_use_mil: False
2024-04-27 12:11:01,932:INFO:   <<< seed: 42
2024-04-27 12:11:01,932:INFO:   <<< sim_header: meanP
2024-04-27 12:11:01,932:INFO:   <<< slice_framepos: 2
2024-04-27 12:11:01,932:INFO:   <<< task_type: retrieval
2024-04-27 12:11:01,932:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:11:01,932:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:11:01,932:INFO:   <<< train_frame_order: 0
2024-04-27 12:11:01,932:INFO:   <<< use_mil: False
2024-04-27 12:11:01,932:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:11:01,932:INFO:   <<< video_dim: 1024
2024-04-27 12:11:01,933:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:11:01,933:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:11:01,933:INFO:   <<< world_size: 1
2024-04-27 12:11:01,933:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:11:04,676:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:11:04,678:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:11:04,679:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:11:06,224:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:11:06,225:WARNING: Test retrieval by loose type.
2024-04-27 12:11:06,226:WARNING: 	 embed_dim: 512
2024-04-27 12:11:06,226:WARNING: 	 image_resolution: 224
2024-04-27 12:11:06,226:WARNING: 	 vision_layers: 12
2024-04-27 12:11:06,226:WARNING: 	 vision_width: 768
2024-04-27 12:11:06,227:WARNING: 	 vision_patch_size: 32
2024-04-27 12:11:06,227:WARNING: 	 context_length: 77
2024-04-27 12:11:06,227:WARNING: 	 vocab_size: 49408
2024-04-27 12:11:06,227:WARNING: 	 transformer_width: 512
2024-04-27 12:11:06,227:WARNING: 	 transformer_heads: 8
2024-04-27 12:11:06,228:WARNING: 	 transformer_layers: 12
2024-04-27 12:11:06,228:WARNING: 		 linear_patch: 2d
2024-04-27 12:11:06,228:WARNING: 	 cut_top_layer: 0
2024-04-27 12:11:07,103:WARNING: 	 sim_header: meanP
2024-04-27 12:12:04,537:INFO: Effective parameters:
2024-04-27 12:12:04,537:INFO:   <<< batch_size: 16
2024-04-27 12:12:04,537:INFO:   <<< batch_size_val: 16
2024-04-27 12:12:04,537:INFO:   <<< cache_dir: 
2024-04-27 12:12:04,537:INFO:   <<< coef_lr: 0.001
2024-04-27 12:12:04,537:INFO:   <<< cross_model: cross-base
2024-04-27 12:12:04,537:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:12:04,537:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:12:04,537:INFO:   <<< datatype: msrvtt
2024-04-27 12:12:04,538:INFO:   <<< do_eval: True
2024-04-27 12:12:04,538:INFO:   <<< do_lower_case: False
2024-04-27 12:12:04,538:INFO:   <<< do_pretrain: False
2024-04-27 12:12:04,538:INFO:   <<< do_train: False
2024-04-27 12:12:04,538:INFO:   <<< epochs: 2
2024-04-27 12:12:04,538:INFO:   <<< eval_frame_order: 0
2024-04-27 12:12:04,538:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:12:04,538:INFO:   <<< feature_framerate: 1
2024-04-27 12:12:04,538:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:12:04,538:INFO:   <<< fp16: False
2024-04-27 12:12:04,538:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:12:04,538:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:12:04,538:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:12:04,538:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:12:04,538:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 12:12:04,538:INFO:   <<< linear_patch: 2d
2024-04-27 12:12:04,539:INFO:   <<< local_rank: 0
2024-04-27 12:12:04,539:INFO:   <<< loose_type: True
2024-04-27 12:12:04,539:INFO:   <<< lr: 0.0001
2024-04-27 12:12:04,539:INFO:   <<< lr_decay: 0.9
2024-04-27 12:12:04,539:INFO:   <<< margin: 0.1
2024-04-27 12:12:04,539:INFO:   <<< max_frames: 12
2024-04-27 12:12:04,539:INFO:   <<< max_words: 32
2024-04-27 12:12:04,539:INFO:   <<< n_display: 100
2024-04-27 12:12:04,539:INFO:   <<< n_gpu: 1
2024-04-27 12:12:04,539:INFO:   <<< n_pair: 1
2024-04-27 12:12:04,539:INFO:   <<< negative_weighting: 1
2024-04-27 12:12:04,539:INFO:   <<< num_thread_reader: 0
2024-04-27 12:12:04,539:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:12:04,539:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:12:04,539:INFO:   <<< rank: 0
2024-04-27 12:12:04,539:INFO:   <<< resume_model: None
2024-04-27 12:12:04,539:INFO:   <<< sampled_use_mil: False
2024-04-27 12:12:04,540:INFO:   <<< seed: 42
2024-04-27 12:12:04,540:INFO:   <<< sim_header: meanP
2024-04-27 12:12:04,540:INFO:   <<< slice_framepos: 2
2024-04-27 12:12:04,540:INFO:   <<< task_type: retrieval
2024-04-27 12:12:04,540:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:12:04,540:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:12:04,540:INFO:   <<< train_frame_order: 0
2024-04-27 12:12:04,540:INFO:   <<< use_mil: False
2024-04-27 12:12:04,540:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:12:04,540:INFO:   <<< video_dim: 1024
2024-04-27 12:12:04,540:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:12:04,540:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:12:04,540:INFO:   <<< world_size: 1
2024-04-27 12:12:04,540:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:12:06,511:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:12:06,513:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:12:06,513:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:12:08,187:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:12:08,187:WARNING: Test retrieval by loose type.
2024-04-27 12:12:08,189:WARNING: 	 embed_dim: 512
2024-04-27 12:12:08,190:WARNING: 	 image_resolution: 224
2024-04-27 12:12:08,190:WARNING: 	 vision_layers: 12
2024-04-27 12:12:08,191:WARNING: 	 vision_width: 768
2024-04-27 12:12:08,191:WARNING: 	 vision_patch_size: 32
2024-04-27 12:12:08,192:WARNING: 	 context_length: 77
2024-04-27 12:12:08,192:WARNING: 	 vocab_size: 49408
2024-04-27 12:12:08,192:WARNING: 	 transformer_width: 512
2024-04-27 12:12:08,193:WARNING: 	 transformer_heads: 8
2024-04-27 12:12:08,193:WARNING: 	 transformer_layers: 12
2024-04-27 12:12:08,193:WARNING: 		 linear_patch: 2d
2024-04-27 12:12:08,194:WARNING: 	 cut_top_layer: 0
2024-04-27 12:12:09,076:WARNING: 	 sim_header: meanP
2024-04-27 12:13:14,679:INFO: Effective parameters:
2024-04-27 12:13:14,680:INFO:   <<< batch_size: 16
2024-04-27 12:13:14,680:INFO:   <<< batch_size_val: 16
2024-04-27 12:13:14,680:INFO:   <<< cache_dir: 
2024-04-27 12:13:14,680:INFO:   <<< coef_lr: 0.001
2024-04-27 12:13:14,680:INFO:   <<< cross_model: cross-base
2024-04-27 12:13:14,680:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:13:14,680:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:13:14,680:INFO:   <<< datatype: msrvtt
2024-04-27 12:13:14,680:INFO:   <<< do_eval: True
2024-04-27 12:13:14,680:INFO:   <<< do_lower_case: False
2024-04-27 12:13:14,680:INFO:   <<< do_pretrain: False
2024-04-27 12:13:14,680:INFO:   <<< do_train: False
2024-04-27 12:13:14,680:INFO:   <<< epochs: 2
2024-04-27 12:13:14,680:INFO:   <<< eval_frame_order: 0
2024-04-27 12:13:14,681:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:13:14,681:INFO:   <<< feature_framerate: 1
2024-04-27 12:13:14,681:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:13:14,681:INFO:   <<< fp16: False
2024-04-27 12:13:14,681:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:13:14,681:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:13:14,681:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:13:14,681:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:13:14,681:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 12:13:14,681:INFO:   <<< linear_patch: 2d
2024-04-27 12:13:14,681:INFO:   <<< local_rank: 0
2024-04-27 12:13:14,681:INFO:   <<< loose_type: True
2024-04-27 12:13:14,681:INFO:   <<< lr: 0.0001
2024-04-27 12:13:14,681:INFO:   <<< lr_decay: 0.9
2024-04-27 12:13:14,681:INFO:   <<< margin: 0.1
2024-04-27 12:13:14,681:INFO:   <<< max_frames: 12
2024-04-27 12:13:14,681:INFO:   <<< max_words: 32
2024-04-27 12:13:14,682:INFO:   <<< n_display: 100
2024-04-27 12:13:14,682:INFO:   <<< n_gpu: 1
2024-04-27 12:13:14,682:INFO:   <<< n_pair: 1
2024-04-27 12:13:14,682:INFO:   <<< negative_weighting: 1
2024-04-27 12:13:14,682:INFO:   <<< num_thread_reader: 0
2024-04-27 12:13:14,682:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:13:14,682:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:13:14,682:INFO:   <<< rank: 0
2024-04-27 12:13:14,682:INFO:   <<< resume_model: None
2024-04-27 12:13:14,682:INFO:   <<< sampled_use_mil: False
2024-04-27 12:13:14,682:INFO:   <<< seed: 42
2024-04-27 12:13:14,682:INFO:   <<< sim_header: meanP
2024-04-27 12:13:14,682:INFO:   <<< slice_framepos: 2
2024-04-27 12:13:14,682:INFO:   <<< task_type: retrieval
2024-04-27 12:13:14,682:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:13:14,682:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:13:14,682:INFO:   <<< train_frame_order: 0
2024-04-27 12:13:14,682:INFO:   <<< use_mil: False
2024-04-27 12:13:14,683:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:13:14,683:INFO:   <<< video_dim: 1024
2024-04-27 12:13:14,683:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:13:14,683:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:13:14,683:INFO:   <<< world_size: 1
2024-04-27 12:13:14,683:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:18:34,140:INFO: Effective parameters:
2024-04-27 12:18:34,140:INFO:   <<< batch_size: 16
2024-04-27 12:18:34,140:INFO:   <<< batch_size_val: 16
2024-04-27 12:18:34,140:INFO:   <<< cache_dir: 
2024-04-27 12:18:34,141:INFO:   <<< coef_lr: 0.001
2024-04-27 12:18:34,141:INFO:   <<< cross_model: cross-base
2024-04-27 12:18:34,141:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:18:34,141:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:18:34,141:INFO:   <<< datatype: msrvtt
2024-04-27 12:18:34,141:INFO:   <<< do_eval: True
2024-04-27 12:18:34,141:INFO:   <<< do_lower_case: False
2024-04-27 12:18:34,141:INFO:   <<< do_pretrain: False
2024-04-27 12:18:34,141:INFO:   <<< do_train: False
2024-04-27 12:18:34,141:INFO:   <<< epochs: 2
2024-04-27 12:18:34,141:INFO:   <<< eval_frame_order: 0
2024-04-27 12:18:34,141:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:18:34,141:INFO:   <<< feature_framerate: 1
2024-04-27 12:18:34,141:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:18:34,141:INFO:   <<< fp16: False
2024-04-27 12:18:34,141:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:18:34,141:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:18:34,142:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:18:34,142:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:18:34,142:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 12:18:34,142:INFO:   <<< linear_patch: 2d
2024-04-27 12:18:34,142:INFO:   <<< local_rank: 0
2024-04-27 12:18:34,142:INFO:   <<< loose_type: True
2024-04-27 12:18:34,142:INFO:   <<< lr: 0.0001
2024-04-27 12:18:34,142:INFO:   <<< lr_decay: 0.9
2024-04-27 12:18:34,142:INFO:   <<< margin: 0.1
2024-04-27 12:18:34,142:INFO:   <<< max_frames: 12
2024-04-27 12:18:34,142:INFO:   <<< max_words: 32
2024-04-27 12:18:34,142:INFO:   <<< n_display: 100
2024-04-27 12:18:34,142:INFO:   <<< n_gpu: 1
2024-04-27 12:18:34,142:INFO:   <<< n_pair: 1
2024-04-27 12:18:34,142:INFO:   <<< negative_weighting: 1
2024-04-27 12:18:34,142:INFO:   <<< num_thread_reader: 0
2024-04-27 12:18:34,142:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:18:34,143:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:18:34,143:INFO:   <<< rank: 0
2024-04-27 12:18:34,143:INFO:   <<< resume_model: None
2024-04-27 12:18:34,143:INFO:   <<< sampled_use_mil: False
2024-04-27 12:18:34,143:INFO:   <<< seed: 42
2024-04-27 12:18:34,143:INFO:   <<< sim_header: meanP
2024-04-27 12:18:34,143:INFO:   <<< slice_framepos: 2
2024-04-27 12:18:34,143:INFO:   <<< task_type: retrieval
2024-04-27 12:18:34,143:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:18:34,143:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:18:34,143:INFO:   <<< train_frame_order: 0
2024-04-27 12:18:34,143:INFO:   <<< use_mil: False
2024-04-27 12:18:34,143:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:18:34,143:INFO:   <<< video_dim: 1024
2024-04-27 12:18:34,143:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:18:34,143:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:18:34,143:INFO:   <<< world_size: 1
2024-04-27 12:18:34,144:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:19:08,656:INFO: Effective parameters:
2024-04-27 12:19:08,656:INFO:   <<< batch_size: 16
2024-04-27 12:19:08,657:INFO:   <<< batch_size_val: 16
2024-04-27 12:19:08,657:INFO:   <<< cache_dir: 
2024-04-27 12:19:08,657:INFO:   <<< coef_lr: 0.001
2024-04-27 12:19:08,657:INFO:   <<< cross_model: cross-base
2024-04-27 12:19:08,657:INFO:   <<< cross_num_hidden_layers: 4
2024-04-27 12:19:08,657:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-27 12:19:08,657:INFO:   <<< datatype: msrvtt
2024-04-27 12:19:08,657:INFO:   <<< do_eval: True
2024-04-27 12:19:08,657:INFO:   <<< do_lower_case: False
2024-04-27 12:19:08,657:INFO:   <<< do_pretrain: False
2024-04-27 12:19:08,657:INFO:   <<< do_train: False
2024-04-27 12:19:08,657:INFO:   <<< epochs: 2
2024-04-27 12:19:08,657:INFO:   <<< eval_frame_order: 0
2024-04-27 12:19:08,657:INFO:   <<< expand_msrvtt_sentences: True
2024-04-27 12:19:08,657:INFO:   <<< feature_framerate: 1
2024-04-27 12:19:08,657:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-27 12:19:08,658:INFO:   <<< fp16: False
2024-04-27 12:19:08,658:INFO:   <<< fp16_opt_level: O1
2024-04-27 12:19:08,658:INFO:   <<< freeze_layer_num: 0
2024-04-27 12:19:08,658:INFO:   <<< gradient_accumulation_steps: 1
2024-04-27 12:19:08,658:INFO:   <<< hard_negative_rate: 0.5
2024-04-27 12:19:08,658:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-27 12:19:08,658:INFO:   <<< linear_patch: 2d
2024-04-27 12:19:08,658:INFO:   <<< local_rank: 0
2024-04-27 12:19:08,658:INFO:   <<< loose_type: True
2024-04-27 12:19:08,658:INFO:   <<< lr: 0.0001
2024-04-27 12:19:08,658:INFO:   <<< lr_decay: 0.9
2024-04-27 12:19:08,658:INFO:   <<< margin: 0.1
2024-04-27 12:19:08,658:INFO:   <<< max_frames: 12
2024-04-27 12:19:08,658:INFO:   <<< max_words: 32
2024-04-27 12:19:08,658:INFO:   <<< n_display: 100
2024-04-27 12:19:08,658:INFO:   <<< n_gpu: 1
2024-04-27 12:19:08,658:INFO:   <<< n_pair: 1
2024-04-27 12:19:08,659:INFO:   <<< negative_weighting: 1
2024-04-27 12:19:08,659:INFO:   <<< num_thread_reader: 0
2024-04-27 12:19:08,659:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-27 12:19:08,659:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-27 12:19:08,659:INFO:   <<< rank: 0
2024-04-27 12:19:08,659:INFO:   <<< resume_model: None
2024-04-27 12:19:08,659:INFO:   <<< sampled_use_mil: False
2024-04-27 12:19:08,659:INFO:   <<< seed: 42
2024-04-27 12:19:08,659:INFO:   <<< sim_header: meanP
2024-04-27 12:19:08,659:INFO:   <<< slice_framepos: 2
2024-04-27 12:19:08,659:INFO:   <<< task_type: retrieval
2024-04-27 12:19:08,659:INFO:   <<< text_num_hidden_layers: 12
2024-04-27 12:19:08,659:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-27 12:19:08,659:INFO:   <<< train_frame_order: 0
2024-04-27 12:19:08,659:INFO:   <<< use_mil: False
2024-04-27 12:19:08,659:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-27 12:19:08,659:INFO:   <<< video_dim: 1024
2024-04-27 12:19:08,660:INFO:   <<< visual_num_hidden_layers: 12
2024-04-27 12:19:08,660:INFO:   <<< warmup_proportion: 0.1
2024-04-27 12:19:08,660:INFO:   <<< world_size: 1
2024-04-27 12:19:08,660:INFO: device: cuda:0 n_gpu: 1
2024-04-27 12:19:32,651:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-27 12:19:32,653:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-27 12:19:32,653:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-27 12:19:34,537:WARNING: Stage-One:True, Stage-Two:False
2024-04-27 12:19:34,538:WARNING: Test retrieval by loose type.
2024-04-27 12:19:34,539:WARNING: 	 embed_dim: 512
2024-04-27 12:19:34,540:WARNING: 	 image_resolution: 224
2024-04-27 12:19:34,540:WARNING: 	 vision_layers: 12
2024-04-27 12:19:34,540:WARNING: 	 vision_width: 768
2024-04-27 12:19:34,541:WARNING: 	 vision_patch_size: 32
2024-04-27 12:19:34,541:WARNING: 	 context_length: 77
2024-04-27 12:19:34,542:WARNING: 	 vocab_size: 49408
2024-04-27 12:19:34,542:WARNING: 	 transformer_width: 512
2024-04-27 12:19:34,542:WARNING: 	 transformer_heads: 8
2024-04-27 12:19:34,543:WARNING: 	 transformer_layers: 12
2024-04-27 12:19:34,543:WARNING: 		 linear_patch: 2d
2024-04-27 12:19:34,543:WARNING: 	 cut_top_layer: 0
2024-04-27 12:19:35,415:WARNING: 	 sim_header: meanP
2024-04-27 12:19:51,680:INFO: --------------------
2024-04-27 12:19:51,680:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-27 12:20:04,444:INFO: ***** Running test *****
2024-04-27 12:20:04,444:INFO:   Num examples = 1000
2024-04-27 12:20:04,444:INFO:   Batch size = 16
2024-04-27 12:20:04,445:INFO:   Num steps = 63
2024-04-27 12:20:04,445:INFO: ***** Running val *****
2024-04-27 12:20:04,445:INFO:   Num examples = 1000
2024-04-27 12:22:27,060:INFO: sim matrix size: 1000, 1000
2024-04-27 12:22:27,125:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-27 12:22:27,125:INFO: Text-to-Video:
2024-04-27 12:22:27,125:INFO: 	>>>  R@1: 43.3 - R@5: 68.8 - R@10: 78.4 - Median R: 2.0 - Mean R: 16.3
2024-04-27 12:22:27,126:INFO: Video-to-Text:
2024-04-27 12:22:27,126:INFO: 	>>>  V2T$R@1: 40.1 - V2T$R@5: 68.6 - V2T$R@10: 79.3 - V2T$Median R: 2.0 - V2T$Mean R: 13.0
2024-04-28 03:22:08,633:INFO: Effective parameters:
2024-04-28 03:22:08,634:INFO:   <<< batch_size: 16
2024-04-28 03:22:08,634:INFO:   <<< batch_size_val: 16
2024-04-28 03:22:08,634:INFO:   <<< cache_dir: 
2024-04-28 03:22:08,634:INFO:   <<< coef_lr: 0.001
2024-04-28 03:22:08,634:INFO:   <<< cross_model: cross-base
2024-04-28 03:22:08,634:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:22:08,634:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:22:08,634:INFO:   <<< datatype: msrvtt
2024-04-28 03:22:08,634:INFO:   <<< do_eval: True
2024-04-28 03:22:08,634:INFO:   <<< do_lower_case: False
2024-04-28 03:22:08,634:INFO:   <<< do_pretrain: False
2024-04-28 03:22:08,634:INFO:   <<< do_train: False
2024-04-28 03:22:08,634:INFO:   <<< epochs: 2
2024-04-28 03:22:08,634:INFO:   <<< eval_frame_order: 0
2024-04-28 03:22:08,634:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:22:08,634:INFO:   <<< feature_framerate: 1
2024-04-28 03:22:08,634:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:22:08,634:INFO:   <<< fp16: False
2024-04-28 03:22:08,634:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:22:08,634:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:22:08,635:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:22:08,635:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:22:08,635:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:22:08,635:INFO:   <<< linear_patch: 2d
2024-04-28 03:22:08,635:INFO:   <<< local_rank: 0
2024-04-28 03:22:08,635:INFO:   <<< loose_type: True
2024-04-28 03:22:08,635:INFO:   <<< lr: 0.0001
2024-04-28 03:22:08,635:INFO:   <<< lr_decay: 0.9
2024-04-28 03:22:08,635:INFO:   <<< margin: 0.1
2024-04-28 03:22:08,635:INFO:   <<< max_frames: 12
2024-04-28 03:22:08,635:INFO:   <<< max_words: 32
2024-04-28 03:22:08,635:INFO:   <<< n_display: 100
2024-04-28 03:22:08,635:INFO:   <<< n_gpu: 1
2024-04-28 03:22:08,635:INFO:   <<< n_pair: 1
2024-04-28 03:22:08,635:INFO:   <<< negative_weighting: 1
2024-04-28 03:22:08,635:INFO:   <<< num_thread_reader: 0
2024-04-28 03:22:08,635:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:22:08,635:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:22:08,635:INFO:   <<< rank: 0
2024-04-28 03:22:08,635:INFO:   <<< resume_model: None
2024-04-28 03:22:08,635:INFO:   <<< sampled_use_mil: False
2024-04-28 03:22:08,635:INFO:   <<< seed: 42
2024-04-28 03:22:08,635:INFO:   <<< sim_header: meanP
2024-04-28 03:22:08,635:INFO:   <<< slice_framepos: 2
2024-04-28 03:22:08,635:INFO:   <<< task_type: retrieval
2024-04-28 03:22:08,635:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:22:08,635:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:22:08,635:INFO:   <<< train_frame_order: 0
2024-04-28 03:22:08,635:INFO:   <<< use_mil: False
2024-04-28 03:22:08,635:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:22:08,635:INFO:   <<< video_dim: 1024
2024-04-28 03:22:08,635:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:22:08,635:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:22:08,635:INFO:   <<< world_size: 1
2024-04-28 03:22:08,636:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:22:11,214:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:22:11,215:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:22:11,215:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:22:11,215:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:22:11,215:WARNING: Test retrieval by loose type.
2024-04-28 03:22:11,215:WARNING: 	 embed_dim: 512
2024-04-28 03:22:11,215:WARNING: 	 image_resolution: 224
2024-04-28 03:22:11,215:WARNING: 	 vision_layers: 12
2024-04-28 03:22:11,215:WARNING: 	 vision_width: 768
2024-04-28 03:22:11,215:WARNING: 	 vision_patch_size: 32
2024-04-28 03:22:11,215:WARNING: 	 context_length: 77
2024-04-28 03:22:11,215:WARNING: 	 vocab_size: 49408
2024-04-28 03:22:11,216:WARNING: 	 transformer_width: 512
2024-04-28 03:22:11,216:WARNING: 	 transformer_heads: 8
2024-04-28 03:22:11,216:WARNING: 	 transformer_layers: 12
2024-04-28 03:22:11,216:WARNING: 		 linear_patch: 2d
2024-04-28 03:22:11,216:WARNING: 	 cut_top_layer: 0
2024-04-28 03:22:12,085:WARNING: 	 sim_header: meanP
2024-04-28 03:22:15,781:INFO: --------------------
2024-04-28 03:22:15,781:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:22:20,108:INFO: ***** Running test *****
2024-04-28 03:22:20,108:INFO:   Num examples = 1000
2024-04-28 03:22:20,108:INFO:   Batch size = 16
2024-04-28 03:22:20,108:INFO:   Num steps = 63
2024-04-28 03:22:20,108:INFO: ***** Running val *****
2024-04-28 03:22:20,108:INFO:   Num examples = 1000
2024-04-28 03:22:20,258:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:22:20,772:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:22:20,772:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:22:20,772:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:22:20,772:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:22:20,772:WARNING: Test retrieval by loose type.
2024-04-28 03:22:20,772:WARNING: 	 embed_dim: 512
2024-04-28 03:22:20,772:WARNING: 	 image_resolution: 224
2024-04-28 03:22:20,772:WARNING: 	 vision_layers: 12
2024-04-28 03:22:20,773:WARNING: 	 vision_width: 768
2024-04-28 03:22:20,773:WARNING: 	 vision_patch_size: 32
2024-04-28 03:22:20,773:WARNING: 	 context_length: 77
2024-04-28 03:22:20,773:WARNING: 	 vocab_size: 49408
2024-04-28 03:22:20,773:WARNING: 	 transformer_width: 512
2024-04-28 03:22:20,773:WARNING: 	 transformer_heads: 8
2024-04-28 03:22:20,773:WARNING: 	 transformer_layers: 12
2024-04-28 03:22:20,773:WARNING: 		 linear_patch: 2d
2024-04-28 03:22:20,773:WARNING: 	 cut_top_layer: 0
2024-04-28 03:22:21,596:WARNING: 	 sim_header: meanP
2024-04-28 03:22:25,283:INFO: --------------------
2024-04-28 03:22:25,283:INFO: Weights from pretrained model not used in CLIP4Clip: 
   meta
   state_dict
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:24:43,753:INFO: Effective parameters:
2024-04-28 03:24:43,753:INFO:   <<< batch_size: 16
2024-04-28 03:24:43,753:INFO:   <<< batch_size_val: 16
2024-04-28 03:24:43,753:INFO:   <<< cache_dir: 
2024-04-28 03:24:43,753:INFO:   <<< coef_lr: 0.001
2024-04-28 03:24:43,753:INFO:   <<< cross_model: cross-base
2024-04-28 03:24:43,753:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:24:43,753:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:24:43,754:INFO:   <<< datatype: msrvtt
2024-04-28 03:24:43,754:INFO:   <<< do_eval: True
2024-04-28 03:24:43,754:INFO:   <<< do_lower_case: False
2024-04-28 03:24:43,754:INFO:   <<< do_pretrain: False
2024-04-28 03:24:43,754:INFO:   <<< do_train: False
2024-04-28 03:24:43,754:INFO:   <<< epochs: 2
2024-04-28 03:24:43,754:INFO:   <<< eval_frame_order: 0
2024-04-28 03:24:43,754:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:24:43,754:INFO:   <<< feature_framerate: 1
2024-04-28 03:24:43,754:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:24:43,754:INFO:   <<< fp16: False
2024-04-28 03:24:43,754:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:24:43,754:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:24:43,754:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:24:43,754:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:24:43,754:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:24:43,754:INFO:   <<< linear_patch: 2d
2024-04-28 03:24:43,754:INFO:   <<< local_rank: 0
2024-04-28 03:24:43,754:INFO:   <<< loose_type: True
2024-04-28 03:24:43,754:INFO:   <<< lr: 0.0001
2024-04-28 03:24:43,754:INFO:   <<< lr_decay: 0.9
2024-04-28 03:24:43,754:INFO:   <<< margin: 0.1
2024-04-28 03:24:43,754:INFO:   <<< max_frames: 12
2024-04-28 03:24:43,754:INFO:   <<< max_words: 32
2024-04-28 03:24:43,754:INFO:   <<< n_display: 100
2024-04-28 03:24:43,754:INFO:   <<< n_gpu: 1
2024-04-28 03:24:43,754:INFO:   <<< n_pair: 1
2024-04-28 03:24:43,754:INFO:   <<< negative_weighting: 1
2024-04-28 03:24:43,754:INFO:   <<< num_thread_reader: 0
2024-04-28 03:24:43,754:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:24:43,754:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:24:43,754:INFO:   <<< rank: 0
2024-04-28 03:24:43,754:INFO:   <<< resume_model: None
2024-04-28 03:24:43,754:INFO:   <<< sampled_use_mil: False
2024-04-28 03:24:43,754:INFO:   <<< seed: 42
2024-04-28 03:24:43,754:INFO:   <<< sim_header: meanP
2024-04-28 03:24:43,754:INFO:   <<< slice_framepos: 2
2024-04-28 03:24:43,754:INFO:   <<< task_type: retrieval
2024-04-28 03:24:43,754:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:24:43,754:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:24:43,754:INFO:   <<< train_frame_order: 0
2024-04-28 03:24:43,754:INFO:   <<< use_mil: False
2024-04-28 03:24:43,754:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:24:43,754:INFO:   <<< video_dim: 1024
2024-04-28 03:24:43,754:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:24:43,754:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:24:43,754:INFO:   <<< world_size: 1
2024-04-28 03:24:43,754:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:24:44,433:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:24:44,433:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:24:44,433:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:24:44,434:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:24:44,434:WARNING: Test retrieval by loose type.
2024-04-28 03:24:44,434:WARNING: 	 embed_dim: 512
2024-04-28 03:24:44,434:WARNING: 	 image_resolution: 224
2024-04-28 03:24:44,434:WARNING: 	 vision_layers: 12
2024-04-28 03:24:44,434:WARNING: 	 vision_width: 768
2024-04-28 03:24:44,434:WARNING: 	 vision_patch_size: 32
2024-04-28 03:24:44,434:WARNING: 	 context_length: 77
2024-04-28 03:24:44,434:WARNING: 	 vocab_size: 49408
2024-04-28 03:24:44,434:WARNING: 	 transformer_width: 512
2024-04-28 03:24:44,434:WARNING: 	 transformer_heads: 8
2024-04-28 03:24:44,434:WARNING: 	 transformer_layers: 12
2024-04-28 03:24:44,434:WARNING: 		 linear_patch: 2d
2024-04-28 03:24:44,434:WARNING: 	 cut_top_layer: 0
2024-04-28 03:24:45,282:WARNING: 	 sim_header: meanP
2024-04-28 03:24:48,960:INFO: --------------------
2024-04-28 03:24:48,961:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:24:51,051:INFO: ***** Running test *****
2024-04-28 03:24:51,051:INFO:   Num examples = 1000
2024-04-28 03:24:51,051:INFO:   Batch size = 16
2024-04-28 03:24:51,051:INFO:   Num steps = 63
2024-04-28 03:24:51,051:INFO: ***** Running val *****
2024-04-28 03:24:51,051:INFO:   Num examples = 1000
2024-04-28 03:24:51,184:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:24:51,654:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:24:51,655:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:24:51,655:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:24:51,655:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:24:51,655:WARNING: Test retrieval by loose type.
2024-04-28 03:24:51,655:WARNING: 	 embed_dim: 512
2024-04-28 03:24:51,655:WARNING: 	 image_resolution: 224
2024-04-28 03:24:51,655:WARNING: 	 vision_layers: 12
2024-04-28 03:24:51,655:WARNING: 	 vision_width: 768
2024-04-28 03:24:51,655:WARNING: 	 vision_patch_size: 32
2024-04-28 03:24:51,655:WARNING: 	 context_length: 77
2024-04-28 03:24:51,655:WARNING: 	 vocab_size: 49408
2024-04-28 03:24:51,655:WARNING: 	 transformer_width: 512
2024-04-28 03:24:51,655:WARNING: 	 transformer_heads: 8
2024-04-28 03:24:51,656:WARNING: 	 transformer_layers: 12
2024-04-28 03:24:51,656:WARNING: 		 linear_patch: 2d
2024-04-28 03:24:51,656:WARNING: 	 cut_top_layer: 0
2024-04-28 03:24:52,498:WARNING: 	 sim_header: meanP
2024-04-28 03:24:56,183:INFO: --------------------
2024-04-28 03:24:56,184:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:26:00,867:INFO: Effective parameters:
2024-04-28 03:26:00,867:INFO:   <<< batch_size: 16
2024-04-28 03:26:00,867:INFO:   <<< batch_size_val: 16
2024-04-28 03:26:00,867:INFO:   <<< cache_dir: 
2024-04-28 03:26:00,867:INFO:   <<< coef_lr: 0.001
2024-04-28 03:26:00,867:INFO:   <<< cross_model: cross-base
2024-04-28 03:26:00,867:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:26:00,867:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:26:00,867:INFO:   <<< datatype: msrvtt
2024-04-28 03:26:00,867:INFO:   <<< do_eval: True
2024-04-28 03:26:00,867:INFO:   <<< do_lower_case: False
2024-04-28 03:26:00,867:INFO:   <<< do_pretrain: False
2024-04-28 03:26:00,867:INFO:   <<< do_train: False
2024-04-28 03:26:00,867:INFO:   <<< epochs: 2
2024-04-28 03:26:00,867:INFO:   <<< eval_frame_order: 0
2024-04-28 03:26:00,867:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:26:00,867:INFO:   <<< feature_framerate: 1
2024-04-28 03:26:00,867:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:26:00,867:INFO:   <<< fp16: False
2024-04-28 03:26:00,867:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:26:00,867:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:26:00,867:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:26:00,867:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:26:00,867:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:26:00,867:INFO:   <<< linear_patch: 2d
2024-04-28 03:26:00,867:INFO:   <<< local_rank: 0
2024-04-28 03:26:00,867:INFO:   <<< loose_type: True
2024-04-28 03:26:00,867:INFO:   <<< lr: 0.0001
2024-04-28 03:26:00,867:INFO:   <<< lr_decay: 0.9
2024-04-28 03:26:00,867:INFO:   <<< margin: 0.1
2024-04-28 03:26:00,867:INFO:   <<< max_frames: 12
2024-04-28 03:26:00,867:INFO:   <<< max_words: 32
2024-04-28 03:26:00,867:INFO:   <<< n_display: 100
2024-04-28 03:26:00,867:INFO:   <<< n_gpu: 1
2024-04-28 03:26:00,867:INFO:   <<< n_pair: 1
2024-04-28 03:26:00,867:INFO:   <<< negative_weighting: 1
2024-04-28 03:26:00,867:INFO:   <<< num_thread_reader: 0
2024-04-28 03:26:00,867:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:26:00,867:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:26:00,867:INFO:   <<< rank: 0
2024-04-28 03:26:00,867:INFO:   <<< resume_model: None
2024-04-28 03:26:00,867:INFO:   <<< sampled_use_mil: False
2024-04-28 03:26:00,867:INFO:   <<< seed: 42
2024-04-28 03:26:00,867:INFO:   <<< sim_header: meanP
2024-04-28 03:26:00,867:INFO:   <<< slice_framepos: 2
2024-04-28 03:26:00,867:INFO:   <<< task_type: retrieval
2024-04-28 03:26:00,867:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:26:00,868:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:26:00,868:INFO:   <<< train_frame_order: 0
2024-04-28 03:26:00,868:INFO:   <<< use_mil: False
2024-04-28 03:26:00,868:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:26:00,868:INFO:   <<< video_dim: 1024
2024-04-28 03:26:00,868:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:26:00,868:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:26:00,868:INFO:   <<< world_size: 1
2024-04-28 03:26:00,868:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:26:01,545:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:26:01,546:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:26:01,546:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:26:01,546:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:26:01,546:WARNING: Test retrieval by loose type.
2024-04-28 03:26:01,546:WARNING: 	 embed_dim: 512
2024-04-28 03:26:01,546:WARNING: 	 image_resolution: 224
2024-04-28 03:26:01,546:WARNING: 	 vision_layers: 12
2024-04-28 03:26:01,546:WARNING: 	 vision_width: 768
2024-04-28 03:26:01,546:WARNING: 	 vision_patch_size: 32
2024-04-28 03:26:01,546:WARNING: 	 context_length: 77
2024-04-28 03:26:01,546:WARNING: 	 vocab_size: 49408
2024-04-28 03:26:01,546:WARNING: 	 transformer_width: 512
2024-04-28 03:26:01,546:WARNING: 	 transformer_heads: 8
2024-04-28 03:26:01,546:WARNING: 	 transformer_layers: 12
2024-04-28 03:26:01,546:WARNING: 		 linear_patch: 2d
2024-04-28 03:26:01,546:WARNING: 	 cut_top_layer: 0
2024-04-28 03:26:02,397:WARNING: 	 sim_header: meanP
2024-04-28 03:26:06,089:INFO: --------------------
2024-04-28 03:26:06,090:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:26:08,196:INFO: ***** Running test *****
2024-04-28 03:26:08,196:INFO:   Num examples = 1000
2024-04-28 03:26:08,196:INFO:   Batch size = 16
2024-04-28 03:26:08,196:INFO:   Num steps = 63
2024-04-28 03:26:08,196:INFO: ***** Running val *****
2024-04-28 03:26:08,196:INFO:   Num examples = 1000
2024-04-28 03:26:08,332:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:26:08,798:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:26:08,798:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:26:08,798:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:26:08,798:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:26:08,798:WARNING: Test retrieval by loose type.
2024-04-28 03:26:08,798:WARNING: 	 embed_dim: 512
2024-04-28 03:26:08,798:WARNING: 	 image_resolution: 224
2024-04-28 03:26:08,798:WARNING: 	 vision_layers: 12
2024-04-28 03:26:08,798:WARNING: 	 vision_width: 768
2024-04-28 03:26:08,798:WARNING: 	 vision_patch_size: 32
2024-04-28 03:26:08,798:WARNING: 	 context_length: 77
2024-04-28 03:26:08,798:WARNING: 	 vocab_size: 49408
2024-04-28 03:26:08,798:WARNING: 	 transformer_width: 512
2024-04-28 03:26:08,798:WARNING: 	 transformer_heads: 8
2024-04-28 03:26:08,798:WARNING: 	 transformer_layers: 12
2024-04-28 03:26:08,798:WARNING: 		 linear_patch: 2d
2024-04-28 03:26:08,798:WARNING: 	 cut_top_layer: 0
2024-04-28 03:26:09,639:WARNING: 	 sim_header: meanP
2024-04-28 03:26:13,341:INFO: --------------------
2024-04-28 03:26:13,341:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:28:39,713:INFO: sim matrix size: 1000, 1000
2024-04-28 03:28:39,781:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-28 03:28:39,781:INFO: Text-to-Video:
2024-04-28 03:28:39,781:INFO: 	>>>  R@1: 43.3 - R@5: 68.8 - R@10: 78.4 - Median R: 2.0 - Mean R: 16.3
2024-04-28 03:28:39,781:INFO: Video-to-Text:
2024-04-28 03:28:39,781:INFO: 	>>>  V2T$R@1: 40.1 - V2T$R@5: 68.6 - V2T$R@10: 79.3 - V2T$Median R: 2.0 - V2T$Mean R: 13.0
2024-04-28 03:29:37,352:INFO: Effective parameters:
2024-04-28 03:29:37,352:INFO:   <<< batch_size: 16
2024-04-28 03:29:37,352:INFO:   <<< batch_size_val: 16
2024-04-28 03:29:37,352:INFO:   <<< cache_dir: 
2024-04-28 03:29:37,352:INFO:   <<< coef_lr: 0.001
2024-04-28 03:29:37,352:INFO:   <<< cross_model: cross-base
2024-04-28 03:29:37,352:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:29:37,352:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:29:37,352:INFO:   <<< datatype: msrvtt
2024-04-28 03:29:37,352:INFO:   <<< do_eval: True
2024-04-28 03:29:37,352:INFO:   <<< do_lower_case: False
2024-04-28 03:29:37,352:INFO:   <<< do_pretrain: False
2024-04-28 03:29:37,352:INFO:   <<< do_train: False
2024-04-28 03:29:37,352:INFO:   <<< epochs: 2
2024-04-28 03:29:37,353:INFO:   <<< eval_frame_order: 0
2024-04-28 03:29:37,353:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:29:37,353:INFO:   <<< feature_framerate: 1
2024-04-28 03:29:37,353:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:29:37,353:INFO:   <<< fp16: False
2024-04-28 03:29:37,353:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:29:37,353:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:29:37,353:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:29:37,353:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:29:37,353:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:29:37,353:INFO:   <<< linear_patch: 2d
2024-04-28 03:29:37,353:INFO:   <<< local_rank: 0
2024-04-28 03:29:37,353:INFO:   <<< loose_type: True
2024-04-28 03:29:37,353:INFO:   <<< lr: 0.0001
2024-04-28 03:29:37,353:INFO:   <<< lr_decay: 0.9
2024-04-28 03:29:37,353:INFO:   <<< margin: 0.1
2024-04-28 03:29:37,353:INFO:   <<< max_frames: 12
2024-04-28 03:29:37,353:INFO:   <<< max_words: 32
2024-04-28 03:29:37,353:INFO:   <<< n_display: 100
2024-04-28 03:29:37,353:INFO:   <<< n_gpu: 1
2024-04-28 03:29:37,353:INFO:   <<< n_pair: 1
2024-04-28 03:29:37,353:INFO:   <<< negative_weighting: 1
2024-04-28 03:29:37,353:INFO:   <<< num_thread_reader: 0
2024-04-28 03:29:37,353:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:29:37,353:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:29:37,353:INFO:   <<< rank: 0
2024-04-28 03:29:37,353:INFO:   <<< resume_model: None
2024-04-28 03:29:37,353:INFO:   <<< sampled_use_mil: False
2024-04-28 03:29:37,353:INFO:   <<< seed: 42
2024-04-28 03:29:37,353:INFO:   <<< sim_header: meanP
2024-04-28 03:29:37,353:INFO:   <<< slice_framepos: 2
2024-04-28 03:29:37,353:INFO:   <<< task_type: retrieval
2024-04-28 03:29:37,353:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:29:37,353:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:29:37,353:INFO:   <<< train_frame_order: 0
2024-04-28 03:29:37,353:INFO:   <<< use_mil: False
2024-04-28 03:29:37,353:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:29:37,353:INFO:   <<< video_dim: 1024
2024-04-28 03:29:37,353:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:29:37,353:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:29:37,353:INFO:   <<< world_size: 1
2024-04-28 03:29:37,353:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:31:12,544:INFO: Effective parameters:
2024-04-28 03:31:12,544:INFO:   <<< batch_size: 16
2024-04-28 03:31:12,544:INFO:   <<< batch_size_val: 16
2024-04-28 03:31:12,544:INFO:   <<< cache_dir: 
2024-04-28 03:31:12,544:INFO:   <<< coef_lr: 0.001
2024-04-28 03:31:12,544:INFO:   <<< cross_model: cross-base
2024-04-28 03:31:12,544:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:31:12,544:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:31:12,544:INFO:   <<< datatype: msrvtt
2024-04-28 03:31:12,544:INFO:   <<< do_eval: True
2024-04-28 03:31:12,544:INFO:   <<< do_lower_case: False
2024-04-28 03:31:12,544:INFO:   <<< do_pretrain: False
2024-04-28 03:31:12,544:INFO:   <<< do_train: False
2024-04-28 03:31:12,544:INFO:   <<< epochs: 2
2024-04-28 03:31:12,544:INFO:   <<< eval_frame_order: 0
2024-04-28 03:31:12,544:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:31:12,544:INFO:   <<< feature_framerate: 1
2024-04-28 03:31:12,544:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:31:12,545:INFO:   <<< fp16: False
2024-04-28 03:31:12,545:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:31:12,545:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:31:12,545:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:31:12,545:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:31:12,545:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:31:12,545:INFO:   <<< linear_patch: 2d
2024-04-28 03:31:12,545:INFO:   <<< local_rank: 0
2024-04-28 03:31:12,545:INFO:   <<< loose_type: True
2024-04-28 03:31:12,545:INFO:   <<< lr: 0.0001
2024-04-28 03:31:12,545:INFO:   <<< lr_decay: 0.9
2024-04-28 03:31:12,545:INFO:   <<< margin: 0.1
2024-04-28 03:31:12,545:INFO:   <<< max_frames: 12
2024-04-28 03:31:12,545:INFO:   <<< max_words: 32
2024-04-28 03:31:12,545:INFO:   <<< n_display: 100
2024-04-28 03:31:12,545:INFO:   <<< n_gpu: 1
2024-04-28 03:31:12,545:INFO:   <<< n_pair: 1
2024-04-28 03:31:12,545:INFO:   <<< negative_weighting: 1
2024-04-28 03:31:12,545:INFO:   <<< num_thread_reader: 0
2024-04-28 03:31:12,545:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:31:12,545:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:31:12,545:INFO:   <<< rank: 0
2024-04-28 03:31:12,545:INFO:   <<< resume_model: None
2024-04-28 03:31:12,545:INFO:   <<< sampled_use_mil: False
2024-04-28 03:31:12,545:INFO:   <<< seed: 42
2024-04-28 03:31:12,545:INFO:   <<< sim_header: meanP
2024-04-28 03:31:12,545:INFO:   <<< slice_framepos: 2
2024-04-28 03:31:12,545:INFO:   <<< task_type: retrieval
2024-04-28 03:31:12,545:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:31:12,545:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:31:12,545:INFO:   <<< train_frame_order: 0
2024-04-28 03:31:12,545:INFO:   <<< use_mil: False
2024-04-28 03:31:12,545:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:31:12,545:INFO:   <<< video_dim: 1024
2024-04-28 03:31:12,545:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:31:12,545:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:31:12,545:INFO:   <<< world_size: 1
2024-04-28 03:31:12,545:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:31:12,744:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:31:13,213:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:31:13,214:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:31:13,214:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:31:13,214:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:31:13,214:WARNING: Test retrieval by loose type.
2024-04-28 03:31:13,214:WARNING: 	 embed_dim: 512
2024-04-28 03:31:13,214:WARNING: 	 image_resolution: 224
2024-04-28 03:31:13,214:WARNING: 	 vision_layers: 12
2024-04-28 03:31:13,214:WARNING: 	 vision_width: 768
2024-04-28 03:31:13,214:WARNING: 	 vision_patch_size: 32
2024-04-28 03:31:13,214:WARNING: 	 context_length: 77
2024-04-28 03:31:13,214:WARNING: 	 vocab_size: 49408
2024-04-28 03:31:13,214:WARNING: 	 transformer_width: 512
2024-04-28 03:31:13,214:WARNING: 	 transformer_heads: 8
2024-04-28 03:31:13,214:WARNING: 	 transformer_layers: 12
2024-04-28 03:31:13,214:WARNING: 		 linear_patch: 2d
2024-04-28 03:31:13,214:WARNING: 	 cut_top_layer: 0
2024-04-28 03:31:14,047:WARNING: 	 sim_header: meanP
2024-04-28 03:31:17,738:INFO: --------------------
2024-04-28 03:31:17,738:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:31:19,789:INFO: ***** Running test *****
2024-04-28 03:31:19,789:INFO:   Num examples = 1000
2024-04-28 03:31:19,789:INFO:   Batch size = 16
2024-04-28 03:31:19,789:INFO:   Num steps = 63
2024-04-28 03:31:19,789:INFO: ***** Running val *****
2024-04-28 03:31:19,789:INFO:   Num examples = 1000
2024-04-28 03:33:46,186:INFO: sim matrix size: 1000, 1000
2024-04-28 03:33:46,251:INFO: 	 Length-T: 1000, Length-V:1000
2024-04-28 03:33:46,251:INFO: Text-to-Video:
2024-04-28 03:33:46,251:INFO: 	>>>  R@1: 43.3 - R@5: 68.8 - R@10: 78.4 - Median R: 2.0 - Mean R: 16.3
2024-04-28 03:33:46,251:INFO: Video-to-Text:
2024-04-28 03:33:46,251:INFO: 	>>>  V2T$R@1: 40.1 - V2T$R@5: 68.6 - V2T$R@10: 79.3 - V2T$Median R: 2.0 - V2T$Mean R: 13.0
2024-04-28 03:34:29,001:INFO: Effective parameters:
2024-04-28 03:34:29,001:INFO:   <<< batch_size: 16
2024-04-28 03:34:29,001:INFO:   <<< batch_size_val: 16
2024-04-28 03:34:29,001:INFO:   <<< cache_dir: 
2024-04-28 03:34:29,002:INFO:   <<< coef_lr: 0.001
2024-04-28 03:34:29,002:INFO:   <<< cross_model: cross-base
2024-04-28 03:34:29,002:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:34:29,002:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:34:29,002:INFO:   <<< datatype: msrvtt
2024-04-28 03:34:29,002:INFO:   <<< do_eval: True
2024-04-28 03:34:29,002:INFO:   <<< do_lower_case: False
2024-04-28 03:34:29,002:INFO:   <<< do_pretrain: False
2024-04-28 03:34:29,002:INFO:   <<< do_train: False
2024-04-28 03:34:29,002:INFO:   <<< epochs: 2
2024-04-28 03:34:29,002:INFO:   <<< eval_frame_order: 0
2024-04-28 03:34:29,002:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:34:29,002:INFO:   <<< feature_framerate: 1
2024-04-28 03:34:29,002:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:34:29,002:INFO:   <<< fp16: False
2024-04-28 03:34:29,002:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:34:29,003:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:34:29,003:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:34:29,003:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:34:29,003:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:34:29,003:INFO:   <<< linear_patch: 2d
2024-04-28 03:34:29,003:INFO:   <<< local_rank: 0
2024-04-28 03:34:29,003:INFO:   <<< loose_type: True
2024-04-28 03:34:29,003:INFO:   <<< lr: 0.0001
2024-04-28 03:34:29,003:INFO:   <<< lr_decay: 0.9
2024-04-28 03:34:29,003:INFO:   <<< margin: 0.1
2024-04-28 03:34:29,003:INFO:   <<< max_frames: 12
2024-04-28 03:34:29,003:INFO:   <<< max_words: 32
2024-04-28 03:34:29,003:INFO:   <<< n_display: 100
2024-04-28 03:34:29,003:INFO:   <<< n_gpu: 1
2024-04-28 03:34:29,003:INFO:   <<< n_pair: 1
2024-04-28 03:34:29,003:INFO:   <<< negative_weighting: 1
2024-04-28 03:34:29,003:INFO:   <<< num_thread_reader: 0
2024-04-28 03:34:29,004:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:34:29,004:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:34:29,004:INFO:   <<< rank: 0
2024-04-28 03:34:29,004:INFO:   <<< resume_model: None
2024-04-28 03:34:29,004:INFO:   <<< sampled_use_mil: False
2024-04-28 03:34:29,004:INFO:   <<< seed: 42
2024-04-28 03:34:29,004:INFO:   <<< sim_header: meanP
2024-04-28 03:34:29,004:INFO:   <<< slice_framepos: 2
2024-04-28 03:34:29,004:INFO:   <<< task_type: retrieval
2024-04-28 03:34:29,004:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:34:29,004:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:34:29,004:INFO:   <<< train_frame_order: 0
2024-04-28 03:34:29,004:INFO:   <<< use_mil: False
2024-04-28 03:34:29,004:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:34:29,004:INFO:   <<< video_dim: 1024
2024-04-28 03:34:29,004:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:34:29,004:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:34:29,005:INFO:   <<< world_size: 1
2024-04-28 03:34:29,005:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:34:29,245:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:34:29,735:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:34:29,736:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:34:29,736:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:34:36,777:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:34:36,778:WARNING: Test retrieval by loose type.
2024-04-28 03:34:36,779:WARNING: 	 embed_dim: 512
2024-04-28 03:34:36,780:WARNING: 	 image_resolution: 224
2024-04-28 03:34:36,780:WARNING: 	 vision_layers: 12
2024-04-28 03:34:36,780:WARNING: 	 vision_width: 768
2024-04-28 03:34:36,781:WARNING: 	 vision_patch_size: 32
2024-04-28 03:34:36,781:WARNING: 	 context_length: 77
2024-04-28 03:34:36,781:WARNING: 	 vocab_size: 49408
2024-04-28 03:34:36,782:WARNING: 	 transformer_width: 512
2024-04-28 03:34:36,782:WARNING: 	 transformer_heads: 8
2024-04-28 03:34:36,782:WARNING: 	 transformer_layers: 12
2024-04-28 03:34:36,782:WARNING: 		 linear_patch: 2d
2024-04-28 03:34:36,783:WARNING: 	 cut_top_layer: 0
2024-04-28 03:34:37,698:WARNING: 	 sim_header: meanP
2024-04-28 03:34:41,456:INFO: --------------------
2024-04-28 03:34:41,456:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:43:42,052:INFO: Effective parameters:
2024-04-28 03:43:42,053:INFO:   <<< batch_size: 16
2024-04-28 03:43:42,053:INFO:   <<< batch_size_val: 16
2024-04-28 03:43:42,053:INFO:   <<< cache_dir: 
2024-04-28 03:43:42,053:INFO:   <<< coef_lr: 0.001
2024-04-28 03:43:42,053:INFO:   <<< cross_model: cross-base
2024-04-28 03:43:42,053:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:43:42,053:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:43:42,053:INFO:   <<< datatype: msrvtt
2024-04-28 03:43:42,053:INFO:   <<< do_eval: True
2024-04-28 03:43:42,053:INFO:   <<< do_lower_case: False
2024-04-28 03:43:42,053:INFO:   <<< do_pretrain: False
2024-04-28 03:43:42,054:INFO:   <<< do_train: False
2024-04-28 03:43:42,054:INFO:   <<< epochs: 2
2024-04-28 03:43:42,054:INFO:   <<< eval_frame_order: 0
2024-04-28 03:43:42,054:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:43:42,054:INFO:   <<< feature_framerate: 1
2024-04-28 03:43:42,054:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:43:42,054:INFO:   <<< fp16: False
2024-04-28 03:43:42,054:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:43:42,054:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:43:42,054:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:43:42,054:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:43:42,054:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:43:42,054:INFO:   <<< linear_patch: 2d
2024-04-28 03:43:42,054:INFO:   <<< local_rank: 0
2024-04-28 03:43:42,054:INFO:   <<< loose_type: True
2024-04-28 03:43:42,055:INFO:   <<< lr: 0.0001
2024-04-28 03:43:42,055:INFO:   <<< lr_decay: 0.9
2024-04-28 03:43:42,055:INFO:   <<< margin: 0.1
2024-04-28 03:43:42,055:INFO:   <<< max_frames: 12
2024-04-28 03:43:42,055:INFO:   <<< max_words: 32
2024-04-28 03:43:42,055:INFO:   <<< n_display: 100
2024-04-28 03:43:42,055:INFO:   <<< n_gpu: 1
2024-04-28 03:43:42,055:INFO:   <<< n_pair: 1
2024-04-28 03:43:42,055:INFO:   <<< negative_weighting: 1
2024-04-28 03:43:42,055:INFO:   <<< num_thread_reader: 0
2024-04-28 03:43:42,055:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:43:42,055:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:43:42,055:INFO:   <<< rank: 0
2024-04-28 03:43:42,055:INFO:   <<< resume_model: None
2024-04-28 03:43:42,055:INFO:   <<< sampled_use_mil: False
2024-04-28 03:43:42,056:INFO:   <<< seed: 42
2024-04-28 03:43:42,056:INFO:   <<< sim_header: meanP
2024-04-28 03:43:42,056:INFO:   <<< slice_framepos: 2
2024-04-28 03:43:42,056:INFO:   <<< task_type: retrieval
2024-04-28 03:43:42,056:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:43:42,056:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:43:42,056:INFO:   <<< train_frame_order: 0
2024-04-28 03:43:42,056:INFO:   <<< use_mil: False
2024-04-28 03:43:42,056:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:43:42,056:INFO:   <<< video_dim: 1024
2024-04-28 03:43:42,056:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:43:42,056:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:43:42,056:INFO:   <<< world_size: 1
2024-04-28 03:43:42,056:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:43:42,303:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:43:42,777:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:43:42,778:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:43:42,778:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:43:42,778:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:43:42,778:WARNING: Test retrieval by loose type.
2024-04-28 03:43:42,779:WARNING: 	 embed_dim: 512
2024-04-28 03:43:42,779:WARNING: 	 image_resolution: 224
2024-04-28 03:43:42,779:WARNING: 	 vision_layers: 12
2024-04-28 03:43:42,779:WARNING: 	 vision_width: 768
2024-04-28 03:43:42,779:WARNING: 	 vision_patch_size: 32
2024-04-28 03:43:42,779:WARNING: 	 context_length: 77
2024-04-28 03:43:42,779:WARNING: 	 vocab_size: 49408
2024-04-28 03:43:42,779:WARNING: 	 transformer_width: 512
2024-04-28 03:43:42,780:WARNING: 	 transformer_heads: 8
2024-04-28 03:43:42,780:WARNING: 	 transformer_layers: 12
2024-04-28 03:43:42,780:WARNING: 		 linear_patch: 2d
2024-04-28 03:43:42,780:WARNING: 	 cut_top_layer: 0
2024-04-28 03:43:43,701:WARNING: 	 sim_header: meanP
2024-04-28 03:43:47,418:INFO: --------------------
2024-04-28 03:43:47,418:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:52:43,715:INFO: ***** Running test *****
2024-04-28 03:52:45,197:INFO:   Num examples = 1000
2024-04-28 03:52:46,456:INFO:   Batch size = 16
2024-04-28 03:52:47,320:INFO:   Num steps = 63
2024-04-28 03:53:52,581:INFO: Effective parameters:
2024-04-28 03:53:52,581:INFO:   <<< batch_size: 16
2024-04-28 03:53:52,581:INFO:   <<< batch_size_val: 16
2024-04-28 03:53:52,581:INFO:   <<< cache_dir: 
2024-04-28 03:53:52,581:INFO:   <<< coef_lr: 0.001
2024-04-28 03:53:52,581:INFO:   <<< cross_model: cross-base
2024-04-28 03:53:52,581:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 03:53:52,581:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 03:53:52,581:INFO:   <<< datatype: msrvtt
2024-04-28 03:53:52,581:INFO:   <<< do_eval: True
2024-04-28 03:53:52,581:INFO:   <<< do_lower_case: False
2024-04-28 03:53:52,581:INFO:   <<< do_pretrain: False
2024-04-28 03:53:52,582:INFO:   <<< do_train: False
2024-04-28 03:53:52,582:INFO:   <<< epochs: 2
2024-04-28 03:53:52,582:INFO:   <<< eval_frame_order: 0
2024-04-28 03:53:52,582:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 03:53:52,582:INFO:   <<< feature_framerate: 1
2024-04-28 03:53:52,582:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 03:53:52,582:INFO:   <<< fp16: False
2024-04-28 03:53:52,582:INFO:   <<< fp16_opt_level: O1
2024-04-28 03:53:52,582:INFO:   <<< freeze_layer_num: 0
2024-04-28 03:53:52,582:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 03:53:52,582:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 03:53:52,582:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:53:52,582:INFO:   <<< linear_patch: 2d
2024-04-28 03:53:52,582:INFO:   <<< local_rank: 0
2024-04-28 03:53:52,582:INFO:   <<< loose_type: True
2024-04-28 03:53:52,582:INFO:   <<< lr: 0.0001
2024-04-28 03:53:52,582:INFO:   <<< lr_decay: 0.9
2024-04-28 03:53:52,583:INFO:   <<< margin: 0.1
2024-04-28 03:53:52,583:INFO:   <<< max_frames: 12
2024-04-28 03:53:52,583:INFO:   <<< max_words: 32
2024-04-28 03:53:52,583:INFO:   <<< n_display: 100
2024-04-28 03:53:52,583:INFO:   <<< n_gpu: 1
2024-04-28 03:53:52,583:INFO:   <<< n_pair: 1
2024-04-28 03:53:52,583:INFO:   <<< negative_weighting: 1
2024-04-28 03:53:52,583:INFO:   <<< num_thread_reader: 0
2024-04-28 03:53:52,583:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 03:53:52,583:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 03:53:52,583:INFO:   <<< rank: 0
2024-04-28 03:53:52,583:INFO:   <<< resume_model: None
2024-04-28 03:53:52,583:INFO:   <<< sampled_use_mil: False
2024-04-28 03:53:52,583:INFO:   <<< seed: 42
2024-04-28 03:53:52,583:INFO:   <<< sim_header: meanP
2024-04-28 03:53:52,583:INFO:   <<< slice_framepos: 2
2024-04-28 03:53:52,583:INFO:   <<< task_type: retrieval
2024-04-28 03:53:52,583:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 03:53:52,584:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 03:53:52,584:INFO:   <<< train_frame_order: 0
2024-04-28 03:53:52,584:INFO:   <<< use_mil: False
2024-04-28 03:53:52,584:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 03:53:52,584:INFO:   <<< video_dim: 1024
2024-04-28 03:53:52,584:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 03:53:52,584:INFO:   <<< warmup_proportion: 0.1
2024-04-28 03:53:52,584:INFO:   <<< world_size: 1
2024-04-28 03:53:52,584:INFO: device: cuda:0 n_gpu: 1
2024-04-28 03:53:52,838:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 03:53:53,334:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 03:53:53,336:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 03:53:53,336:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 03:53:53,336:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 03:53:53,336:WARNING: Test retrieval by loose type.
2024-04-28 03:53:53,337:WARNING: 	 embed_dim: 512
2024-04-28 03:53:53,338:WARNING: 	 image_resolution: 224
2024-04-28 03:53:53,338:WARNING: 	 vision_layers: 12
2024-04-28 03:53:53,338:WARNING: 	 vision_width: 768
2024-04-28 03:53:53,338:WARNING: 	 vision_patch_size: 32
2024-04-28 03:53:53,338:WARNING: 	 context_length: 77
2024-04-28 03:53:53,338:WARNING: 	 vocab_size: 49408
2024-04-28 03:53:53,338:WARNING: 	 transformer_width: 512
2024-04-28 03:53:53,338:WARNING: 	 transformer_heads: 8
2024-04-28 03:53:53,339:WARNING: 	 transformer_layers: 12
2024-04-28 03:53:53,339:WARNING: 		 linear_patch: 2d
2024-04-28 03:53:53,339:WARNING: 	 cut_top_layer: 0
2024-04-28 03:53:54,197:WARNING: 	 sim_header: meanP
2024-04-28 03:53:57,958:INFO: --------------------
2024-04-28 03:53:57,958:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 03:56:26,066:INFO: ***** Running test *****
2024-04-28 03:56:26,543:INFO:   Num examples = 1000
2024-04-28 03:56:27,021:INFO:   Batch size = 16
2024-04-28 03:56:27,804:INFO:   Num steps = 63
2024-04-28 03:56:29,420:INFO: ***** Running val *****
2024-04-28 03:56:30,283:INFO:   Num examples = 1000
2024-04-28 04:20:05,183:INFO: Effective parameters:
2024-04-28 04:20:05,184:INFO:   <<< batch_size: 16
2024-04-28 04:20:05,184:INFO:   <<< batch_size_val: 16
2024-04-28 04:20:05,184:INFO:   <<< cache_dir: 
2024-04-28 04:20:05,184:INFO:   <<< coef_lr: 0.001
2024-04-28 04:20:05,184:INFO:   <<< cross_model: cross-base
2024-04-28 04:20:05,184:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 04:20:05,184:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 04:20:05,184:INFO:   <<< datatype: msrvtt
2024-04-28 04:20:05,184:INFO:   <<< do_eval: True
2024-04-28 04:20:05,184:INFO:   <<< do_lower_case: False
2024-04-28 04:20:05,184:INFO:   <<< do_pretrain: False
2024-04-28 04:20:05,184:INFO:   <<< do_train: False
2024-04-28 04:20:05,184:INFO:   <<< epochs: 2
2024-04-28 04:20:05,184:INFO:   <<< eval_frame_order: 0
2024-04-28 04:20:05,184:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 04:20:05,185:INFO:   <<< feature_framerate: 1
2024-04-28 04:20:05,185:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 04:20:05,185:INFO:   <<< fp16: False
2024-04-28 04:20:05,185:INFO:   <<< fp16_opt_level: O1
2024-04-28 04:20:05,185:INFO:   <<< freeze_layer_num: 0
2024-04-28 04:20:05,185:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 04:20:05,185:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 04:20:05,185:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 04:20:05,185:INFO:   <<< linear_patch: 2d
2024-04-28 04:20:05,185:INFO:   <<< local_rank: 0
2024-04-28 04:20:05,185:INFO:   <<< loose_type: True
2024-04-28 04:20:05,185:INFO:   <<< lr: 0.0001
2024-04-28 04:20:05,185:INFO:   <<< lr_decay: 0.9
2024-04-28 04:20:05,185:INFO:   <<< margin: 0.1
2024-04-28 04:20:05,185:INFO:   <<< max_frames: 12
2024-04-28 04:20:05,185:INFO:   <<< max_words: 32
2024-04-28 04:20:05,185:INFO:   <<< n_display: 100
2024-04-28 04:20:05,186:INFO:   <<< n_gpu: 1
2024-04-28 04:20:05,186:INFO:   <<< n_pair: 1
2024-04-28 04:20:05,186:INFO:   <<< negative_weighting: 1
2024-04-28 04:20:05,186:INFO:   <<< num_thread_reader: 0
2024-04-28 04:20:05,186:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 04:20:05,186:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 04:20:05,186:INFO:   <<< rank: 0
2024-04-28 04:20:05,186:INFO:   <<< resume_model: None
2024-04-28 04:20:05,186:INFO:   <<< sampled_use_mil: False
2024-04-28 04:20:05,186:INFO:   <<< seed: 42
2024-04-28 04:20:05,186:INFO:   <<< sim_header: meanP
2024-04-28 04:20:05,186:INFO:   <<< slice_framepos: 2
2024-04-28 04:20:05,186:INFO:   <<< task_type: retrieval
2024-04-28 04:20:05,186:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 04:20:05,186:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 04:20:05,186:INFO:   <<< train_frame_order: 0
2024-04-28 04:20:05,186:INFO:   <<< use_mil: False
2024-04-28 04:20:05,187:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 04:20:05,187:INFO:   <<< video_dim: 1024
2024-04-28 04:20:05,187:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 04:20:05,187:INFO:   <<< warmup_proportion: 0.1
2024-04-28 04:20:05,187:INFO:   <<< world_size: 1
2024-04-28 04:20:05,187:INFO: device: cuda:0 n_gpu: 1
2024-04-28 04:20:05,436:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 04:20:05,925:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 04:20:05,927:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 04:20:05,927:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 04:20:05,928:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 04:20:05,928:WARNING: Test retrieval by loose type.
2024-04-28 04:20:05,929:WARNING: 	 embed_dim: 512
2024-04-28 04:20:05,929:WARNING: 	 image_resolution: 224
2024-04-28 04:20:05,929:WARNING: 	 vision_layers: 12
2024-04-28 04:20:05,929:WARNING: 	 vision_width: 768
2024-04-28 04:20:05,929:WARNING: 	 vision_patch_size: 32
2024-04-28 04:20:05,929:WARNING: 	 context_length: 77
2024-04-28 04:20:05,929:WARNING: 	 vocab_size: 49408
2024-04-28 04:20:05,929:WARNING: 	 transformer_width: 512
2024-04-28 04:20:05,929:WARNING: 	 transformer_heads: 8
2024-04-28 04:20:05,930:WARNING: 	 transformer_layers: 12
2024-04-28 04:20:05,930:WARNING: 		 linear_patch: 2d
2024-04-28 04:20:05,930:WARNING: 	 cut_top_layer: 0
2024-04-28 04:20:06,792:WARNING: 	 sim_header: meanP
2024-04-28 04:20:10,512:INFO: --------------------
2024-04-28 04:20:10,513:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 04:27:33,731:INFO: ***** Running test *****
2024-04-28 04:27:34,127:INFO:   Num examples = 1000
2024-04-28 04:27:34,624:INFO:   Batch size = 16
2024-04-28 04:27:35,622:INFO:   Num steps = 63
2024-04-28 04:27:36,087:INFO: ***** Running val *****
2024-04-28 04:27:36,617:INFO:   Num examples = 1000
2024-04-28 04:37:54,525:INFO: Effective parameters:
2024-04-28 04:37:54,525:INFO:   <<< batch_size: 16
2024-04-28 04:37:54,525:INFO:   <<< batch_size_val: 16
2024-04-28 04:37:54,525:INFO:   <<< cache_dir: 
2024-04-28 04:37:54,526:INFO:   <<< coef_lr: 0.001
2024-04-28 04:37:54,526:INFO:   <<< cross_model: cross-base
2024-04-28 04:37:54,526:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 04:37:54,526:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 04:37:54,526:INFO:   <<< datatype: msrvtt
2024-04-28 04:37:54,526:INFO:   <<< do_eval: True
2024-04-28 04:37:54,526:INFO:   <<< do_lower_case: False
2024-04-28 04:37:54,526:INFO:   <<< do_pretrain: False
2024-04-28 04:37:54,526:INFO:   <<< do_train: False
2024-04-28 04:37:54,526:INFO:   <<< epochs: 2
2024-04-28 04:37:54,526:INFO:   <<< eval_frame_order: 0
2024-04-28 04:37:54,526:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 04:37:54,526:INFO:   <<< feature_framerate: 1
2024-04-28 04:37:54,526:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 04:37:54,526:INFO:   <<< fp16: False
2024-04-28 04:37:54,526:INFO:   <<< fp16_opt_level: O1
2024-04-28 04:37:54,526:INFO:   <<< freeze_layer_num: 0
2024-04-28 04:37:54,527:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 04:37:54,527:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 04:37:54,527:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 04:37:54,527:INFO:   <<< linear_patch: 2d
2024-04-28 04:37:54,527:INFO:   <<< local_rank: 0
2024-04-28 04:37:54,527:INFO:   <<< loose_type: True
2024-04-28 04:37:54,527:INFO:   <<< lr: 0.0001
2024-04-28 04:37:54,527:INFO:   <<< lr_decay: 0.9
2024-04-28 04:37:54,527:INFO:   <<< margin: 0.1
2024-04-28 04:37:54,527:INFO:   <<< max_frames: 12
2024-04-28 04:37:54,527:INFO:   <<< max_words: 32
2024-04-28 04:37:54,527:INFO:   <<< n_display: 100
2024-04-28 04:37:54,527:INFO:   <<< n_gpu: 1
2024-04-28 04:37:54,527:INFO:   <<< n_pair: 1
2024-04-28 04:37:54,527:INFO:   <<< negative_weighting: 1
2024-04-28 04:37:54,527:INFO:   <<< num_thread_reader: 0
2024-04-28 04:37:54,527:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 04:37:54,527:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 04:37:54,528:INFO:   <<< rank: 0
2024-04-28 04:37:54,528:INFO:   <<< resume_model: None
2024-04-28 04:37:54,528:INFO:   <<< sampled_use_mil: False
2024-04-28 04:37:54,528:INFO:   <<< seed: 42
2024-04-28 04:37:54,528:INFO:   <<< sim_header: meanP
2024-04-28 04:37:54,528:INFO:   <<< slice_framepos: 2
2024-04-28 04:37:54,528:INFO:   <<< task_type: retrieval
2024-04-28 04:37:54,528:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 04:37:54,528:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 04:37:54,528:INFO:   <<< train_frame_order: 0
2024-04-28 04:37:54,528:INFO:   <<< use_mil: False
2024-04-28 04:37:54,528:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 04:37:54,528:INFO:   <<< video_dim: 1024
2024-04-28 04:37:54,528:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 04:37:54,528:INFO:   <<< warmup_proportion: 0.1
2024-04-28 04:37:54,528:INFO:   <<< world_size: 1
2024-04-28 04:37:54,528:INFO: device: cuda:0 n_gpu: 1
2024-04-28 04:37:54,788:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 04:37:55,254:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 04:37:55,255:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 04:37:55,255:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 04:37:55,256:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 04:37:55,256:WARNING: Test retrieval by loose type.
2024-04-28 04:37:55,256:WARNING: 	 embed_dim: 512
2024-04-28 04:37:55,256:WARNING: 	 image_resolution: 224
2024-04-28 04:37:55,257:WARNING: 	 vision_layers: 12
2024-04-28 04:37:55,257:WARNING: 	 vision_width: 768
2024-04-28 04:37:55,257:WARNING: 	 vision_patch_size: 32
2024-04-28 04:37:55,257:WARNING: 	 context_length: 77
2024-04-28 04:37:55,257:WARNING: 	 vocab_size: 49408
2024-04-28 04:37:55,257:WARNING: 	 transformer_width: 512
2024-04-28 04:37:55,257:WARNING: 	 transformer_heads: 8
2024-04-28 04:37:55,257:WARNING: 	 transformer_layers: 12
2024-04-28 04:37:55,257:WARNING: 		 linear_patch: 2d
2024-04-28 04:37:55,257:WARNING: 	 cut_top_layer: 0
2024-04-28 04:37:56,137:WARNING: 	 sim_header: meanP
2024-04-28 04:37:59,806:INFO: --------------------
2024-04-28 04:37:59,806:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 04:38:29,347:INFO: ***** Running test *****
2024-04-28 04:38:29,743:INFO:   Num examples = 1000
2024-04-28 04:38:30,108:INFO:   Batch size = 16
2024-04-28 04:38:30,533:INFO:   Num steps = 63
2024-04-28 04:38:31,122:INFO: ***** Running val *****
2024-04-28 04:38:31,459:INFO:   Num examples = 1000
2024-04-28 05:06:49,196:INFO: Effective parameters:
2024-04-28 05:06:49,196:INFO:   <<< batch_size: 16
2024-04-28 05:06:49,196:INFO:   <<< batch_size_val: 16
2024-04-28 05:06:49,196:INFO:   <<< cache_dir: 
2024-04-28 05:06:49,196:INFO:   <<< coef_lr: 0.001
2024-04-28 05:06:49,196:INFO:   <<< cross_model: cross-base
2024-04-28 05:06:49,196:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 05:06:49,196:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 05:06:49,196:INFO:   <<< datatype: msrvtt
2024-04-28 05:06:49,196:INFO:   <<< do_eval: True
2024-04-28 05:06:49,196:INFO:   <<< do_lower_case: False
2024-04-28 05:06:49,197:INFO:   <<< do_pretrain: False
2024-04-28 05:06:49,197:INFO:   <<< do_train: False
2024-04-28 05:06:49,197:INFO:   <<< epochs: 2
2024-04-28 05:06:49,197:INFO:   <<< eval_frame_order: 0
2024-04-28 05:06:49,197:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 05:06:49,197:INFO:   <<< feature_framerate: 1
2024-04-28 05:06:49,197:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 05:06:49,197:INFO:   <<< fp16: False
2024-04-28 05:06:49,197:INFO:   <<< fp16_opt_level: O1
2024-04-28 05:06:49,197:INFO:   <<< freeze_layer_num: 0
2024-04-28 05:06:49,197:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 05:06:49,197:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 05:06:49,197:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 05:06:49,197:INFO:   <<< linear_patch: 2d
2024-04-28 05:06:49,197:INFO:   <<< local_rank: 0
2024-04-28 05:06:49,197:INFO:   <<< loose_type: True
2024-04-28 05:06:49,198:INFO:   <<< lr: 0.0001
2024-04-28 05:06:49,198:INFO:   <<< lr_decay: 0.9
2024-04-28 05:06:49,198:INFO:   <<< margin: 0.1
2024-04-28 05:06:49,198:INFO:   <<< max_frames: 12
2024-04-28 05:06:49,198:INFO:   <<< max_words: 32
2024-04-28 05:06:49,198:INFO:   <<< n_display: 100
2024-04-28 05:06:49,198:INFO:   <<< n_gpu: 1
2024-04-28 05:06:49,198:INFO:   <<< n_pair: 1
2024-04-28 05:06:49,198:INFO:   <<< negative_weighting: 1
2024-04-28 05:06:49,198:INFO:   <<< num_thread_reader: 0
2024-04-28 05:06:49,198:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 05:06:49,198:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 05:06:49,198:INFO:   <<< rank: 0
2024-04-28 05:06:49,198:INFO:   <<< resume_model: None
2024-04-28 05:06:49,198:INFO:   <<< sampled_use_mil: False
2024-04-28 05:06:49,198:INFO:   <<< seed: 42
2024-04-28 05:06:49,199:INFO:   <<< sim_header: meanP
2024-04-28 05:06:49,199:INFO:   <<< slice_framepos: 2
2024-04-28 05:06:49,199:INFO:   <<< task_type: retrieval
2024-04-28 05:06:49,199:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 05:06:49,199:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 05:06:49,199:INFO:   <<< train_frame_order: 0
2024-04-28 05:06:49,199:INFO:   <<< use_mil: False
2024-04-28 05:06:49,199:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 05:06:49,199:INFO:   <<< video_dim: 1024
2024-04-28 05:06:49,199:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 05:06:49,199:INFO:   <<< warmup_proportion: 0.1
2024-04-28 05:06:49,199:INFO:   <<< world_size: 1
2024-04-28 05:06:49,199:INFO: device: cuda:0 n_gpu: 1
2024-04-28 05:06:49,448:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 05:06:49,918:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 05:06:49,920:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 05:06:49,920:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 05:06:49,920:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 05:06:49,920:WARNING: Test retrieval by loose type.
2024-04-28 05:06:49,921:WARNING: 	 embed_dim: 512
2024-04-28 05:06:49,921:WARNING: 	 image_resolution: 224
2024-04-28 05:06:49,921:WARNING: 	 vision_layers: 12
2024-04-28 05:06:49,921:WARNING: 	 vision_width: 768
2024-04-28 05:06:49,921:WARNING: 	 vision_patch_size: 32
2024-04-28 05:06:49,921:WARNING: 	 context_length: 77
2024-04-28 05:06:49,921:WARNING: 	 vocab_size: 49408
2024-04-28 05:06:49,921:WARNING: 	 transformer_width: 512
2024-04-28 05:06:49,921:WARNING: 	 transformer_heads: 8
2024-04-28 05:06:49,921:WARNING: 	 transformer_layers: 12
2024-04-28 05:06:49,921:WARNING: 		 linear_patch: 2d
2024-04-28 05:06:49,921:WARNING: 	 cut_top_layer: 0
2024-04-28 05:06:50,794:WARNING: 	 sim_header: meanP
2024-04-28 05:06:54,513:INFO: --------------------
2024-04-28 05:06:54,513:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 05:07:28,108:INFO: ***** Running test *****
2024-04-28 05:07:28,108:INFO:   Num examples = 1000
2024-04-28 05:07:28,108:INFO:   Batch size = 16
2024-04-28 05:07:28,109:INFO:   Num steps = 63
2024-04-28 05:07:28,109:INFO: ***** Running val *****
2024-04-28 05:07:28,109:INFO:   Num examples = 1000
2024-04-28 08:36:59,796:INFO: Effective parameters:
2024-04-28 08:36:59,797:INFO:   <<< batch_size: 16
2024-04-28 08:36:59,797:INFO:   <<< batch_size_val: 16
2024-04-28 08:36:59,797:INFO:   <<< cache_dir: 
2024-04-28 08:36:59,797:INFO:   <<< coef_lr: 0.001
2024-04-28 08:36:59,797:INFO:   <<< cross_model: cross-base
2024-04-28 08:36:59,797:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 08:36:59,798:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 08:36:59,798:INFO:   <<< datatype: msrvtt
2024-04-28 08:36:59,798:INFO:   <<< do_eval: True
2024-04-28 08:36:59,798:INFO:   <<< do_lower_case: False
2024-04-28 08:36:59,798:INFO:   <<< do_pretrain: False
2024-04-28 08:36:59,798:INFO:   <<< do_train: False
2024-04-28 08:36:59,798:INFO:   <<< epochs: 2
2024-04-28 08:36:59,798:INFO:   <<< eval_frame_order: 0
2024-04-28 08:36:59,798:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 08:36:59,798:INFO:   <<< feature_framerate: 1
2024-04-28 08:36:59,798:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 08:36:59,798:INFO:   <<< fp16: False
2024-04-28 08:36:59,798:INFO:   <<< fp16_opt_level: O1
2024-04-28 08:36:59,798:INFO:   <<< freeze_layer_num: 0
2024-04-28 08:36:59,798:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 08:36:59,798:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 08:36:59,798:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 08:36:59,799:INFO:   <<< linear_patch: 2d
2024-04-28 08:36:59,799:INFO:   <<< local_rank: 0
2024-04-28 08:36:59,799:INFO:   <<< loose_type: True
2024-04-28 08:36:59,799:INFO:   <<< lr: 0.0001
2024-04-28 08:36:59,799:INFO:   <<< lr_decay: 0.9
2024-04-28 08:36:59,799:INFO:   <<< margin: 0.1
2024-04-28 08:36:59,799:INFO:   <<< max_frames: 12
2024-04-28 08:36:59,799:INFO:   <<< max_words: 32
2024-04-28 08:36:59,799:INFO:   <<< n_display: 100
2024-04-28 08:36:59,799:INFO:   <<< n_gpu: 1
2024-04-28 08:36:59,799:INFO:   <<< n_pair: 1
2024-04-28 08:36:59,799:INFO:   <<< negative_weighting: 1
2024-04-28 08:36:59,799:INFO:   <<< num_thread_reader: 0
2024-04-28 08:36:59,799:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 08:36:59,799:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 08:36:59,799:INFO:   <<< rank: 0
2024-04-28 08:36:59,799:INFO:   <<< resume_model: None
2024-04-28 08:36:59,800:INFO:   <<< sampled_use_mil: False
2024-04-28 08:36:59,800:INFO:   <<< seed: 42
2024-04-28 08:36:59,800:INFO:   <<< sim_header: meanP
2024-04-28 08:36:59,800:INFO:   <<< slice_framepos: 2
2024-04-28 08:36:59,800:INFO:   <<< task_type: retrieval
2024-04-28 08:36:59,800:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 08:36:59,800:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 08:36:59,800:INFO:   <<< train_frame_order: 0
2024-04-28 08:36:59,800:INFO:   <<< use_mil: False
2024-04-28 08:36:59,800:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 08:36:59,800:INFO:   <<< video_dim: 1024
2024-04-28 08:36:59,800:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 08:36:59,800:INFO:   <<< warmup_proportion: 0.1
2024-04-28 08:36:59,800:INFO:   <<< world_size: 1
2024-04-28 08:36:59,800:INFO: device: cuda:0 n_gpu: 1
2024-04-28 08:37:01,214:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 08:37:02,435:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 08:37:02,437:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 08:37:02,437:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 08:37:02,437:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 08:37:02,438:WARNING: Test retrieval by loose type.
2024-04-28 08:37:02,439:WARNING: 	 embed_dim: 512
2024-04-28 08:37:02,439:WARNING: 	 image_resolution: 224
2024-04-28 08:37:02,439:WARNING: 	 vision_layers: 12
2024-04-28 08:37:02,439:WARNING: 	 vision_width: 768
2024-04-28 08:37:02,439:WARNING: 	 vision_patch_size: 32
2024-04-28 08:37:02,439:WARNING: 	 context_length: 77
2024-04-28 08:37:02,439:WARNING: 	 vocab_size: 49408
2024-04-28 08:37:02,440:WARNING: 	 transformer_width: 512
2024-04-28 08:37:02,440:WARNING: 	 transformer_heads: 8
2024-04-28 08:37:02,440:WARNING: 	 transformer_layers: 12
2024-04-28 08:37:02,440:WARNING: 		 linear_patch: 2d
2024-04-28 08:37:02,440:WARNING: 	 cut_top_layer: 0
2024-04-28 08:37:03,336:WARNING: 	 sim_header: meanP
2024-04-28 08:37:07,041:INFO: --------------------
2024-04-28 08:37:07,042:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 08:37:11,405:INFO: ***** Running test *****
2024-04-28 08:37:11,405:INFO:   Num examples = 1000
2024-04-28 08:37:11,405:INFO:   Batch size = 16
2024-04-28 08:37:11,405:INFO:   Num steps = 63
2024-04-28 08:37:11,405:INFO: ***** Running val *****
2024-04-28 08:37:11,405:INFO:   Num examples = 1000
2024-04-28 08:37:31,392:INFO: Effective parameters:
2024-04-28 08:37:31,392:INFO:   <<< batch_size: 16
2024-04-28 08:37:31,392:INFO:   <<< batch_size_val: 16
2024-04-28 08:37:31,392:INFO:   <<< cache_dir: 
2024-04-28 08:37:31,392:INFO:   <<< coef_lr: 0.001
2024-04-28 08:37:31,392:INFO:   <<< cross_model: cross-base
2024-04-28 08:37:31,393:INFO:   <<< cross_num_hidden_layers: 4
2024-04-28 08:37:31,393:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-28 08:37:31,393:INFO:   <<< datatype: msrvtt
2024-04-28 08:37:31,393:INFO:   <<< do_eval: True
2024-04-28 08:37:31,393:INFO:   <<< do_lower_case: False
2024-04-28 08:37:31,393:INFO:   <<< do_pretrain: False
2024-04-28 08:37:31,393:INFO:   <<< do_train: False
2024-04-28 08:37:31,393:INFO:   <<< epochs: 2
2024-04-28 08:37:31,393:INFO:   <<< eval_frame_order: 0
2024-04-28 08:37:31,393:INFO:   <<< expand_msrvtt_sentences: True
2024-04-28 08:37:31,393:INFO:   <<< feature_framerate: 1
2024-04-28 08:37:31,393:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-28 08:37:31,393:INFO:   <<< fp16: False
2024-04-28 08:37:31,393:INFO:   <<< fp16_opt_level: O1
2024-04-28 08:37:31,393:INFO:   <<< freeze_layer_num: 0
2024-04-28 08:37:31,393:INFO:   <<< gradient_accumulation_steps: 1
2024-04-28 08:37:31,393:INFO:   <<< hard_negative_rate: 0.5
2024-04-28 08:37:31,394:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 08:37:31,394:INFO:   <<< linear_patch: 2d
2024-04-28 08:37:31,394:INFO:   <<< local_rank: 0
2024-04-28 08:37:31,394:INFO:   <<< loose_type: True
2024-04-28 08:37:31,394:INFO:   <<< lr: 0.0001
2024-04-28 08:37:31,394:INFO:   <<< lr_decay: 0.9
2024-04-28 08:37:31,394:INFO:   <<< margin: 0.1
2024-04-28 08:37:31,394:INFO:   <<< max_frames: 12
2024-04-28 08:37:31,394:INFO:   <<< max_words: 32
2024-04-28 08:37:31,394:INFO:   <<< n_display: 100
2024-04-28 08:37:31,394:INFO:   <<< n_gpu: 1
2024-04-28 08:37:31,394:INFO:   <<< n_pair: 1
2024-04-28 08:37:31,394:INFO:   <<< negative_weighting: 1
2024-04-28 08:37:31,394:INFO:   <<< num_thread_reader: 0
2024-04-28 08:37:31,394:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-28 08:37:31,394:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-28 08:37:31,394:INFO:   <<< rank: 0
2024-04-28 08:37:31,395:INFO:   <<< resume_model: None
2024-04-28 08:37:31,395:INFO:   <<< sampled_use_mil: False
2024-04-28 08:37:31,395:INFO:   <<< seed: 42
2024-04-28 08:37:31,395:INFO:   <<< sim_header: meanP
2024-04-28 08:37:31,395:INFO:   <<< slice_framepos: 2
2024-04-28 08:37:31,395:INFO:   <<< task_type: retrieval
2024-04-28 08:37:31,395:INFO:   <<< text_num_hidden_layers: 12
2024-04-28 08:37:31,395:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-28 08:37:31,395:INFO:   <<< train_frame_order: 0
2024-04-28 08:37:31,395:INFO:   <<< use_mil: False
2024-04-28 08:37:31,395:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-28 08:37:31,395:INFO:   <<< video_dim: 1024
2024-04-28 08:37:31,395:INFO:   <<< visual_num_hidden_layers: 12
2024-04-28 08:37:31,395:INFO:   <<< warmup_proportion: 0.1
2024-04-28 08:37:31,395:INFO:   <<< world_size: 1
2024-04-28 08:37:31,395:INFO: device: cuda:0 n_gpu: 1
2024-04-28 08:37:31,648:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-28 08:37:32,118:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-28 08:37:32,119:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-28 08:37:32,120:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-28 08:37:32,120:WARNING: Stage-One:True, Stage-Two:False
2024-04-28 08:37:32,120:WARNING: Test retrieval by loose type.
2024-04-28 08:37:32,120:WARNING: 	 embed_dim: 512
2024-04-28 08:37:32,121:WARNING: 	 image_resolution: 224
2024-04-28 08:37:32,121:WARNING: 	 vision_layers: 12
2024-04-28 08:37:32,121:WARNING: 	 vision_width: 768
2024-04-28 08:37:32,121:WARNING: 	 vision_patch_size: 32
2024-04-28 08:37:32,121:WARNING: 	 context_length: 77
2024-04-28 08:37:32,121:WARNING: 	 vocab_size: 49408
2024-04-28 08:37:32,121:WARNING: 	 transformer_width: 512
2024-04-28 08:37:32,121:WARNING: 	 transformer_heads: 8
2024-04-28 08:37:32,121:WARNING: 	 transformer_layers: 12
2024-04-28 08:37:32,121:WARNING: 		 linear_patch: 2d
2024-04-28 08:37:32,121:WARNING: 	 cut_top_layer: 0
2024-04-28 08:37:32,979:WARNING: 	 sim_header: meanP
2024-04-28 08:37:36,700:INFO: --------------------
2024-04-28 08:37:36,700:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-28 08:37:38,833:INFO: ***** Running test *****
2024-04-28 08:37:38,833:INFO:   Num examples = 1000
2024-04-28 08:37:38,833:INFO:   Batch size = 16
2024-04-28 08:37:38,833:INFO:   Num steps = 63
2024-04-28 08:37:38,833:INFO: ***** Running val *****
2024-04-28 08:37:38,833:INFO:   Num examples = 1000
2024-04-28 09:26:56,601:INFO: sim matrix size: 1000, 1000
2024-04-29 10:58:15,316:INFO: Effective parameters:
2024-04-29 10:58:15,316:INFO:   <<< batch_size: 16
2024-04-29 10:58:15,316:INFO:   <<< batch_size_val: 16
2024-04-29 10:58:15,316:INFO:   <<< cache_dir: 
2024-04-29 10:58:15,316:INFO:   <<< coef_lr: 0.001
2024-04-29 10:58:15,317:INFO:   <<< cross_model: cross-base
2024-04-29 10:58:15,317:INFO:   <<< cross_num_hidden_layers: 4
2024-04-29 10:58:15,317:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-29 10:58:15,317:INFO:   <<< datatype: msrvtt
2024-04-29 10:58:15,317:INFO:   <<< do_eval: True
2024-04-29 10:58:15,317:INFO:   <<< do_lower_case: False
2024-04-29 10:58:15,317:INFO:   <<< do_pretrain: False
2024-04-29 10:58:15,317:INFO:   <<< do_train: False
2024-04-29 10:58:15,317:INFO:   <<< epochs: 2
2024-04-29 10:58:15,317:INFO:   <<< eval_frame_order: 0
2024-04-29 10:58:15,317:INFO:   <<< expand_msrvtt_sentences: True
2024-04-29 10:58:15,317:INFO:   <<< feature_framerate: 1
2024-04-29 10:58:15,317:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-29 10:58:15,317:INFO:   <<< fp16: False
2024-04-29 10:58:15,317:INFO:   <<< fp16_opt_level: O1
2024-04-29 10:58:15,317:INFO:   <<< freeze_layer_num: 0
2024-04-29 10:58:15,317:INFO:   <<< gradient_accumulation_steps: 1
2024-04-29 10:58:15,318:INFO:   <<< hard_negative_rate: 0.5
2024-04-29 10:58:15,318:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 10:58:15,318:INFO:   <<< linear_patch: 2d
2024-04-29 10:58:15,318:INFO:   <<< local_rank: 0
2024-04-29 10:58:15,318:INFO:   <<< loose_type: True
2024-04-29 10:58:15,318:INFO:   <<< lr: 0.0001
2024-04-29 10:58:15,318:INFO:   <<< lr_decay: 0.9
2024-04-29 10:58:15,318:INFO:   <<< margin: 0.1
2024-04-29 10:58:15,318:INFO:   <<< max_frames: 12
2024-04-29 10:58:15,318:INFO:   <<< max_words: 32
2024-04-29 10:58:15,318:INFO:   <<< n_display: 100
2024-04-29 10:58:15,318:INFO:   <<< n_gpu: 1
2024-04-29 10:58:15,318:INFO:   <<< n_pair: 1
2024-04-29 10:58:15,318:INFO:   <<< negative_weighting: 1
2024-04-29 10:58:15,318:INFO:   <<< num_thread_reader: 0
2024-04-29 10:58:15,318:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-29 10:58:15,318:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-29 10:58:15,319:INFO:   <<< rank: 0
2024-04-29 10:58:15,319:INFO:   <<< resume_model: None
2024-04-29 10:58:15,319:INFO:   <<< sampled_use_mil: False
2024-04-29 10:58:15,319:INFO:   <<< seed: 42
2024-04-29 10:58:15,319:INFO:   <<< sim_header: meanP
2024-04-29 10:58:15,319:INFO:   <<< slice_framepos: 2
2024-04-29 10:58:15,319:INFO:   <<< task_type: retrieval
2024-04-29 10:58:15,319:INFO:   <<< text_num_hidden_layers: 12
2024-04-29 10:58:15,319:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-29 10:58:15,319:INFO:   <<< train_frame_order: 0
2024-04-29 10:58:15,319:INFO:   <<< use_mil: False
2024-04-29 10:58:15,319:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-29 10:58:15,319:INFO:   <<< video_dim: 1024
2024-04-29 10:58:15,319:INFO:   <<< visual_num_hidden_layers: 12
2024-04-29 10:58:15,319:INFO:   <<< warmup_proportion: 0.1
2024-04-29 10:58:15,319:INFO:   <<< world_size: 1
2024-04-29 10:58:15,319:INFO: device: cuda:0 n_gpu: 1
2024-04-29 10:58:16,725:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 10:58:17,947:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-29 10:58:17,949:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-29 10:58:17,949:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-29 10:58:17,949:WARNING: Stage-One:True, Stage-Two:False
2024-04-29 10:58:17,949:WARNING: Test retrieval by loose type.
2024-04-29 10:58:17,950:WARNING: 	 embed_dim: 512
2024-04-29 10:58:17,950:WARNING: 	 image_resolution: 224
2024-04-29 10:58:17,950:WARNING: 	 vision_layers: 12
2024-04-29 10:58:17,950:WARNING: 	 vision_width: 768
2024-04-29 10:58:17,950:WARNING: 	 vision_patch_size: 32
2024-04-29 10:58:17,950:WARNING: 	 context_length: 77
2024-04-29 10:58:17,950:WARNING: 	 vocab_size: 49408
2024-04-29 10:58:17,950:WARNING: 	 transformer_width: 512
2024-04-29 10:58:17,950:WARNING: 	 transformer_heads: 8
2024-04-29 10:58:17,951:WARNING: 	 transformer_layers: 12
2024-04-29 10:58:17,951:WARNING: 		 linear_patch: 2d
2024-04-29 10:58:17,951:WARNING: 	 cut_top_layer: 0
2024-04-29 10:58:18,819:WARNING: 	 sim_header: meanP
2024-04-29 10:58:22,465:INFO: --------------------
2024-04-29 10:58:22,465:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-29 10:58:26,863:INFO: ***** Running test *****
2024-04-29 10:58:26,864:INFO:   Num examples = 1000
2024-04-29 10:58:26,864:INFO:   Batch size = 16
2024-04-29 10:58:26,864:INFO:   Num steps = 63
2024-04-29 10:58:26,864:INFO: ***** Running val *****
2024-04-29 10:58:26,864:INFO:   Num examples = 1000
2024-04-29 10:59:27,047:INFO: Effective parameters:
2024-04-29 10:59:27,047:INFO:   <<< batch_size: 16
2024-04-29 10:59:27,048:INFO:   <<< batch_size_val: 16
2024-04-29 10:59:27,048:INFO:   <<< cache_dir: 
2024-04-29 10:59:27,048:INFO:   <<< coef_lr: 0.001
2024-04-29 10:59:27,048:INFO:   <<< cross_model: cross-base
2024-04-29 10:59:27,048:INFO:   <<< cross_num_hidden_layers: 4
2024-04-29 10:59:27,048:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-29 10:59:27,048:INFO:   <<< datatype: msrvtt
2024-04-29 10:59:27,048:INFO:   <<< do_eval: True
2024-04-29 10:59:27,048:INFO:   <<< do_lower_case: False
2024-04-29 10:59:27,048:INFO:   <<< do_pretrain: False
2024-04-29 10:59:27,048:INFO:   <<< do_train: False
2024-04-29 10:59:27,048:INFO:   <<< epochs: 2
2024-04-29 10:59:27,048:INFO:   <<< eval_frame_order: 0
2024-04-29 10:59:27,048:INFO:   <<< expand_msrvtt_sentences: True
2024-04-29 10:59:27,049:INFO:   <<< feature_framerate: 1
2024-04-29 10:59:27,049:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-29 10:59:27,049:INFO:   <<< fp16: False
2024-04-29 10:59:27,049:INFO:   <<< fp16_opt_level: O1
2024-04-29 10:59:27,049:INFO:   <<< freeze_layer_num: 0
2024-04-29 10:59:27,049:INFO:   <<< gradient_accumulation_steps: 1
2024-04-29 10:59:27,049:INFO:   <<< hard_negative_rate: 0.5
2024-04-29 10:59:27,049:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 10:59:27,049:INFO:   <<< linear_patch: 2d
2024-04-29 10:59:27,049:INFO:   <<< local_rank: 0
2024-04-29 10:59:27,049:INFO:   <<< loose_type: True
2024-04-29 10:59:27,049:INFO:   <<< lr: 0.0001
2024-04-29 10:59:27,049:INFO:   <<< lr_decay: 0.9
2024-04-29 10:59:27,049:INFO:   <<< margin: 0.1
2024-04-29 10:59:27,050:INFO:   <<< max_frames: 12
2024-04-29 10:59:27,050:INFO:   <<< max_words: 32
2024-04-29 10:59:27,050:INFO:   <<< n_display: 100
2024-04-29 10:59:27,050:INFO:   <<< n_gpu: 1
2024-04-29 10:59:27,050:INFO:   <<< n_pair: 1
2024-04-29 10:59:27,050:INFO:   <<< negative_weighting: 1
2024-04-29 10:59:27,050:INFO:   <<< num_thread_reader: 0
2024-04-29 10:59:27,050:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-29 10:59:27,050:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-29 10:59:27,050:INFO:   <<< rank: 0
2024-04-29 10:59:27,050:INFO:   <<< resume_model: None
2024-04-29 10:59:27,050:INFO:   <<< sampled_use_mil: False
2024-04-29 10:59:27,050:INFO:   <<< seed: 42
2024-04-29 10:59:27,050:INFO:   <<< sim_header: meanP
2024-04-29 10:59:27,050:INFO:   <<< slice_framepos: 2
2024-04-29 10:59:27,051:INFO:   <<< task_type: retrieval
2024-04-29 10:59:27,051:INFO:   <<< text_num_hidden_layers: 12
2024-04-29 10:59:27,051:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-29 10:59:27,051:INFO:   <<< train_frame_order: 0
2024-04-29 10:59:27,051:INFO:   <<< use_mil: False
2024-04-29 10:59:27,051:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-29 10:59:27,051:INFO:   <<< video_dim: 1024
2024-04-29 10:59:27,051:INFO:   <<< visual_num_hidden_layers: 12
2024-04-29 10:59:27,051:INFO:   <<< warmup_proportion: 0.1
2024-04-29 10:59:27,051:INFO:   <<< world_size: 1
2024-04-29 10:59:27,051:INFO: device: cuda:0 n_gpu: 1
2024-04-29 10:59:27,298:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 10:59:27,762:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-29 10:59:27,763:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-29 10:59:27,763:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-29 10:59:27,764:WARNING: Stage-One:True, Stage-Two:False
2024-04-29 10:59:27,764:WARNING: Test retrieval by loose type.
2024-04-29 10:59:27,764:WARNING: 	 embed_dim: 512
2024-04-29 10:59:27,764:WARNING: 	 image_resolution: 224
2024-04-29 10:59:27,765:WARNING: 	 vision_layers: 12
2024-04-29 10:59:27,765:WARNING: 	 vision_width: 768
2024-04-29 10:59:27,765:WARNING: 	 vision_patch_size: 32
2024-04-29 10:59:27,765:WARNING: 	 context_length: 77
2024-04-29 10:59:27,765:WARNING: 	 vocab_size: 49408
2024-04-29 10:59:27,765:WARNING: 	 transformer_width: 512
2024-04-29 10:59:27,765:WARNING: 	 transformer_heads: 8
2024-04-29 10:59:27,765:WARNING: 	 transformer_layers: 12
2024-04-29 10:59:27,765:WARNING: 		 linear_patch: 2d
2024-04-29 10:59:27,765:WARNING: 	 cut_top_layer: 0
2024-04-29 10:59:28,604:WARNING: 	 sim_header: meanP
2024-04-29 10:59:32,215:INFO: --------------------
2024-04-29 10:59:32,215:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-29 10:59:34,329:INFO: ***** Running test *****
2024-04-29 10:59:34,329:INFO:   Num examples = 22
2024-04-29 10:59:34,329:INFO:   Batch size = 16
2024-04-29 10:59:34,330:INFO:   Num steps = 2
2024-04-29 10:59:34,330:INFO: ***** Running val *****
2024-04-29 10:59:34,330:INFO:   Num examples = 22
2024-04-29 11:00:43,780:INFO: Effective parameters:
2024-04-29 11:00:43,780:INFO:   <<< batch_size: 16
2024-04-29 11:00:43,781:INFO:   <<< batch_size_val: 16
2024-04-29 11:00:43,781:INFO:   <<< cache_dir: 
2024-04-29 11:00:43,781:INFO:   <<< coef_lr: 0.001
2024-04-29 11:00:43,781:INFO:   <<< cross_model: cross-base
2024-04-29 11:00:43,781:INFO:   <<< cross_num_hidden_layers: 4
2024-04-29 11:00:43,781:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-29 11:00:43,781:INFO:   <<< datatype: msrvtt
2024-04-29 11:00:43,781:INFO:   <<< do_eval: True
2024-04-29 11:00:43,781:INFO:   <<< do_lower_case: False
2024-04-29 11:00:43,781:INFO:   <<< do_pretrain: False
2024-04-29 11:00:43,781:INFO:   <<< do_train: False
2024-04-29 11:00:43,781:INFO:   <<< epochs: 2
2024-04-29 11:00:43,781:INFO:   <<< eval_frame_order: 0
2024-04-29 11:00:43,781:INFO:   <<< expand_msrvtt_sentences: True
2024-04-29 11:00:43,781:INFO:   <<< feature_framerate: 1
2024-04-29 11:00:43,781:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-29 11:00:43,782:INFO:   <<< fp16: False
2024-04-29 11:00:43,782:INFO:   <<< fp16_opt_level: O1
2024-04-29 11:00:43,782:INFO:   <<< freeze_layer_num: 0
2024-04-29 11:00:43,782:INFO:   <<< gradient_accumulation_steps: 1
2024-04-29 11:00:43,782:INFO:   <<< hard_negative_rate: 0.5
2024-04-29 11:00:43,782:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 11:00:43,782:INFO:   <<< linear_patch: 2d
2024-04-29 11:00:43,782:INFO:   <<< local_rank: 0
2024-04-29 11:00:43,782:INFO:   <<< loose_type: True
2024-04-29 11:00:43,782:INFO:   <<< lr: 0.0001
2024-04-29 11:00:43,782:INFO:   <<< lr_decay: 0.9
2024-04-29 11:00:43,782:INFO:   <<< margin: 0.1
2024-04-29 11:00:43,782:INFO:   <<< max_frames: 12
2024-04-29 11:00:43,782:INFO:   <<< max_words: 32
2024-04-29 11:00:43,782:INFO:   <<< n_display: 100
2024-04-29 11:00:43,782:INFO:   <<< n_gpu: 1
2024-04-29 11:00:43,782:INFO:   <<< n_pair: 1
2024-04-29 11:00:43,782:INFO:   <<< negative_weighting: 1
2024-04-29 11:00:43,783:INFO:   <<< num_thread_reader: 0
2024-04-29 11:00:43,783:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-29 11:00:43,783:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-29 11:00:43,783:INFO:   <<< rank: 0
2024-04-29 11:00:43,783:INFO:   <<< resume_model: None
2024-04-29 11:00:43,783:INFO:   <<< sampled_use_mil: False
2024-04-29 11:00:43,783:INFO:   <<< seed: 42
2024-04-29 11:00:43,783:INFO:   <<< sim_header: meanP
2024-04-29 11:00:43,783:INFO:   <<< slice_framepos: 2
2024-04-29 11:00:43,783:INFO:   <<< task_type: retrieval
2024-04-29 11:00:43,783:INFO:   <<< text_num_hidden_layers: 12
2024-04-29 11:00:43,783:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-29 11:00:43,783:INFO:   <<< train_frame_order: 0
2024-04-29 11:00:43,783:INFO:   <<< use_mil: False
2024-04-29 11:00:43,783:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-29 11:00:43,783:INFO:   <<< video_dim: 1024
2024-04-29 11:00:43,783:INFO:   <<< visual_num_hidden_layers: 12
2024-04-29 11:00:43,784:INFO:   <<< warmup_proportion: 0.1
2024-04-29 11:00:43,784:INFO:   <<< world_size: 1
2024-04-29 11:00:43,784:INFO: device: cuda:0 n_gpu: 1
2024-04-29 11:00:44,025:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 11:00:44,483:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-29 11:00:44,484:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-29 11:00:44,484:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-29 11:00:44,485:WARNING: Stage-One:True, Stage-Two:False
2024-04-29 11:00:44,485:WARNING: Test retrieval by loose type.
2024-04-29 11:00:44,485:WARNING: 	 embed_dim: 512
2024-04-29 11:00:44,485:WARNING: 	 image_resolution: 224
2024-04-29 11:00:44,485:WARNING: 	 vision_layers: 12
2024-04-29 11:00:44,486:WARNING: 	 vision_width: 768
2024-04-29 11:00:44,486:WARNING: 	 vision_patch_size: 32
2024-04-29 11:00:44,486:WARNING: 	 context_length: 77
2024-04-29 11:00:44,486:WARNING: 	 vocab_size: 49408
2024-04-29 11:00:44,486:WARNING: 	 transformer_width: 512
2024-04-29 11:00:44,486:WARNING: 	 transformer_heads: 8
2024-04-29 11:00:44,486:WARNING: 	 transformer_layers: 12
2024-04-29 11:00:44,486:WARNING: 		 linear_patch: 2d
2024-04-29 11:00:44,486:WARNING: 	 cut_top_layer: 0
2024-04-29 11:00:45,329:WARNING: 	 sim_header: meanP
2024-04-29 11:00:48,946:INFO: --------------------
2024-04-29 11:00:48,946:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-29 11:00:51,068:INFO: ***** Running test *****
2024-04-29 11:00:51,068:INFO:   Num examples = 22
2024-04-29 11:00:51,068:INFO:   Batch size = 16
2024-04-29 11:00:51,068:INFO:   Num steps = 2
2024-04-29 11:00:51,069:INFO: ***** Running val *****
2024-04-29 11:00:51,069:INFO:   Num examples = 22
2024-04-29 12:54:42,593:INFO: Effective parameters:
2024-04-29 12:54:42,593:INFO:   <<< batch_size: 16
2024-04-29 12:54:42,593:INFO:   <<< batch_size_val: 16
2024-04-29 12:54:42,593:INFO:   <<< cache_dir: 
2024-04-29 12:54:42,593:INFO:   <<< coef_lr: 0.001
2024-04-29 12:54:42,593:INFO:   <<< cross_model: cross-base
2024-04-29 12:54:42,593:INFO:   <<< cross_num_hidden_layers: 4
2024-04-29 12:54:42,594:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-29 12:54:42,594:INFO:   <<< datatype: msrvtt
2024-04-29 12:54:42,594:INFO:   <<< do_eval: True
2024-04-29 12:54:42,594:INFO:   <<< do_lower_case: False
2024-04-29 12:54:42,594:INFO:   <<< do_pretrain: False
2024-04-29 12:54:42,594:INFO:   <<< do_train: False
2024-04-29 12:54:42,594:INFO:   <<< epochs: 2
2024-04-29 12:54:42,594:INFO:   <<< eval_frame_order: 0
2024-04-29 12:54:42,594:INFO:   <<< expand_msrvtt_sentences: True
2024-04-29 12:54:42,594:INFO:   <<< feature_framerate: 1
2024-04-29 12:54:42,594:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-29 12:54:42,594:INFO:   <<< fp16: False
2024-04-29 12:54:42,594:INFO:   <<< fp16_opt_level: O1
2024-04-29 12:54:42,594:INFO:   <<< freeze_layer_num: 0
2024-04-29 12:54:42,594:INFO:   <<< gradient_accumulation_steps: 1
2024-04-29 12:54:42,594:INFO:   <<< hard_negative_rate: 0.5
2024-04-29 12:54:42,594:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 12:54:42,595:INFO:   <<< linear_patch: 2d
2024-04-29 12:54:42,595:INFO:   <<< local_rank: 0
2024-04-29 12:54:42,595:INFO:   <<< loose_type: True
2024-04-29 12:54:42,595:INFO:   <<< lr: 0.0001
2024-04-29 12:54:42,595:INFO:   <<< lr_decay: 0.9
2024-04-29 12:54:42,595:INFO:   <<< margin: 0.1
2024-04-29 12:54:42,595:INFO:   <<< max_frames: 12
2024-04-29 12:54:42,595:INFO:   <<< max_words: 32
2024-04-29 12:54:42,595:INFO:   <<< n_display: 100
2024-04-29 12:54:42,595:INFO:   <<< n_gpu: 1
2024-04-29 12:54:42,595:INFO:   <<< n_pair: 1
2024-04-29 12:54:42,595:INFO:   <<< negative_weighting: 1
2024-04-29 12:54:42,595:INFO:   <<< num_thread_reader: 0
2024-04-29 12:54:42,595:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-29 12:54:42,595:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-29 12:54:42,595:INFO:   <<< rank: 0
2024-04-29 12:54:42,595:INFO:   <<< resume_model: None
2024-04-29 12:54:42,595:INFO:   <<< sampled_use_mil: False
2024-04-29 12:54:42,596:INFO:   <<< seed: 42
2024-04-29 12:54:42,596:INFO:   <<< sim_header: meanP
2024-04-29 12:54:42,596:INFO:   <<< slice_framepos: 2
2024-04-29 12:54:42,596:INFO:   <<< task_type: retrieval
2024-04-29 12:54:42,596:INFO:   <<< text_num_hidden_layers: 12
2024-04-29 12:54:42,596:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-29 12:54:42,596:INFO:   <<< train_frame_order: 0
2024-04-29 12:54:42,596:INFO:   <<< use_mil: False
2024-04-29 12:54:42,596:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-29 12:54:42,596:INFO:   <<< video_dim: 1024
2024-04-29 12:54:42,596:INFO:   <<< visual_num_hidden_layers: 12
2024-04-29 12:54:42,596:INFO:   <<< warmup_proportion: 0.1
2024-04-29 12:54:42,596:INFO:   <<< world_size: 1
2024-04-29 12:54:42,596:INFO: device: cuda:0 n_gpu: 1
2024-04-29 12:54:42,831:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 12:54:43,304:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-29 12:54:43,306:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-29 12:54:43,307:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-29 12:54:43,307:WARNING: Stage-One:True, Stage-Two:False
2024-04-29 12:54:43,307:WARNING: Test retrieval by loose type.
2024-04-29 12:54:43,309:WARNING: 	 embed_dim: 512
2024-04-29 12:54:43,309:WARNING: 	 image_resolution: 224
2024-04-29 12:54:43,309:WARNING: 	 vision_layers: 12
2024-04-29 12:54:43,309:WARNING: 	 vision_width: 768
2024-04-29 12:54:43,309:WARNING: 	 vision_patch_size: 32
2024-04-29 12:54:43,309:WARNING: 	 context_length: 77
2024-04-29 12:54:43,309:WARNING: 	 vocab_size: 49408
2024-04-29 12:54:43,309:WARNING: 	 transformer_width: 512
2024-04-29 12:54:43,310:WARNING: 	 transformer_heads: 8
2024-04-29 12:54:43,310:WARNING: 	 transformer_layers: 12
2024-04-29 12:54:43,310:WARNING: 		 linear_patch: 2d
2024-04-29 12:54:43,310:WARNING: 	 cut_top_layer: 0
2024-04-29 12:54:44,164:WARNING: 	 sim_header: meanP
2024-04-29 12:54:47,836:INFO: --------------------
2024-04-29 12:54:47,837:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-29 12:54:49,943:INFO: ***** Running test *****
2024-04-29 12:54:49,944:INFO:   Num examples = 22
2024-04-29 12:54:49,944:INFO:   Batch size = 16
2024-04-29 12:54:49,944:INFO:   Num steps = 2
2024-04-29 12:54:49,944:INFO: ***** Running val *****
2024-04-29 12:54:49,944:INFO:   Num examples = 22
2024-04-29 13:58:58,351:INFO: Effective parameters:
2024-04-29 13:58:58,352:INFO:   <<< batch_size: 16
2024-04-29 13:58:58,352:INFO:   <<< batch_size_val: 16
2024-04-29 13:58:58,352:INFO:   <<< cache_dir: 
2024-04-29 13:58:58,352:INFO:   <<< coef_lr: 0.001
2024-04-29 13:58:58,352:INFO:   <<< cross_model: cross-base
2024-04-29 13:58:58,352:INFO:   <<< cross_num_hidden_layers: 4
2024-04-29 13:58:58,352:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-04-29 13:58:58,352:INFO:   <<< datatype: msrvtt
2024-04-29 13:58:58,352:INFO:   <<< do_eval: True
2024-04-29 13:58:58,352:INFO:   <<< do_lower_case: False
2024-04-29 13:58:58,352:INFO:   <<< do_pretrain: False
2024-04-29 13:58:58,352:INFO:   <<< do_train: False
2024-04-29 13:58:58,352:INFO:   <<< epochs: 2
2024-04-29 13:58:58,352:INFO:   <<< eval_frame_order: 0
2024-04-29 13:58:58,352:INFO:   <<< expand_msrvtt_sentences: True
2024-04-29 13:58:58,352:INFO:   <<< feature_framerate: 1
2024-04-29 13:58:58,353:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-04-29 13:58:58,353:INFO:   <<< fp16: False
2024-04-29 13:58:58,353:INFO:   <<< fp16_opt_level: O1
2024-04-29 13:58:58,353:INFO:   <<< freeze_layer_num: 0
2024-04-29 13:58:58,353:INFO:   <<< gradient_accumulation_steps: 1
2024-04-29 13:58:58,353:INFO:   <<< hard_negative_rate: 0.5
2024-04-29 13:58:58,353:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 13:58:58,353:INFO:   <<< linear_patch: 2d
2024-04-29 13:58:58,353:INFO:   <<< local_rank: 0
2024-04-29 13:58:58,353:INFO:   <<< loose_type: True
2024-04-29 13:58:58,353:INFO:   <<< lr: 0.0001
2024-04-29 13:58:58,353:INFO:   <<< lr_decay: 0.9
2024-04-29 13:58:58,353:INFO:   <<< margin: 0.1
2024-04-29 13:58:58,353:INFO:   <<< max_frames: 12
2024-04-29 13:58:58,353:INFO:   <<< max_words: 32
2024-04-29 13:58:58,353:INFO:   <<< n_display: 100
2024-04-29 13:58:58,353:INFO:   <<< n_gpu: 1
2024-04-29 13:58:58,353:INFO:   <<< n_pair: 1
2024-04-29 13:58:58,354:INFO:   <<< negative_weighting: 1
2024-04-29 13:58:58,354:INFO:   <<< num_thread_reader: 0
2024-04-29 13:58:58,354:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-04-29 13:58:58,354:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-04-29 13:58:58,354:INFO:   <<< rank: 0
2024-04-29 13:58:58,354:INFO:   <<< resume_model: None
2024-04-29 13:58:58,354:INFO:   <<< sampled_use_mil: False
2024-04-29 13:58:58,354:INFO:   <<< seed: 42
2024-04-29 13:58:58,354:INFO:   <<< sim_header: meanP
2024-04-29 13:58:58,354:INFO:   <<< slice_framepos: 2
2024-04-29 13:58:58,354:INFO:   <<< task_type: retrieval
2024-04-29 13:58:58,354:INFO:   <<< text_num_hidden_layers: 12
2024-04-29 13:58:58,354:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-04-29 13:58:58,354:INFO:   <<< train_frame_order: 0
2024-04-29 13:58:58,354:INFO:   <<< use_mil: False
2024-04-29 13:58:58,354:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-04-29 13:58:58,354:INFO:   <<< video_dim: 1024
2024-04-29 13:58:58,355:INFO:   <<< visual_num_hidden_layers: 12
2024-04-29 13:58:58,355:INFO:   <<< warmup_proportion: 0.1
2024-04-29 13:58:58,355:INFO:   <<< world_size: 1
2024-04-29 13:58:58,355:INFO: device: cuda:0 n_gpu: 1
2024-04-29 13:58:58,583:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-04-29 13:58:59,053:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-04-29 13:58:59,054:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-04-29 13:58:59,055:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-04-29 13:58:59,055:WARNING: Stage-One:True, Stage-Two:False
2024-04-29 13:58:59,055:WARNING: Test retrieval by loose type.
2024-04-29 13:58:59,056:WARNING: 	 embed_dim: 512
2024-04-29 13:58:59,056:WARNING: 	 image_resolution: 224
2024-04-29 13:58:59,057:WARNING: 	 vision_layers: 12
2024-04-29 13:58:59,057:WARNING: 	 vision_width: 768
2024-04-29 13:58:59,057:WARNING: 	 vision_patch_size: 32
2024-04-29 13:58:59,057:WARNING: 	 context_length: 77
2024-04-29 13:58:59,057:WARNING: 	 vocab_size: 49408
2024-04-29 13:58:59,057:WARNING: 	 transformer_width: 512
2024-04-29 13:58:59,057:WARNING: 	 transformer_heads: 8
2024-04-29 13:58:59,057:WARNING: 	 transformer_layers: 12
2024-04-29 13:58:59,057:WARNING: 		 linear_patch: 2d
2024-04-29 13:58:59,058:WARNING: 	 cut_top_layer: 0
2024-04-29 13:58:59,899:WARNING: 	 sim_header: meanP
2024-04-29 13:59:03,511:INFO: --------------------
2024-04-29 13:59:03,511:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-04-29 13:59:05,531:INFO: ***** Running test *****
2024-04-29 13:59:05,531:INFO:   Num examples = 22
2024-04-29 13:59:05,531:INFO:   Batch size = 16
2024-04-29 13:59:05,531:INFO:   Num steps = 2
2024-04-29 13:59:05,531:INFO: ***** Running val *****
2024-04-29 13:59:05,531:INFO:   Num examples = 22
2024-05-02 09:26:30,352:INFO: Effective parameters:
2024-05-02 09:26:30,353:INFO:   <<< batch_size: 16
2024-05-02 09:26:30,353:INFO:   <<< batch_size_val: 16
2024-05-02 09:26:30,353:INFO:   <<< cache_dir: 
2024-05-02 09:26:30,353:INFO:   <<< coef_lr: 0.001
2024-05-02 09:26:30,353:INFO:   <<< cross_model: cross-base
2024-05-02 09:26:30,353:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 09:26:30,353:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 09:26:30,353:INFO:   <<< datatype: msrvtt
2024-05-02 09:26:30,353:INFO:   <<< do_eval: True
2024-05-02 09:26:30,353:INFO:   <<< do_lower_case: False
2024-05-02 09:26:30,353:INFO:   <<< do_pretrain: False
2024-05-02 09:26:30,353:INFO:   <<< do_train: False
2024-05-02 09:26:30,353:INFO:   <<< epochs: 2
2024-05-02 09:26:30,353:INFO:   <<< eval_frame_order: 0
2024-05-02 09:26:30,354:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 09:26:30,354:INFO:   <<< feature_framerate: 1
2024-05-02 09:26:30,354:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 09:26:30,354:INFO:   <<< fp16: False
2024-05-02 09:26:30,354:INFO:   <<< fp16_opt_level: O1
2024-05-02 09:26:30,354:INFO:   <<< freeze_layer_num: 0
2024-05-02 09:26:30,354:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 09:26:30,354:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 09:26:30,354:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 09:26:30,354:INFO:   <<< linear_patch: 2d
2024-05-02 09:26:30,354:INFO:   <<< local_rank: 0
2024-05-02 09:26:30,354:INFO:   <<< loose_type: True
2024-05-02 09:26:30,354:INFO:   <<< lr: 0.0001
2024-05-02 09:26:30,354:INFO:   <<< lr_decay: 0.9
2024-05-02 09:26:30,354:INFO:   <<< margin: 0.1
2024-05-02 09:26:30,354:INFO:   <<< max_frames: 12
2024-05-02 09:26:30,354:INFO:   <<< max_words: 32
2024-05-02 09:26:30,354:INFO:   <<< n_display: 100
2024-05-02 09:26:30,355:INFO:   <<< n_gpu: 1
2024-05-02 09:26:30,355:INFO:   <<< n_pair: 1
2024-05-02 09:26:30,355:INFO:   <<< negative_weighting: 1
2024-05-02 09:26:30,355:INFO:   <<< num_thread_reader: 0
2024-05-02 09:26:30,355:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 09:26:30,355:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 09:26:30,355:INFO:   <<< rank: 0
2024-05-02 09:26:30,355:INFO:   <<< resume_model: None
2024-05-02 09:26:30,355:INFO:   <<< sampled_use_mil: False
2024-05-02 09:26:30,355:INFO:   <<< seed: 42
2024-05-02 09:26:30,355:INFO:   <<< sim_header: meanP
2024-05-02 09:26:30,355:INFO:   <<< slice_framepos: 2
2024-05-02 09:26:30,355:INFO:   <<< task_type: retrieval
2024-05-02 09:26:30,355:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 09:26:30,355:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 09:26:30,355:INFO:   <<< train_frame_order: 0
2024-05-02 09:26:30,355:INFO:   <<< use_mil: False
2024-05-02 09:26:30,356:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 09:26:30,356:INFO:   <<< video_dim: 1024
2024-05-02 09:26:30,356:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 09:26:30,356:INFO:   <<< warmup_proportion: 0.1
2024-05-02 09:26:30,356:INFO:   <<< world_size: 1
2024-05-02 09:26:30,356:INFO: device: cuda:0 n_gpu: 1
2024-05-02 09:26:31,766:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 09:26:32,999:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 09:26:33,001:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 09:26:33,001:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 09:26:33,001:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 09:26:33,001:WARNING: Test retrieval by loose type.
2024-05-02 09:26:33,003:WARNING: 	 embed_dim: 512
2024-05-02 09:26:33,003:WARNING: 	 image_resolution: 224
2024-05-02 09:26:33,003:WARNING: 	 vision_layers: 12
2024-05-02 09:26:33,003:WARNING: 	 vision_width: 768
2024-05-02 09:26:33,003:WARNING: 	 vision_patch_size: 32
2024-05-02 09:26:33,003:WARNING: 	 context_length: 77
2024-05-02 09:26:33,003:WARNING: 	 vocab_size: 49408
2024-05-02 09:26:33,004:WARNING: 	 transformer_width: 512
2024-05-02 09:26:33,004:WARNING: 	 transformer_heads: 8
2024-05-02 09:26:33,004:WARNING: 	 transformer_layers: 12
2024-05-02 09:26:33,004:WARNING: 		 linear_patch: 2d
2024-05-02 09:26:33,004:WARNING: 	 cut_top_layer: 0
2024-05-02 09:26:33,892:WARNING: 	 sim_header: meanP
2024-05-02 09:26:37,614:INFO: --------------------
2024-05-02 09:26:37,614:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 09:26:41,952:INFO: ***** Running test *****
2024-05-02 09:26:41,952:INFO:   Num examples = 22
2024-05-02 09:26:41,953:INFO:   Batch size = 16
2024-05-02 09:26:41,953:INFO:   Num steps = 2
2024-05-02 09:26:41,953:INFO: ***** Running val *****
2024-05-02 09:26:41,953:INFO:   Num examples = 22
2024-05-02 10:18:07,885:INFO: Effective parameters:
2024-05-02 10:18:07,885:INFO:   <<< batch_size: 1
2024-05-02 10:18:07,885:INFO:   <<< batch_size_val: 16
2024-05-02 10:18:07,885:INFO:   <<< cache_dir: 
2024-05-02 10:18:07,885:INFO:   <<< coef_lr: 0.001
2024-05-02 10:18:07,885:INFO:   <<< cross_model: cross-base
2024-05-02 10:18:07,885:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:18:07,885:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:18:07,885:INFO:   <<< datatype: msrvtt
2024-05-02 10:18:07,886:INFO:   <<< do_eval: True
2024-05-02 10:18:07,886:INFO:   <<< do_lower_case: False
2024-05-02 10:18:07,886:INFO:   <<< do_pretrain: False
2024-05-02 10:18:07,886:INFO:   <<< do_train: False
2024-05-02 10:18:07,886:INFO:   <<< epochs: 2
2024-05-02 10:18:07,886:INFO:   <<< eval_frame_order: 0
2024-05-02 10:18:07,886:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:18:07,886:INFO:   <<< feature_framerate: 1
2024-05-02 10:18:07,886:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:18:07,886:INFO:   <<< fp16: False
2024-05-02 10:18:07,886:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:18:07,886:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:18:07,886:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:18:07,886:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:18:07,886:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:18:07,886:INFO:   <<< linear_patch: 2d
2024-05-02 10:18:07,886:INFO:   <<< local_rank: 0
2024-05-02 10:18:07,886:INFO:   <<< loose_type: True
2024-05-02 10:18:07,887:INFO:   <<< lr: 0.0001
2024-05-02 10:18:07,887:INFO:   <<< lr_decay: 0.9
2024-05-02 10:18:07,887:INFO:   <<< margin: 0.1
2024-05-02 10:18:07,887:INFO:   <<< max_frames: 90
2024-05-02 10:18:07,887:INFO:   <<< max_words: 32
2024-05-02 10:18:07,887:INFO:   <<< n_display: 100
2024-05-02 10:18:07,887:INFO:   <<< n_gpu: 1
2024-05-02 10:18:07,887:INFO:   <<< n_pair: 1
2024-05-02 10:18:07,887:INFO:   <<< negative_weighting: 1
2024-05-02 10:18:07,887:INFO:   <<< num_thread_reader: 0
2024-05-02 10:18:07,887:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:18:07,887:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:18:07,887:INFO:   <<< rank: 0
2024-05-02 10:18:07,887:INFO:   <<< resume_model: None
2024-05-02 10:18:07,887:INFO:   <<< sampled_use_mil: False
2024-05-02 10:18:07,887:INFO:   <<< seed: 42
2024-05-02 10:18:07,887:INFO:   <<< sim_header: meanP
2024-05-02 10:18:07,888:INFO:   <<< slice_framepos: 2
2024-05-02 10:18:07,888:INFO:   <<< task_type: retrieval
2024-05-02 10:18:07,888:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:18:07,888:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:18:07,888:INFO:   <<< train_frame_order: 0
2024-05-02 10:18:07,888:INFO:   <<< use_mil: False
2024-05-02 10:18:07,888:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:18:07,888:INFO:   <<< video_dim: 1024
2024-05-02 10:18:07,888:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:18:07,888:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:18:07,888:INFO:   <<< world_size: 1
2024-05-02 10:18:07,888:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:18:08,050:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:18:08,545:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:18:08,547:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:18:08,547:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:18:08,547:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:18:08,547:WARNING: Test retrieval by loose type.
2024-05-02 10:18:08,548:WARNING: 	 embed_dim: 512
2024-05-02 10:18:08,548:WARNING: 	 image_resolution: 224
2024-05-02 10:18:08,548:WARNING: 	 vision_layers: 12
2024-05-02 10:18:08,548:WARNING: 	 vision_width: 768
2024-05-02 10:18:08,548:WARNING: 	 vision_patch_size: 32
2024-05-02 10:18:08,548:WARNING: 	 context_length: 77
2024-05-02 10:18:08,548:WARNING: 	 vocab_size: 49408
2024-05-02 10:18:08,548:WARNING: 	 transformer_width: 512
2024-05-02 10:18:08,548:WARNING: 	 transformer_heads: 8
2024-05-02 10:18:08,548:WARNING: 	 transformer_layers: 12
2024-05-02 10:18:08,549:WARNING: 		 linear_patch: 2d
2024-05-02 10:18:08,549:WARNING: 	 cut_top_layer: 0
2024-05-02 10:18:09,441:WARNING: 	 sim_header: meanP
2024-05-02 10:18:13,123:INFO: --------------------
2024-05-02 10:18:13,124:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:33:37,528:INFO: Effective parameters:
2024-05-02 10:33:37,529:INFO:   <<< batch_size: 1
2024-05-02 10:33:37,529:INFO:   <<< batch_size_val: 16
2024-05-02 10:33:37,529:INFO:   <<< cache_dir: 
2024-05-02 10:33:37,529:INFO:   <<< coef_lr: 0.001
2024-05-02 10:33:37,529:INFO:   <<< cross_model: cross-base
2024-05-02 10:33:37,529:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:33:37,529:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:33:37,529:INFO:   <<< datatype: msrvtt
2024-05-02 10:33:37,529:INFO:   <<< do_eval: True
2024-05-02 10:33:37,529:INFO:   <<< do_lower_case: False
2024-05-02 10:33:37,529:INFO:   <<< do_pretrain: False
2024-05-02 10:33:37,529:INFO:   <<< do_train: False
2024-05-02 10:33:37,529:INFO:   <<< epochs: 2
2024-05-02 10:33:37,529:INFO:   <<< eval_frame_order: 0
2024-05-02 10:33:37,529:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:33:37,530:INFO:   <<< feature_framerate: 1
2024-05-02 10:33:37,530:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:33:37,530:INFO:   <<< fp16: False
2024-05-02 10:33:37,530:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:33:37,530:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:33:37,530:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:33:37,530:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:33:37,530:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:33:37,530:INFO:   <<< linear_patch: 2d
2024-05-02 10:33:37,530:INFO:   <<< local_rank: 0
2024-05-02 10:33:37,530:INFO:   <<< loose_type: True
2024-05-02 10:33:37,530:INFO:   <<< lr: 0.0001
2024-05-02 10:33:37,530:INFO:   <<< lr_decay: 0.9
2024-05-02 10:33:37,530:INFO:   <<< margin: 0.1
2024-05-02 10:33:37,530:INFO:   <<< max_frames: 90
2024-05-02 10:33:37,530:INFO:   <<< max_words: 32
2024-05-02 10:33:37,530:INFO:   <<< n_display: 100
2024-05-02 10:33:37,531:INFO:   <<< n_gpu: 1
2024-05-02 10:33:37,531:INFO:   <<< n_pair: 1
2024-05-02 10:33:37,531:INFO:   <<< negative_weighting: 1
2024-05-02 10:33:37,531:INFO:   <<< num_thread_reader: 0
2024-05-02 10:33:37,531:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:33:37,531:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:33:37,531:INFO:   <<< rank: 0
2024-05-02 10:33:37,531:INFO:   <<< resume_model: None
2024-05-02 10:33:37,531:INFO:   <<< sampled_use_mil: False
2024-05-02 10:33:37,531:INFO:   <<< seed: 42
2024-05-02 10:33:37,531:INFO:   <<< sim_header: meanP
2024-05-02 10:33:37,531:INFO:   <<< slice_framepos: 2
2024-05-02 10:33:37,531:INFO:   <<< task_type: retrieval
2024-05-02 10:33:37,531:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:33:37,531:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:33:37,531:INFO:   <<< train_frame_order: 0
2024-05-02 10:33:37,531:INFO:   <<< use_mil: False
2024-05-02 10:33:37,531:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:33:37,532:INFO:   <<< video_dim: 1024
2024-05-02 10:33:37,532:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:33:37,532:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:33:37,532:INFO:   <<< world_size: 1
2024-05-02 10:33:37,532:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:33:37,675:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:33:38,178:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:33:38,180:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:33:38,180:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:33:38,180:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:33:38,180:WARNING: Test retrieval by loose type.
2024-05-02 10:33:38,181:WARNING: 	 embed_dim: 512
2024-05-02 10:33:38,182:WARNING: 	 image_resolution: 224
2024-05-02 10:33:38,182:WARNING: 	 vision_layers: 12
2024-05-02 10:33:38,182:WARNING: 	 vision_width: 768
2024-05-02 10:33:38,182:WARNING: 	 vision_patch_size: 32
2024-05-02 10:33:38,182:WARNING: 	 context_length: 77
2024-05-02 10:33:38,182:WARNING: 	 vocab_size: 49408
2024-05-02 10:33:38,182:WARNING: 	 transformer_width: 512
2024-05-02 10:33:38,182:WARNING: 	 transformer_heads: 8
2024-05-02 10:33:38,182:WARNING: 	 transformer_layers: 12
2024-05-02 10:33:38,183:WARNING: 		 linear_patch: 2d
2024-05-02 10:33:38,183:WARNING: 	 cut_top_layer: 0
2024-05-02 10:33:39,083:WARNING: 	 sim_header: meanP
2024-05-02 10:33:42,815:INFO: --------------------
2024-05-02 10:33:42,815:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:39:30,835:INFO: Effective parameters:
2024-05-02 10:39:30,835:INFO:   <<< batch_size: 1
2024-05-02 10:39:30,835:INFO:   <<< batch_size_val: 16
2024-05-02 10:39:30,835:INFO:   <<< cache_dir: 
2024-05-02 10:39:30,835:INFO:   <<< coef_lr: 0.001
2024-05-02 10:39:30,835:INFO:   <<< cross_model: cross-base
2024-05-02 10:39:30,836:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:39:30,836:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:39:30,836:INFO:   <<< datatype: msrvtt
2024-05-02 10:39:30,836:INFO:   <<< do_eval: True
2024-05-02 10:39:30,836:INFO:   <<< do_lower_case: False
2024-05-02 10:39:30,836:INFO:   <<< do_pretrain: False
2024-05-02 10:39:30,836:INFO:   <<< do_train: False
2024-05-02 10:39:30,836:INFO:   <<< epochs: 2
2024-05-02 10:39:30,836:INFO:   <<< eval_frame_order: 0
2024-05-02 10:39:30,836:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:39:30,836:INFO:   <<< feature_framerate: 1
2024-05-02 10:39:30,836:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:39:30,836:INFO:   <<< fp16: False
2024-05-02 10:39:30,836:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:39:30,836:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:39:30,836:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:39:30,836:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:39:30,837:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:39:30,837:INFO:   <<< linear_patch: 2d
2024-05-02 10:39:30,837:INFO:   <<< local_rank: 0
2024-05-02 10:39:30,837:INFO:   <<< loose_type: True
2024-05-02 10:39:30,837:INFO:   <<< lr: 0.0001
2024-05-02 10:39:30,837:INFO:   <<< lr_decay: 0.9
2024-05-02 10:39:30,837:INFO:   <<< margin: 0.1
2024-05-02 10:39:30,837:INFO:   <<< max_frames: 90
2024-05-02 10:39:30,837:INFO:   <<< max_words: 32
2024-05-02 10:39:30,837:INFO:   <<< n_display: 100
2024-05-02 10:39:30,837:INFO:   <<< n_gpu: 1
2024-05-02 10:39:30,837:INFO:   <<< n_pair: 1
2024-05-02 10:39:30,837:INFO:   <<< negative_weighting: 1
2024-05-02 10:39:30,837:INFO:   <<< num_thread_reader: 0
2024-05-02 10:39:30,837:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:39:30,837:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:39:30,837:INFO:   <<< rank: 0
2024-05-02 10:39:30,838:INFO:   <<< resume_model: None
2024-05-02 10:39:30,838:INFO:   <<< sampled_use_mil: False
2024-05-02 10:39:30,838:INFO:   <<< seed: 42
2024-05-02 10:39:30,838:INFO:   <<< sim_header: meanP
2024-05-02 10:39:30,838:INFO:   <<< slice_framepos: 2
2024-05-02 10:39:30,838:INFO:   <<< task_type: retrieval
2024-05-02 10:39:30,838:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:39:30,838:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:39:30,838:INFO:   <<< train_frame_order: 0
2024-05-02 10:39:30,838:INFO:   <<< use_mil: False
2024-05-02 10:39:30,838:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:39:30,838:INFO:   <<< video_dim: 1024
2024-05-02 10:39:30,838:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:39:30,838:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:39:30,838:INFO:   <<< world_size: 1
2024-05-02 10:39:30,838:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:39:30,985:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:39:31,470:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:39:31,472:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:39:31,472:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:39:31,473:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:39:31,473:WARNING: Test retrieval by loose type.
2024-05-02 10:39:31,474:WARNING: 	 embed_dim: 512
2024-05-02 10:39:31,474:WARNING: 	 image_resolution: 224
2024-05-02 10:39:31,475:WARNING: 	 vision_layers: 12
2024-05-02 10:39:31,475:WARNING: 	 vision_width: 768
2024-05-02 10:39:31,475:WARNING: 	 vision_patch_size: 32
2024-05-02 10:39:31,475:WARNING: 	 context_length: 77
2024-05-02 10:39:31,475:WARNING: 	 vocab_size: 49408
2024-05-02 10:39:31,475:WARNING: 	 transformer_width: 512
2024-05-02 10:39:31,475:WARNING: 	 transformer_heads: 8
2024-05-02 10:39:31,475:WARNING: 	 transformer_layers: 12
2024-05-02 10:39:31,476:WARNING: 		 linear_patch: 2d
2024-05-02 10:39:31,476:WARNING: 	 cut_top_layer: 0
2024-05-02 10:39:32,368:WARNING: 	 sim_header: meanP
2024-05-02 10:39:36,086:INFO: --------------------
2024-05-02 10:39:36,086:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:41:02,389:INFO: Effective parameters:
2024-05-02 10:41:02,389:INFO:   <<< batch_size: 1
2024-05-02 10:41:02,390:INFO:   <<< batch_size_val: 16
2024-05-02 10:41:02,390:INFO:   <<< cache_dir: 
2024-05-02 10:41:02,390:INFO:   <<< coef_lr: 0.001
2024-05-02 10:41:02,390:INFO:   <<< cross_model: cross-base
2024-05-02 10:41:02,390:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:41:02,390:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:41:02,390:INFO:   <<< datatype: msrvtt
2024-05-02 10:41:02,390:INFO:   <<< do_eval: True
2024-05-02 10:41:02,390:INFO:   <<< do_lower_case: False
2024-05-02 10:41:02,390:INFO:   <<< do_pretrain: False
2024-05-02 10:41:02,390:INFO:   <<< do_train: False
2024-05-02 10:41:02,390:INFO:   <<< epochs: 2
2024-05-02 10:41:02,390:INFO:   <<< eval_frame_order: 0
2024-05-02 10:41:02,390:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:41:02,390:INFO:   <<< feature_framerate: 1
2024-05-02 10:41:02,390:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:41:02,390:INFO:   <<< fp16: False
2024-05-02 10:41:02,391:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:41:02,391:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:41:02,391:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:41:02,391:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:41:02,391:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:41:02,391:INFO:   <<< linear_patch: 2d
2024-05-02 10:41:02,391:INFO:   <<< local_rank: 0
2024-05-02 10:41:02,391:INFO:   <<< loose_type: True
2024-05-02 10:41:02,391:INFO:   <<< lr: 0.0001
2024-05-02 10:41:02,391:INFO:   <<< lr_decay: 0.9
2024-05-02 10:41:02,391:INFO:   <<< margin: 0.1
2024-05-02 10:41:02,391:INFO:   <<< max_frames: 90
2024-05-02 10:41:02,391:INFO:   <<< max_words: 32
2024-05-02 10:41:02,391:INFO:   <<< n_display: 100
2024-05-02 10:41:02,391:INFO:   <<< n_gpu: 1
2024-05-02 10:41:02,391:INFO:   <<< n_pair: 1
2024-05-02 10:41:02,391:INFO:   <<< negative_weighting: 1
2024-05-02 10:41:02,392:INFO:   <<< num_thread_reader: 0
2024-05-02 10:41:02,392:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:41:02,392:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:41:02,392:INFO:   <<< rank: 0
2024-05-02 10:41:02,392:INFO:   <<< resume_model: None
2024-05-02 10:41:02,392:INFO:   <<< sampled_use_mil: False
2024-05-02 10:41:02,392:INFO:   <<< seed: 42
2024-05-02 10:41:02,392:INFO:   <<< sim_header: meanP
2024-05-02 10:41:02,392:INFO:   <<< slice_framepos: 2
2024-05-02 10:41:02,392:INFO:   <<< task_type: retrieval
2024-05-02 10:41:02,392:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:41:02,392:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:41:02,392:INFO:   <<< train_frame_order: 0
2024-05-02 10:41:02,392:INFO:   <<< use_mil: False
2024-05-02 10:41:02,392:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:41:02,392:INFO:   <<< video_dim: 1024
2024-05-02 10:41:02,392:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:41:02,393:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:41:02,393:INFO:   <<< world_size: 1
2024-05-02 10:41:02,393:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:41:02,544:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:41:03,034:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:41:03,035:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:41:03,035:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:41:03,035:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:41:03,035:WARNING: Test retrieval by loose type.
2024-05-02 10:41:03,036:WARNING: 	 embed_dim: 512
2024-05-02 10:41:03,036:WARNING: 	 image_resolution: 224
2024-05-02 10:41:03,036:WARNING: 	 vision_layers: 12
2024-05-02 10:41:03,036:WARNING: 	 vision_width: 768
2024-05-02 10:41:03,036:WARNING: 	 vision_patch_size: 32
2024-05-02 10:41:03,036:WARNING: 	 context_length: 77
2024-05-02 10:41:03,036:WARNING: 	 vocab_size: 49408
2024-05-02 10:41:03,036:WARNING: 	 transformer_width: 512
2024-05-02 10:41:03,036:WARNING: 	 transformer_heads: 8
2024-05-02 10:41:03,036:WARNING: 	 transformer_layers: 12
2024-05-02 10:41:03,037:WARNING: 		 linear_patch: 2d
2024-05-02 10:41:03,037:WARNING: 	 cut_top_layer: 0
2024-05-02 10:41:03,921:WARNING: 	 sim_header: meanP
2024-05-02 10:41:07,622:INFO: --------------------
2024-05-02 10:41:07,623:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:41:25,145:INFO: Effective parameters:
2024-05-02 10:41:25,146:INFO:   <<< batch_size: 1
2024-05-02 10:41:25,146:INFO:   <<< batch_size_val: 16
2024-05-02 10:41:25,146:INFO:   <<< cache_dir: 
2024-05-02 10:41:25,146:INFO:   <<< coef_lr: 0.001
2024-05-02 10:41:25,146:INFO:   <<< cross_model: cross-base
2024-05-02 10:41:25,146:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:41:25,146:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:41:25,146:INFO:   <<< datatype: msrvtt
2024-05-02 10:41:25,146:INFO:   <<< do_eval: True
2024-05-02 10:41:25,146:INFO:   <<< do_lower_case: False
2024-05-02 10:41:25,146:INFO:   <<< do_pretrain: False
2024-05-02 10:41:25,146:INFO:   <<< do_train: False
2024-05-02 10:41:25,146:INFO:   <<< epochs: 2
2024-05-02 10:41:25,146:INFO:   <<< eval_frame_order: 0
2024-05-02 10:41:25,146:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:41:25,146:INFO:   <<< feature_framerate: 1
2024-05-02 10:41:25,146:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:41:25,147:INFO:   <<< fp16: False
2024-05-02 10:41:25,147:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:41:25,147:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:41:25,147:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:41:25,147:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:41:25,147:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:41:25,147:INFO:   <<< linear_patch: 2d
2024-05-02 10:41:25,147:INFO:   <<< local_rank: 0
2024-05-02 10:41:25,147:INFO:   <<< loose_type: True
2024-05-02 10:41:25,147:INFO:   <<< lr: 0.0001
2024-05-02 10:41:25,147:INFO:   <<< lr_decay: 0.9
2024-05-02 10:41:25,147:INFO:   <<< margin: 0.1
2024-05-02 10:41:25,147:INFO:   <<< max_frames: 90
2024-05-02 10:41:25,147:INFO:   <<< max_words: 32
2024-05-02 10:41:25,147:INFO:   <<< n_display: 100
2024-05-02 10:41:25,147:INFO:   <<< n_gpu: 1
2024-05-02 10:41:25,147:INFO:   <<< n_pair: 1
2024-05-02 10:41:25,148:INFO:   <<< negative_weighting: 1
2024-05-02 10:41:25,148:INFO:   <<< num_thread_reader: 0
2024-05-02 10:41:25,148:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:41:25,148:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:41:25,148:INFO:   <<< rank: 0
2024-05-02 10:41:25,148:INFO:   <<< resume_model: None
2024-05-02 10:41:25,148:INFO:   <<< sampled_use_mil: False
2024-05-02 10:41:25,148:INFO:   <<< seed: 42
2024-05-02 10:41:25,148:INFO:   <<< sim_header: meanP
2024-05-02 10:41:25,148:INFO:   <<< slice_framepos: 2
2024-05-02 10:41:25,148:INFO:   <<< task_type: retrieval
2024-05-02 10:41:25,148:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:41:25,148:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:41:25,148:INFO:   <<< train_frame_order: 0
2024-05-02 10:41:25,148:INFO:   <<< use_mil: False
2024-05-02 10:41:25,148:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:41:25,148:INFO:   <<< video_dim: 1024
2024-05-02 10:41:25,148:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:41:25,149:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:41:25,149:INFO:   <<< world_size: 1
2024-05-02 10:41:25,149:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:41:25,307:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:41:25,782:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:41:25,783:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:41:25,783:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:41:25,783:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:41:25,784:WARNING: Test retrieval by loose type.
2024-05-02 10:41:25,784:WARNING: 	 embed_dim: 512
2024-05-02 10:41:25,784:WARNING: 	 image_resolution: 224
2024-05-02 10:41:25,784:WARNING: 	 vision_layers: 12
2024-05-02 10:41:25,784:WARNING: 	 vision_width: 768
2024-05-02 10:41:25,784:WARNING: 	 vision_patch_size: 32
2024-05-02 10:41:25,785:WARNING: 	 context_length: 77
2024-05-02 10:41:25,785:WARNING: 	 vocab_size: 49408
2024-05-02 10:41:25,785:WARNING: 	 transformer_width: 512
2024-05-02 10:41:25,785:WARNING: 	 transformer_heads: 8
2024-05-02 10:41:25,785:WARNING: 	 transformer_layers: 12
2024-05-02 10:41:25,785:WARNING: 		 linear_patch: 2d
2024-05-02 10:41:25,785:WARNING: 	 cut_top_layer: 0
2024-05-02 10:41:26,685:WARNING: 	 sim_header: meanP
2024-05-02 10:41:30,393:INFO: --------------------
2024-05-02 10:41:30,393:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:42:19,716:INFO: Effective parameters:
2024-05-02 10:42:19,716:INFO:   <<< batch_size: 1
2024-05-02 10:42:19,716:INFO:   <<< batch_size_val: 16
2024-05-02 10:42:19,716:INFO:   <<< cache_dir: 
2024-05-02 10:42:19,716:INFO:   <<< coef_lr: 0.001
2024-05-02 10:42:19,716:INFO:   <<< cross_model: cross-base
2024-05-02 10:42:19,716:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:42:19,716:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:42:19,716:INFO:   <<< datatype: msrvtt
2024-05-02 10:42:19,717:INFO:   <<< do_eval: True
2024-05-02 10:42:19,717:INFO:   <<< do_lower_case: False
2024-05-02 10:42:19,717:INFO:   <<< do_pretrain: False
2024-05-02 10:42:19,717:INFO:   <<< do_train: False
2024-05-02 10:42:19,717:INFO:   <<< epochs: 2
2024-05-02 10:42:19,717:INFO:   <<< eval_frame_order: 0
2024-05-02 10:42:19,717:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:42:19,717:INFO:   <<< feature_framerate: 1
2024-05-02 10:42:19,717:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:42:19,717:INFO:   <<< fp16: False
2024-05-02 10:42:19,717:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:42:19,717:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:42:19,718:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:42:19,718:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:42:19,718:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:42:19,718:INFO:   <<< linear_patch: 2d
2024-05-02 10:42:19,718:INFO:   <<< local_rank: 0
2024-05-02 10:42:19,718:INFO:   <<< loose_type: True
2024-05-02 10:42:19,718:INFO:   <<< lr: 0.0001
2024-05-02 10:42:19,718:INFO:   <<< lr_decay: 0.9
2024-05-02 10:42:19,718:INFO:   <<< margin: 0.1
2024-05-02 10:42:19,718:INFO:   <<< max_frames: 90
2024-05-02 10:42:19,718:INFO:   <<< max_words: 32
2024-05-02 10:42:19,718:INFO:   <<< n_display: 100
2024-05-02 10:42:19,718:INFO:   <<< n_gpu: 1
2024-05-02 10:42:19,718:INFO:   <<< n_pair: 1
2024-05-02 10:42:19,719:INFO:   <<< negative_weighting: 1
2024-05-02 10:42:19,719:INFO:   <<< num_thread_reader: 0
2024-05-02 10:42:19,719:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:42:19,719:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:42:19,719:INFO:   <<< rank: 0
2024-05-02 10:42:19,719:INFO:   <<< resume_model: None
2024-05-02 10:42:19,719:INFO:   <<< sampled_use_mil: False
2024-05-02 10:42:19,719:INFO:   <<< seed: 42
2024-05-02 10:42:19,719:INFO:   <<< sim_header: meanP
2024-05-02 10:42:19,719:INFO:   <<< slice_framepos: 2
2024-05-02 10:42:19,719:INFO:   <<< task_type: retrieval
2024-05-02 10:42:19,719:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:42:19,719:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:42:19,720:INFO:   <<< train_frame_order: 0
2024-05-02 10:42:19,720:INFO:   <<< use_mil: False
2024-05-02 10:42:19,720:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:42:19,720:INFO:   <<< video_dim: 1024
2024-05-02 10:42:19,720:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:42:19,720:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:42:19,720:INFO:   <<< world_size: 1
2024-05-02 10:42:19,720:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:42:19,874:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:42:20,385:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:42:20,387:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:42:20,387:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:42:20,388:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:42:20,388:WARNING: Test retrieval by loose type.
2024-05-02 10:42:20,389:WARNING: 	 embed_dim: 512
2024-05-02 10:42:20,389:WARNING: 	 image_resolution: 224
2024-05-02 10:42:20,389:WARNING: 	 vision_layers: 12
2024-05-02 10:42:20,390:WARNING: 	 vision_width: 768
2024-05-02 10:42:20,390:WARNING: 	 vision_patch_size: 32
2024-05-02 10:42:20,390:WARNING: 	 context_length: 77
2024-05-02 10:42:20,390:WARNING: 	 vocab_size: 49408
2024-05-02 10:42:20,390:WARNING: 	 transformer_width: 512
2024-05-02 10:42:20,390:WARNING: 	 transformer_heads: 8
2024-05-02 10:42:20,390:WARNING: 	 transformer_layers: 12
2024-05-02 10:42:20,390:WARNING: 		 linear_patch: 2d
2024-05-02 10:42:20,390:WARNING: 	 cut_top_layer: 0
2024-05-02 10:42:21,297:WARNING: 	 sim_header: meanP
2024-05-02 10:42:25,019:INFO: --------------------
2024-05-02 10:42:25,019:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:42:59,153:INFO: Effective parameters:
2024-05-02 10:42:59,153:INFO:   <<< batch_size: 1
2024-05-02 10:42:59,153:INFO:   <<< batch_size_val: 16
2024-05-02 10:42:59,153:INFO:   <<< cache_dir: 
2024-05-02 10:42:59,153:INFO:   <<< coef_lr: 0.001
2024-05-02 10:42:59,153:INFO:   <<< cross_model: cross-base
2024-05-02 10:42:59,153:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:42:59,153:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:42:59,153:INFO:   <<< datatype: msrvtt
2024-05-02 10:42:59,153:INFO:   <<< do_eval: True
2024-05-02 10:42:59,153:INFO:   <<< do_lower_case: False
2024-05-02 10:42:59,153:INFO:   <<< do_pretrain: False
2024-05-02 10:42:59,154:INFO:   <<< do_train: False
2024-05-02 10:42:59,154:INFO:   <<< epochs: 2
2024-05-02 10:42:59,154:INFO:   <<< eval_frame_order: 0
2024-05-02 10:42:59,154:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:42:59,154:INFO:   <<< feature_framerate: 1
2024-05-02 10:42:59,154:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:42:59,154:INFO:   <<< fp16: False
2024-05-02 10:42:59,154:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:42:59,154:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:42:59,154:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:42:59,154:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:42:59,154:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:42:59,154:INFO:   <<< linear_patch: 2d
2024-05-02 10:42:59,154:INFO:   <<< local_rank: 0
2024-05-02 10:42:59,154:INFO:   <<< loose_type: True
2024-05-02 10:42:59,154:INFO:   <<< lr: 0.0001
2024-05-02 10:42:59,154:INFO:   <<< lr_decay: 0.9
2024-05-02 10:42:59,154:INFO:   <<< margin: 0.1
2024-05-02 10:42:59,155:INFO:   <<< max_frames: 90
2024-05-02 10:42:59,155:INFO:   <<< max_words: 32
2024-05-02 10:42:59,155:INFO:   <<< n_display: 100
2024-05-02 10:42:59,155:INFO:   <<< n_gpu: 1
2024-05-02 10:42:59,155:INFO:   <<< n_pair: 1
2024-05-02 10:42:59,155:INFO:   <<< negative_weighting: 1
2024-05-02 10:42:59,155:INFO:   <<< num_thread_reader: 0
2024-05-02 10:42:59,155:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:42:59,155:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:42:59,155:INFO:   <<< rank: 0
2024-05-02 10:42:59,155:INFO:   <<< resume_model: None
2024-05-02 10:42:59,155:INFO:   <<< sampled_use_mil: False
2024-05-02 10:42:59,155:INFO:   <<< seed: 42
2024-05-02 10:42:59,155:INFO:   <<< sim_header: meanP
2024-05-02 10:42:59,155:INFO:   <<< slice_framepos: 2
2024-05-02 10:42:59,155:INFO:   <<< task_type: retrieval
2024-05-02 10:42:59,155:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:42:59,156:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:42:59,156:INFO:   <<< train_frame_order: 0
2024-05-02 10:42:59,156:INFO:   <<< use_mil: False
2024-05-02 10:42:59,156:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:42:59,156:INFO:   <<< video_dim: 1024
2024-05-02 10:42:59,156:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:42:59,156:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:42:59,156:INFO:   <<< world_size: 1
2024-05-02 10:42:59,156:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:42:59,322:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:42:59,825:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:42:59,827:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:42:59,828:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:42:59,828:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:42:59,828:WARNING: Test retrieval by loose type.
2024-05-02 10:42:59,829:WARNING: 	 embed_dim: 512
2024-05-02 10:42:59,830:WARNING: 	 image_resolution: 224
2024-05-02 10:42:59,830:WARNING: 	 vision_layers: 12
2024-05-02 10:42:59,830:WARNING: 	 vision_width: 768
2024-05-02 10:42:59,830:WARNING: 	 vision_patch_size: 32
2024-05-02 10:42:59,830:WARNING: 	 context_length: 77
2024-05-02 10:42:59,830:WARNING: 	 vocab_size: 49408
2024-05-02 10:42:59,830:WARNING: 	 transformer_width: 512
2024-05-02 10:42:59,830:WARNING: 	 transformer_heads: 8
2024-05-02 10:42:59,830:WARNING: 	 transformer_layers: 12
2024-05-02 10:42:59,831:WARNING: 		 linear_patch: 2d
2024-05-02 10:42:59,831:WARNING: 	 cut_top_layer: 0
2024-05-02 10:43:00,741:WARNING: 	 sim_header: meanP
2024-05-02 10:43:04,462:INFO: --------------------
2024-05-02 10:43:04,462:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:43:43,605:INFO: Effective parameters:
2024-05-02 10:43:43,605:INFO:   <<< batch_size: 1
2024-05-02 10:43:43,605:INFO:   <<< batch_size_val: 16
2024-05-02 10:43:43,605:INFO:   <<< cache_dir: 
2024-05-02 10:43:43,605:INFO:   <<< coef_lr: 0.001
2024-05-02 10:43:43,605:INFO:   <<< cross_model: cross-base
2024-05-02 10:43:43,605:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:43:43,605:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:43:43,605:INFO:   <<< datatype: msrvtt
2024-05-02 10:43:43,605:INFO:   <<< do_eval: True
2024-05-02 10:43:43,605:INFO:   <<< do_lower_case: False
2024-05-02 10:43:43,605:INFO:   <<< do_pretrain: False
2024-05-02 10:43:43,605:INFO:   <<< do_train: False
2024-05-02 10:43:43,606:INFO:   <<< epochs: 2
2024-05-02 10:43:43,606:INFO:   <<< eval_frame_order: 0
2024-05-02 10:43:43,606:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:43:43,606:INFO:   <<< feature_framerate: 1
2024-05-02 10:43:43,606:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:43:43,606:INFO:   <<< fp16: False
2024-05-02 10:43:43,606:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:43:43,606:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:43:43,606:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:43:43,606:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:43:43,606:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:43:43,606:INFO:   <<< linear_patch: 2d
2024-05-02 10:43:43,606:INFO:   <<< local_rank: 0
2024-05-02 10:43:43,606:INFO:   <<< loose_type: True
2024-05-02 10:43:43,606:INFO:   <<< lr: 0.0001
2024-05-02 10:43:43,606:INFO:   <<< lr_decay: 0.9
2024-05-02 10:43:43,606:INFO:   <<< margin: 0.1
2024-05-02 10:43:43,607:INFO:   <<< max_frames: 90
2024-05-02 10:43:43,607:INFO:   <<< max_words: 32
2024-05-02 10:43:43,607:INFO:   <<< n_display: 100
2024-05-02 10:43:43,607:INFO:   <<< n_gpu: 1
2024-05-02 10:43:43,607:INFO:   <<< n_pair: 1
2024-05-02 10:43:43,607:INFO:   <<< negative_weighting: 1
2024-05-02 10:43:43,607:INFO:   <<< num_thread_reader: 0
2024-05-02 10:43:43,607:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:43:43,607:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:43:43,607:INFO:   <<< rank: 0
2024-05-02 10:43:43,607:INFO:   <<< resume_model: None
2024-05-02 10:43:43,607:INFO:   <<< sampled_use_mil: False
2024-05-02 10:43:43,607:INFO:   <<< seed: 42
2024-05-02 10:43:43,607:INFO:   <<< sim_header: meanP
2024-05-02 10:43:43,607:INFO:   <<< slice_framepos: 2
2024-05-02 10:43:43,607:INFO:   <<< task_type: retrieval
2024-05-02 10:43:43,607:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:43:43,608:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:43:43,608:INFO:   <<< train_frame_order: 0
2024-05-02 10:43:43,608:INFO:   <<< use_mil: False
2024-05-02 10:43:43,608:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:43:43,608:INFO:   <<< video_dim: 1024
2024-05-02 10:43:43,608:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:43:43,608:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:43:43,608:INFO:   <<< world_size: 1
2024-05-02 10:43:43,608:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:43:43,760:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:43:44,241:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:43:44,243:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:43:44,243:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:43:44,243:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:43:44,243:WARNING: Test retrieval by loose type.
2024-05-02 10:43:44,244:WARNING: 	 embed_dim: 512
2024-05-02 10:43:44,244:WARNING: 	 image_resolution: 224
2024-05-02 10:43:44,244:WARNING: 	 vision_layers: 12
2024-05-02 10:43:44,244:WARNING: 	 vision_width: 768
2024-05-02 10:43:44,244:WARNING: 	 vision_patch_size: 32
2024-05-02 10:43:44,244:WARNING: 	 context_length: 77
2024-05-02 10:43:44,244:WARNING: 	 vocab_size: 49408
2024-05-02 10:43:44,244:WARNING: 	 transformer_width: 512
2024-05-02 10:43:44,245:WARNING: 	 transformer_heads: 8
2024-05-02 10:43:44,245:WARNING: 	 transformer_layers: 12
2024-05-02 10:43:44,245:WARNING: 		 linear_patch: 2d
2024-05-02 10:43:44,245:WARNING: 	 cut_top_layer: 0
2024-05-02 10:43:45,123:WARNING: 	 sim_header: meanP
2024-05-02 10:43:48,829:INFO: --------------------
2024-05-02 10:43:48,829:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:45:21,449:INFO: Effective parameters:
2024-05-02 10:45:21,449:INFO:   <<< batch_size: 1
2024-05-02 10:45:21,449:INFO:   <<< batch_size_val: 16
2024-05-02 10:45:21,449:INFO:   <<< cache_dir: 
2024-05-02 10:45:21,449:INFO:   <<< coef_lr: 0.001
2024-05-02 10:45:21,449:INFO:   <<< cross_model: cross-base
2024-05-02 10:45:21,449:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:45:21,449:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:45:21,449:INFO:   <<< datatype: msrvtt
2024-05-02 10:45:21,449:INFO:   <<< do_eval: True
2024-05-02 10:45:21,449:INFO:   <<< do_lower_case: False
2024-05-02 10:45:21,449:INFO:   <<< do_pretrain: False
2024-05-02 10:45:21,450:INFO:   <<< do_train: False
2024-05-02 10:45:21,450:INFO:   <<< epochs: 2
2024-05-02 10:45:21,450:INFO:   <<< eval_frame_order: 0
2024-05-02 10:45:21,450:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:45:21,450:INFO:   <<< feature_framerate: 1
2024-05-02 10:45:21,450:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:45:21,450:INFO:   <<< fp16: False
2024-05-02 10:45:21,450:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:45:21,450:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:45:21,450:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:45:21,450:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:45:21,450:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:45:21,450:INFO:   <<< linear_patch: 2d
2024-05-02 10:45:21,450:INFO:   <<< local_rank: 0
2024-05-02 10:45:21,450:INFO:   <<< loose_type: True
2024-05-02 10:45:21,450:INFO:   <<< lr: 0.0001
2024-05-02 10:45:21,450:INFO:   <<< lr_decay: 0.9
2024-05-02 10:45:21,451:INFO:   <<< margin: 0.1
2024-05-02 10:45:21,451:INFO:   <<< max_frames: 90
2024-05-02 10:45:21,451:INFO:   <<< max_words: 32
2024-05-02 10:45:21,451:INFO:   <<< n_display: 100
2024-05-02 10:45:21,451:INFO:   <<< n_gpu: 1
2024-05-02 10:45:21,451:INFO:   <<< n_pair: 1
2024-05-02 10:45:21,451:INFO:   <<< negative_weighting: 1
2024-05-02 10:45:21,451:INFO:   <<< num_thread_reader: 0
2024-05-02 10:45:21,451:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:45:21,451:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:45:21,451:INFO:   <<< rank: 0
2024-05-02 10:45:21,451:INFO:   <<< resume_model: None
2024-05-02 10:45:21,451:INFO:   <<< sampled_use_mil: False
2024-05-02 10:45:21,451:INFO:   <<< seed: 42
2024-05-02 10:45:21,451:INFO:   <<< sim_header: meanP
2024-05-02 10:45:21,451:INFO:   <<< slice_framepos: 2
2024-05-02 10:45:21,451:INFO:   <<< task_type: retrieval
2024-05-02 10:45:21,452:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:45:21,452:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:45:21,452:INFO:   <<< train_frame_order: 0
2024-05-02 10:45:21,452:INFO:   <<< use_mil: False
2024-05-02 10:45:21,452:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:45:21,452:INFO:   <<< video_dim: 1024
2024-05-02 10:45:21,452:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:45:21,452:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:45:21,452:INFO:   <<< world_size: 1
2024-05-02 10:45:21,452:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:45:21,603:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:45:22,090:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:45:22,091:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:45:22,091:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:45:22,091:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:45:22,091:WARNING: Test retrieval by loose type.
2024-05-02 10:45:22,092:WARNING: 	 embed_dim: 512
2024-05-02 10:45:22,092:WARNING: 	 image_resolution: 224
2024-05-02 10:45:22,092:WARNING: 	 vision_layers: 12
2024-05-02 10:45:22,092:WARNING: 	 vision_width: 768
2024-05-02 10:45:22,092:WARNING: 	 vision_patch_size: 32
2024-05-02 10:45:22,092:WARNING: 	 context_length: 77
2024-05-02 10:45:22,093:WARNING: 	 vocab_size: 49408
2024-05-02 10:45:22,093:WARNING: 	 transformer_width: 512
2024-05-02 10:45:22,093:WARNING: 	 transformer_heads: 8
2024-05-02 10:45:22,093:WARNING: 	 transformer_layers: 12
2024-05-02 10:45:22,093:WARNING: 		 linear_patch: 2d
2024-05-02 10:45:22,093:WARNING: 	 cut_top_layer: 0
2024-05-02 10:45:22,973:WARNING: 	 sim_header: meanP
2024-05-02 10:45:26,658:INFO: --------------------
2024-05-02 10:45:26,659:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:47:48,807:INFO: Effective parameters:
2024-05-02 10:47:48,807:INFO:   <<< batch_size: 16
2024-05-02 10:47:48,807:INFO:   <<< batch_size_val: 16
2024-05-02 10:47:48,807:INFO:   <<< cache_dir: 
2024-05-02 10:47:48,807:INFO:   <<< coef_lr: 0.001
2024-05-02 10:47:48,807:INFO:   <<< cross_model: cross-base
2024-05-02 10:47:48,808:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:47:48,808:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:47:48,808:INFO:   <<< datatype: msrvtt
2024-05-02 10:47:48,808:INFO:   <<< do_eval: True
2024-05-02 10:47:48,808:INFO:   <<< do_lower_case: False
2024-05-02 10:47:48,808:INFO:   <<< do_pretrain: False
2024-05-02 10:47:48,808:INFO:   <<< do_train: False
2024-05-02 10:47:48,808:INFO:   <<< epochs: 2
2024-05-02 10:47:48,808:INFO:   <<< eval_frame_order: 0
2024-05-02 10:47:48,808:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:47:48,808:INFO:   <<< feature_framerate: 1
2024-05-02 10:47:48,808:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:47:48,808:INFO:   <<< fp16: False
2024-05-02 10:47:48,808:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:47:48,808:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:47:48,808:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:47:48,809:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:47:48,809:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:47:48,809:INFO:   <<< linear_patch: 2d
2024-05-02 10:47:48,809:INFO:   <<< local_rank: 0
2024-05-02 10:47:48,809:INFO:   <<< loose_type: True
2024-05-02 10:47:48,809:INFO:   <<< lr: 0.0001
2024-05-02 10:47:48,809:INFO:   <<< lr_decay: 0.9
2024-05-02 10:47:48,809:INFO:   <<< margin: 0.1
2024-05-02 10:47:48,809:INFO:   <<< max_frames: 12
2024-05-02 10:47:48,809:INFO:   <<< max_words: 32
2024-05-02 10:47:48,809:INFO:   <<< n_display: 100
2024-05-02 10:47:48,809:INFO:   <<< n_gpu: 1
2024-05-02 10:47:48,809:INFO:   <<< n_pair: 1
2024-05-02 10:47:48,809:INFO:   <<< negative_weighting: 1
2024-05-02 10:47:48,809:INFO:   <<< num_thread_reader: 0
2024-05-02 10:47:48,809:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:47:48,809:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:47:48,810:INFO:   <<< rank: 0
2024-05-02 10:47:48,810:INFO:   <<< resume_model: None
2024-05-02 10:47:48,810:INFO:   <<< sampled_use_mil: False
2024-05-02 10:47:48,810:INFO:   <<< seed: 42
2024-05-02 10:47:48,810:INFO:   <<< sim_header: meanP
2024-05-02 10:47:48,810:INFO:   <<< slice_framepos: 2
2024-05-02 10:47:48,810:INFO:   <<< task_type: retrieval
2024-05-02 10:47:48,810:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:47:48,810:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:47:48,810:INFO:   <<< train_frame_order: 0
2024-05-02 10:47:48,810:INFO:   <<< use_mil: False
2024-05-02 10:47:48,810:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:47:48,810:INFO:   <<< video_dim: 1024
2024-05-02 10:47:48,810:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:47:48,810:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:47:48,810:INFO:   <<< world_size: 1
2024-05-02 10:47:48,810:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:47:49,053:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:47:49,528:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:47:49,529:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:47:49,529:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:47:49,529:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:47:49,529:WARNING: Test retrieval by loose type.
2024-05-02 10:47:49,530:WARNING: 	 embed_dim: 512
2024-05-02 10:47:49,530:WARNING: 	 image_resolution: 224
2024-05-02 10:47:49,530:WARNING: 	 vision_layers: 12
2024-05-02 10:47:49,530:WARNING: 	 vision_width: 768
2024-05-02 10:47:49,530:WARNING: 	 vision_patch_size: 32
2024-05-02 10:47:49,530:WARNING: 	 context_length: 77
2024-05-02 10:47:49,530:WARNING: 	 vocab_size: 49408
2024-05-02 10:47:49,530:WARNING: 	 transformer_width: 512
2024-05-02 10:47:49,531:WARNING: 	 transformer_heads: 8
2024-05-02 10:47:49,531:WARNING: 	 transformer_layers: 12
2024-05-02 10:47:49,531:WARNING: 		 linear_patch: 2d
2024-05-02 10:47:49,531:WARNING: 	 cut_top_layer: 0
2024-05-02 10:47:50,393:WARNING: 	 sim_header: meanP
2024-05-02 10:47:54,095:INFO: --------------------
2024-05-02 10:47:54,095:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:47:56,193:INFO: ***** Running test *****
2024-05-02 10:47:56,193:INFO:   Num examples = 22
2024-05-02 10:47:56,193:INFO:   Batch size = 16
2024-05-02 10:47:56,193:INFO:   Num steps = 2
2024-05-02 10:47:56,193:INFO: ***** Running val *****
2024-05-02 10:47:56,194:INFO:   Num examples = 22
2024-05-02 10:57:41,722:INFO: Effective parameters:
2024-05-02 10:57:41,723:INFO:   <<< batch_size: 1
2024-05-02 10:57:41,723:INFO:   <<< batch_size_val: 16
2024-05-02 10:57:41,723:INFO:   <<< cache_dir: 
2024-05-02 10:57:41,723:INFO:   <<< coef_lr: 0.001
2024-05-02 10:57:41,723:INFO:   <<< cross_model: cross-base
2024-05-02 10:57:41,723:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:57:41,723:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:57:41,723:INFO:   <<< datatype: msrvtt
2024-05-02 10:57:41,723:INFO:   <<< do_eval: True
2024-05-02 10:57:41,723:INFO:   <<< do_lower_case: False
2024-05-02 10:57:41,723:INFO:   <<< do_pretrain: False
2024-05-02 10:57:41,723:INFO:   <<< do_train: False
2024-05-02 10:57:41,723:INFO:   <<< epochs: 2
2024-05-02 10:57:41,724:INFO:   <<< eval_frame_order: 0
2024-05-02 10:57:41,724:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:57:41,724:INFO:   <<< feature_framerate: 1
2024-05-02 10:57:41,724:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:57:41,724:INFO:   <<< fp16: False
2024-05-02 10:57:41,724:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:57:41,724:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:57:41,724:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:57:41,724:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:57:41,724:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:57:41,724:INFO:   <<< linear_patch: 2d
2024-05-02 10:57:41,725:INFO:   <<< local_rank: 0
2024-05-02 10:57:41,725:INFO:   <<< loose_type: True
2024-05-02 10:57:41,725:INFO:   <<< lr: 0.0001
2024-05-02 10:57:41,725:INFO:   <<< lr_decay: 0.9
2024-05-02 10:57:41,725:INFO:   <<< margin: 0.1
2024-05-02 10:57:41,725:INFO:   <<< max_frames: 90
2024-05-02 10:57:41,725:INFO:   <<< max_words: 32
2024-05-02 10:57:41,725:INFO:   <<< n_display: 100
2024-05-02 10:57:41,725:INFO:   <<< n_gpu: 1
2024-05-02 10:57:41,725:INFO:   <<< n_pair: 1
2024-05-02 10:57:41,725:INFO:   <<< negative_weighting: 1
2024-05-02 10:57:41,725:INFO:   <<< num_thread_reader: 0
2024-05-02 10:57:41,725:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:57:41,725:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:57:41,725:INFO:   <<< rank: 0
2024-05-02 10:57:41,725:INFO:   <<< resume_model: None
2024-05-02 10:57:41,726:INFO:   <<< sampled_use_mil: False
2024-05-02 10:57:41,726:INFO:   <<< seed: 42
2024-05-02 10:57:41,726:INFO:   <<< sim_header: meanP
2024-05-02 10:57:41,726:INFO:   <<< slice_framepos: 2
2024-05-02 10:57:41,726:INFO:   <<< task_type: retrieval
2024-05-02 10:57:41,726:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:57:41,726:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:57:41,726:INFO:   <<< train_frame_order: 0
2024-05-02 10:57:41,726:INFO:   <<< use_mil: False
2024-05-02 10:57:41,726:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:57:41,726:INFO:   <<< video_dim: 1024
2024-05-02 10:57:41,726:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:57:41,726:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:57:41,726:INFO:   <<< world_size: 1
2024-05-02 10:57:41,727:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:57:41,876:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:57:42,352:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:57:42,353:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:57:42,354:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:57:42,354:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:57:42,354:WARNING: Test retrieval by loose type.
2024-05-02 10:57:42,355:WARNING: 	 embed_dim: 512
2024-05-02 10:57:42,355:WARNING: 	 image_resolution: 224
2024-05-02 10:57:42,355:WARNING: 	 vision_layers: 12
2024-05-02 10:57:42,355:WARNING: 	 vision_width: 768
2024-05-02 10:57:42,355:WARNING: 	 vision_patch_size: 32
2024-05-02 10:57:42,355:WARNING: 	 context_length: 77
2024-05-02 10:57:42,355:WARNING: 	 vocab_size: 49408
2024-05-02 10:57:42,355:WARNING: 	 transformer_width: 512
2024-05-02 10:57:42,355:WARNING: 	 transformer_heads: 8
2024-05-02 10:57:42,355:WARNING: 	 transformer_layers: 12
2024-05-02 10:57:42,355:WARNING: 		 linear_patch: 2d
2024-05-02 10:57:42,355:WARNING: 	 cut_top_layer: 0
2024-05-02 10:57:43,248:WARNING: 	 sim_header: meanP
2024-05-02 10:57:46,944:INFO: --------------------
2024-05-02 10:57:46,944:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:58:55,374:INFO: Effective parameters:
2024-05-02 10:58:55,374:INFO:   <<< batch_size: 16
2024-05-02 10:58:55,374:INFO:   <<< batch_size_val: 16
2024-05-02 10:58:55,374:INFO:   <<< cache_dir: 
2024-05-02 10:58:55,374:INFO:   <<< coef_lr: 0.001
2024-05-02 10:58:55,374:INFO:   <<< cross_model: cross-base
2024-05-02 10:58:55,374:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 10:58:55,374:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 10:58:55,375:INFO:   <<< datatype: msrvtt
2024-05-02 10:58:55,375:INFO:   <<< do_eval: True
2024-05-02 10:58:55,375:INFO:   <<< do_lower_case: False
2024-05-02 10:58:55,375:INFO:   <<< do_pretrain: False
2024-05-02 10:58:55,375:INFO:   <<< do_train: False
2024-05-02 10:58:55,375:INFO:   <<< epochs: 2
2024-05-02 10:58:55,375:INFO:   <<< eval_frame_order: 0
2024-05-02 10:58:55,375:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 10:58:55,375:INFO:   <<< feature_framerate: 1
2024-05-02 10:58:55,375:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 10:58:55,375:INFO:   <<< fp16: False
2024-05-02 10:58:55,375:INFO:   <<< fp16_opt_level: O1
2024-05-02 10:58:55,375:INFO:   <<< freeze_layer_num: 0
2024-05-02 10:58:55,375:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 10:58:55,375:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 10:58:55,375:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:58:55,375:INFO:   <<< linear_patch: 2d
2024-05-02 10:58:55,376:INFO:   <<< local_rank: 0
2024-05-02 10:58:55,376:INFO:   <<< loose_type: True
2024-05-02 10:58:55,376:INFO:   <<< lr: 0.0001
2024-05-02 10:58:55,376:INFO:   <<< lr_decay: 0.9
2024-05-02 10:58:55,376:INFO:   <<< margin: 0.1
2024-05-02 10:58:55,376:INFO:   <<< max_frames: 12
2024-05-02 10:58:55,376:INFO:   <<< max_words: 32
2024-05-02 10:58:55,376:INFO:   <<< n_display: 100
2024-05-02 10:58:55,376:INFO:   <<< n_gpu: 1
2024-05-02 10:58:55,376:INFO:   <<< n_pair: 1
2024-05-02 10:58:55,376:INFO:   <<< negative_weighting: 1
2024-05-02 10:58:55,376:INFO:   <<< num_thread_reader: 0
2024-05-02 10:58:55,376:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 10:58:55,376:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 10:58:55,376:INFO:   <<< rank: 0
2024-05-02 10:58:55,376:INFO:   <<< resume_model: None
2024-05-02 10:58:55,376:INFO:   <<< sampled_use_mil: False
2024-05-02 10:58:55,377:INFO:   <<< seed: 42
2024-05-02 10:58:55,377:INFO:   <<< sim_header: meanP
2024-05-02 10:58:55,377:INFO:   <<< slice_framepos: 2
2024-05-02 10:58:55,377:INFO:   <<< task_type: retrieval
2024-05-02 10:58:55,377:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 10:58:55,377:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 10:58:55,377:INFO:   <<< train_frame_order: 0
2024-05-02 10:58:55,377:INFO:   <<< use_mil: False
2024-05-02 10:58:55,377:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 10:58:55,377:INFO:   <<< video_dim: 1024
2024-05-02 10:58:55,377:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 10:58:55,378:INFO:   <<< warmup_proportion: 0.1
2024-05-02 10:58:55,378:INFO:   <<< world_size: 1
2024-05-02 10:58:55,378:INFO: device: cuda:0 n_gpu: 1
2024-05-02 10:58:55,631:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 10:58:56,138:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 10:58:56,140:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 10:58:56,140:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 10:58:56,140:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 10:58:56,140:WARNING: Test retrieval by loose type.
2024-05-02 10:58:56,142:WARNING: 	 embed_dim: 512
2024-05-02 10:58:56,142:WARNING: 	 image_resolution: 224
2024-05-02 10:58:56,142:WARNING: 	 vision_layers: 12
2024-05-02 10:58:56,142:WARNING: 	 vision_width: 768
2024-05-02 10:58:56,142:WARNING: 	 vision_patch_size: 32
2024-05-02 10:58:56,142:WARNING: 	 context_length: 77
2024-05-02 10:58:56,143:WARNING: 	 vocab_size: 49408
2024-05-02 10:58:56,143:WARNING: 	 transformer_width: 512
2024-05-02 10:58:56,143:WARNING: 	 transformer_heads: 8
2024-05-02 10:58:56,143:WARNING: 	 transformer_layers: 12
2024-05-02 10:58:56,143:WARNING: 		 linear_patch: 2d
2024-05-02 10:58:56,143:WARNING: 	 cut_top_layer: 0
2024-05-02 10:58:57,027:WARNING: 	 sim_header: meanP
2024-05-02 10:59:00,745:INFO: --------------------
2024-05-02 10:59:00,746:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 10:59:02,835:INFO: ***** Running test *****
2024-05-02 10:59:02,835:INFO:   Num examples = 22
2024-05-02 10:59:02,838:INFO:   Batch size = 16
2024-05-02 10:59:02,838:INFO:   Num steps = 2
2024-05-02 10:59:02,838:INFO: ***** Running val *****
2024-05-02 10:59:02,838:INFO:   Num examples = 22
2024-05-02 11:00:52,305:INFO: Effective parameters:
2024-05-02 11:00:52,306:INFO:   <<< batch_size: 16
2024-05-02 11:00:52,306:INFO:   <<< batch_size_val: 16
2024-05-02 11:00:52,306:INFO:   <<< cache_dir: 
2024-05-02 11:00:52,306:INFO:   <<< coef_lr: 0.001
2024-05-02 11:00:52,306:INFO:   <<< cross_model: cross-base
2024-05-02 11:00:52,306:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 11:00:52,306:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 11:00:52,306:INFO:   <<< datatype: msrvtt
2024-05-02 11:00:52,306:INFO:   <<< do_eval: True
2024-05-02 11:00:52,306:INFO:   <<< do_lower_case: False
2024-05-02 11:00:52,306:INFO:   <<< do_pretrain: False
2024-05-02 11:00:52,306:INFO:   <<< do_train: False
2024-05-02 11:00:52,306:INFO:   <<< epochs: 2
2024-05-02 11:00:52,306:INFO:   <<< eval_frame_order: 0
2024-05-02 11:00:52,307:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 11:00:52,307:INFO:   <<< feature_framerate: 1
2024-05-02 11:00:52,307:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 11:00:52,307:INFO:   <<< fp16: False
2024-05-02 11:00:52,307:INFO:   <<< fp16_opt_level: O1
2024-05-02 11:00:52,307:INFO:   <<< freeze_layer_num: 0
2024-05-02 11:00:52,307:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 11:00:52,307:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 11:00:52,307:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 11:00:52,307:INFO:   <<< linear_patch: 2d
2024-05-02 11:00:52,307:INFO:   <<< local_rank: 0
2024-05-02 11:00:52,307:INFO:   <<< loose_type: True
2024-05-02 11:00:52,307:INFO:   <<< lr: 0.0001
2024-05-02 11:00:52,307:INFO:   <<< lr_decay: 0.9
2024-05-02 11:00:52,307:INFO:   <<< margin: 0.1
2024-05-02 11:00:52,307:INFO:   <<< max_frames: 12
2024-05-02 11:00:52,307:INFO:   <<< max_words: 32
2024-05-02 11:00:52,308:INFO:   <<< n_display: 100
2024-05-02 11:00:52,308:INFO:   <<< n_gpu: 1
2024-05-02 11:00:52,308:INFO:   <<< n_pair: 1
2024-05-02 11:00:52,308:INFO:   <<< negative_weighting: 1
2024-05-02 11:00:52,308:INFO:   <<< num_thread_reader: 0
2024-05-02 11:00:52,308:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 11:00:52,308:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 11:00:52,308:INFO:   <<< rank: 0
2024-05-02 11:00:52,308:INFO:   <<< resume_model: None
2024-05-02 11:00:52,308:INFO:   <<< sampled_use_mil: False
2024-05-02 11:00:52,308:INFO:   <<< seed: 42
2024-05-02 11:00:52,308:INFO:   <<< sim_header: meanP
2024-05-02 11:00:52,308:INFO:   <<< slice_framepos: 2
2024-05-02 11:00:52,308:INFO:   <<< task_type: retrieval
2024-05-02 11:00:52,308:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 11:00:52,308:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 11:00:52,308:INFO:   <<< train_frame_order: 0
2024-05-02 11:00:52,309:INFO:   <<< use_mil: False
2024-05-02 11:00:52,309:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 11:00:52,309:INFO:   <<< video_dim: 1024
2024-05-02 11:00:52,309:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 11:00:52,309:INFO:   <<< warmup_proportion: 0.1
2024-05-02 11:00:52,309:INFO:   <<< world_size: 1
2024-05-02 11:00:52,309:INFO: device: cuda:0 n_gpu: 1
2024-05-02 11:00:52,553:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 11:00:53,022:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 11:00:53,023:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 11:00:53,023:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 11:00:53,023:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 11:00:53,023:WARNING: Test retrieval by loose type.
2024-05-02 11:00:53,024:WARNING: 	 embed_dim: 512
2024-05-02 11:00:53,024:WARNING: 	 image_resolution: 224
2024-05-02 11:00:53,024:WARNING: 	 vision_layers: 12
2024-05-02 11:00:53,024:WARNING: 	 vision_width: 768
2024-05-02 11:00:53,024:WARNING: 	 vision_patch_size: 32
2024-05-02 11:00:53,024:WARNING: 	 context_length: 77
2024-05-02 11:00:53,024:WARNING: 	 vocab_size: 49408
2024-05-02 11:00:53,024:WARNING: 	 transformer_width: 512
2024-05-02 11:00:53,025:WARNING: 	 transformer_heads: 8
2024-05-02 11:00:53,025:WARNING: 	 transformer_layers: 12
2024-05-02 11:00:53,025:WARNING: 		 linear_patch: 2d
2024-05-02 11:00:53,025:WARNING: 	 cut_top_layer: 0
2024-05-02 11:00:53,891:WARNING: 	 sim_header: meanP
2024-05-02 11:00:57,591:INFO: --------------------
2024-05-02 11:00:57,591:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 11:00:59,677:INFO: ***** Running test *****
2024-05-02 11:00:59,677:INFO:   Num examples = 22
2024-05-02 11:00:59,678:INFO:   Batch size = 16
2024-05-02 11:00:59,678:INFO:   Num steps = 2
2024-05-02 11:00:59,678:INFO: ***** Running val *****
2024-05-02 11:00:59,678:INFO:   Num examples = 22
2024-05-02 11:49:17,400:INFO: Effective parameters:
2024-05-02 11:49:17,401:INFO:   <<< batch_size: 1
2024-05-02 11:49:17,401:INFO:   <<< batch_size_val: 16
2024-05-02 11:49:17,401:INFO:   <<< cache_dir: 
2024-05-02 11:49:17,401:INFO:   <<< coef_lr: 0.001
2024-05-02 11:49:17,401:INFO:   <<< cross_model: cross-base
2024-05-02 11:49:17,401:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 11:49:17,401:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 11:49:17,401:INFO:   <<< datatype: msrvtt
2024-05-02 11:49:17,401:INFO:   <<< do_eval: True
2024-05-02 11:49:17,401:INFO:   <<< do_lower_case: False
2024-05-02 11:49:17,401:INFO:   <<< do_pretrain: False
2024-05-02 11:49:17,401:INFO:   <<< do_train: False
2024-05-02 11:49:17,401:INFO:   <<< epochs: 2
2024-05-02 11:49:17,401:INFO:   <<< eval_frame_order: 0
2024-05-02 11:49:17,401:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 11:49:17,401:INFO:   <<< feature_framerate: 1
2024-05-02 11:49:17,402:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 11:49:17,402:INFO:   <<< fp16: False
2024-05-02 11:49:17,402:INFO:   <<< fp16_opt_level: O1
2024-05-02 11:49:17,402:INFO:   <<< freeze_layer_num: 0
2024-05-02 11:49:17,402:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 11:49:17,402:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 11:49:17,402:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 11:49:17,402:INFO:   <<< linear_patch: 2d
2024-05-02 11:49:17,402:INFO:   <<< local_rank: 0
2024-05-02 11:49:17,402:INFO:   <<< loose_type: True
2024-05-02 11:49:17,402:INFO:   <<< lr: 0.0001
2024-05-02 11:49:17,402:INFO:   <<< lr_decay: 0.9
2024-05-02 11:49:17,402:INFO:   <<< margin: 0.1
2024-05-02 11:49:17,402:INFO:   <<< max_frames: 90
2024-05-02 11:49:17,402:INFO:   <<< max_words: 32
2024-05-02 11:49:17,402:INFO:   <<< n_display: 100
2024-05-02 11:49:17,402:INFO:   <<< n_gpu: 1
2024-05-02 11:49:17,403:INFO:   <<< n_pair: 1
2024-05-02 11:49:17,403:INFO:   <<< negative_weighting: 1
2024-05-02 11:49:17,403:INFO:   <<< num_thread_reader: 0
2024-05-02 11:49:17,403:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 11:49:17,403:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 11:49:17,403:INFO:   <<< rank: 0
2024-05-02 11:49:17,403:INFO:   <<< resume_model: None
2024-05-02 11:49:17,403:INFO:   <<< sampled_use_mil: False
2024-05-02 11:49:17,403:INFO:   <<< seed: 42
2024-05-02 11:49:17,403:INFO:   <<< sim_header: meanP
2024-05-02 11:49:17,403:INFO:   <<< slice_framepos: 2
2024-05-02 11:49:17,403:INFO:   <<< task_type: retrieval
2024-05-02 11:49:17,403:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 11:49:17,403:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 11:49:17,403:INFO:   <<< train_frame_order: 0
2024-05-02 11:49:17,403:INFO:   <<< use_mil: False
2024-05-02 11:49:17,403:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 11:49:17,403:INFO:   <<< video_dim: 1024
2024-05-02 11:49:17,404:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 11:49:17,404:INFO:   <<< warmup_proportion: 0.1
2024-05-02 11:49:17,404:INFO:   <<< world_size: 1
2024-05-02 11:49:17,404:INFO: device: cuda:0 n_gpu: 1
2024-05-02 11:49:17,552:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 11:49:18,020:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 11:49:18,021:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 11:49:18,021:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 11:49:18,022:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 11:49:18,022:WARNING: Test retrieval by loose type.
2024-05-02 11:49:18,022:WARNING: 	 embed_dim: 512
2024-05-02 11:49:18,022:WARNING: 	 image_resolution: 224
2024-05-02 11:49:18,023:WARNING: 	 vision_layers: 12
2024-05-02 11:49:18,023:WARNING: 	 vision_width: 768
2024-05-02 11:49:18,023:WARNING: 	 vision_patch_size: 32
2024-05-02 11:49:18,023:WARNING: 	 context_length: 77
2024-05-02 11:49:18,023:WARNING: 	 vocab_size: 49408
2024-05-02 11:49:18,023:WARNING: 	 transformer_width: 512
2024-05-02 11:49:18,023:WARNING: 	 transformer_heads: 8
2024-05-02 11:49:18,023:WARNING: 	 transformer_layers: 12
2024-05-02 11:49:18,023:WARNING: 		 linear_patch: 2d
2024-05-02 11:49:18,023:WARNING: 	 cut_top_layer: 0
2024-05-02 11:49:18,915:WARNING: 	 sim_header: meanP
2024-05-02 11:49:22,623:INFO: --------------------
2024-05-02 11:49:22,623:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:13:26,220:INFO: Effective parameters:
2024-05-02 13:13:26,220:INFO:   <<< batch_size: 1
2024-05-02 13:13:26,220:INFO:   <<< batch_size_val: 16
2024-05-02 13:13:26,220:INFO:   <<< cache_dir: 
2024-05-02 13:13:26,220:INFO:   <<< coef_lr: 0.001
2024-05-02 13:13:26,220:INFO:   <<< cross_model: cross-base
2024-05-02 13:13:26,221:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:13:26,221:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:13:26,221:INFO:   <<< datatype: msrvtt
2024-05-02 13:13:26,221:INFO:   <<< do_eval: True
2024-05-02 13:13:26,221:INFO:   <<< do_lower_case: False
2024-05-02 13:13:26,221:INFO:   <<< do_pretrain: False
2024-05-02 13:13:26,221:INFO:   <<< do_train: False
2024-05-02 13:13:26,221:INFO:   <<< epochs: 2
2024-05-02 13:13:26,221:INFO:   <<< eval_frame_order: 0
2024-05-02 13:13:26,221:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:13:26,221:INFO:   <<< feature_framerate: 1
2024-05-02 13:13:26,221:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:13:26,221:INFO:   <<< fp16: False
2024-05-02 13:13:26,221:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:13:26,221:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:13:26,221:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:13:26,221:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:13:26,222:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:13:26,222:INFO:   <<< linear_patch: 2d
2024-05-02 13:13:26,222:INFO:   <<< local_rank: 0
2024-05-02 13:13:26,222:INFO:   <<< loose_type: True
2024-05-02 13:13:26,222:INFO:   <<< lr: 0.0001
2024-05-02 13:13:26,222:INFO:   <<< lr_decay: 0.9
2024-05-02 13:13:26,222:INFO:   <<< margin: 0.1
2024-05-02 13:13:26,222:INFO:   <<< max_frames: 90
2024-05-02 13:13:26,222:INFO:   <<< max_words: 32
2024-05-02 13:13:26,222:INFO:   <<< n_display: 100
2024-05-02 13:13:26,222:INFO:   <<< n_gpu: 1
2024-05-02 13:13:26,222:INFO:   <<< n_pair: 1
2024-05-02 13:13:26,222:INFO:   <<< negative_weighting: 1
2024-05-02 13:13:26,222:INFO:   <<< num_thread_reader: 0
2024-05-02 13:13:26,222:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:13:26,222:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:13:26,222:INFO:   <<< rank: 0
2024-05-02 13:13:26,222:INFO:   <<< resume_model: None
2024-05-02 13:13:26,223:INFO:   <<< sampled_use_mil: False
2024-05-02 13:13:26,223:INFO:   <<< seed: 42
2024-05-02 13:13:26,223:INFO:   <<< sim_header: meanP
2024-05-02 13:13:26,223:INFO:   <<< slice_framepos: 2
2024-05-02 13:13:26,223:INFO:   <<< task_type: retrieval
2024-05-02 13:13:26,223:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:13:26,223:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:13:26,223:INFO:   <<< train_frame_order: 0
2024-05-02 13:13:26,223:INFO:   <<< use_mil: False
2024-05-02 13:13:26,223:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:13:26,223:INFO:   <<< video_dim: 1024
2024-05-02 13:13:26,223:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:13:26,223:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:13:26,223:INFO:   <<< world_size: 1
2024-05-02 13:13:26,223:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:13:26,367:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:13:26,855:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:13:26,856:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:13:26,856:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:13:26,856:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:13:26,856:WARNING: Test retrieval by loose type.
2024-05-02 13:13:26,857:WARNING: 	 embed_dim: 512
2024-05-02 13:13:26,857:WARNING: 	 image_resolution: 224
2024-05-02 13:13:26,857:WARNING: 	 vision_layers: 12
2024-05-02 13:13:26,857:WARNING: 	 vision_width: 768
2024-05-02 13:13:26,857:WARNING: 	 vision_patch_size: 32
2024-05-02 13:13:26,857:WARNING: 	 context_length: 77
2024-05-02 13:13:26,857:WARNING: 	 vocab_size: 49408
2024-05-02 13:13:26,857:WARNING: 	 transformer_width: 512
2024-05-02 13:13:26,857:WARNING: 	 transformer_heads: 8
2024-05-02 13:13:26,857:WARNING: 	 transformer_layers: 12
2024-05-02 13:13:26,857:WARNING: 		 linear_patch: 2d
2024-05-02 13:13:26,857:WARNING: 	 cut_top_layer: 0
2024-05-02 13:13:27,726:WARNING: 	 sim_header: meanP
2024-05-02 13:13:31,362:INFO: --------------------
2024-05-02 13:13:31,362:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:14:45,347:INFO: Effective parameters:
2024-05-02 13:14:45,348:INFO:   <<< batch_size: 1
2024-05-02 13:14:45,348:INFO:   <<< batch_size_val: 16
2024-05-02 13:14:45,348:INFO:   <<< cache_dir: 
2024-05-02 13:14:45,348:INFO:   <<< coef_lr: 0.001
2024-05-02 13:14:45,348:INFO:   <<< cross_model: cross-base
2024-05-02 13:14:45,348:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:14:45,348:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:14:45,348:INFO:   <<< datatype: msrvtt
2024-05-02 13:14:45,348:INFO:   <<< do_eval: True
2024-05-02 13:14:45,348:INFO:   <<< do_lower_case: False
2024-05-02 13:14:45,348:INFO:   <<< do_pretrain: False
2024-05-02 13:14:45,348:INFO:   <<< do_train: False
2024-05-02 13:14:45,348:INFO:   <<< epochs: 2
2024-05-02 13:14:45,348:INFO:   <<< eval_frame_order: 0
2024-05-02 13:14:45,348:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:14:45,348:INFO:   <<< feature_framerate: 1
2024-05-02 13:14:45,348:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:14:45,349:INFO:   <<< fp16: False
2024-05-02 13:14:45,349:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:14:45,349:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:14:45,349:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:14:45,349:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:14:45,349:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:14:45,349:INFO:   <<< linear_patch: 2d
2024-05-02 13:14:45,349:INFO:   <<< local_rank: 0
2024-05-02 13:14:45,349:INFO:   <<< loose_type: True
2024-05-02 13:14:45,349:INFO:   <<< lr: 0.0001
2024-05-02 13:14:45,349:INFO:   <<< lr_decay: 0.9
2024-05-02 13:14:45,349:INFO:   <<< margin: 0.1
2024-05-02 13:14:45,349:INFO:   <<< max_frames: 90
2024-05-02 13:14:45,349:INFO:   <<< max_words: 32
2024-05-02 13:14:45,349:INFO:   <<< n_display: 100
2024-05-02 13:14:45,349:INFO:   <<< n_gpu: 1
2024-05-02 13:14:45,349:INFO:   <<< n_pair: 1
2024-05-02 13:14:45,350:INFO:   <<< negative_weighting: 1
2024-05-02 13:14:45,350:INFO:   <<< num_thread_reader: 0
2024-05-02 13:14:45,350:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:14:45,350:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:14:45,350:INFO:   <<< rank: 0
2024-05-02 13:14:45,350:INFO:   <<< resume_model: None
2024-05-02 13:14:45,350:INFO:   <<< sampled_use_mil: False
2024-05-02 13:14:45,350:INFO:   <<< seed: 42
2024-05-02 13:14:45,350:INFO:   <<< sim_header: meanP
2024-05-02 13:14:45,350:INFO:   <<< slice_framepos: 2
2024-05-02 13:14:45,350:INFO:   <<< task_type: retrieval
2024-05-02 13:14:45,350:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:14:45,350:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:14:45,350:INFO:   <<< train_frame_order: 0
2024-05-02 13:14:45,350:INFO:   <<< use_mil: False
2024-05-02 13:14:45,350:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:14:45,350:INFO:   <<< video_dim: 1024
2024-05-02 13:14:45,351:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:14:45,351:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:14:45,351:INFO:   <<< world_size: 1
2024-05-02 13:14:45,351:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:14:45,499:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:14:45,997:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:14:45,998:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:14:45,998:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:14:45,998:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:14:45,998:WARNING: Test retrieval by loose type.
2024-05-02 13:14:45,999:WARNING: 	 embed_dim: 512
2024-05-02 13:14:45,999:WARNING: 	 image_resolution: 224
2024-05-02 13:14:45,999:WARNING: 	 vision_layers: 12
2024-05-02 13:14:45,999:WARNING: 	 vision_width: 768
2024-05-02 13:14:45,999:WARNING: 	 vision_patch_size: 32
2024-05-02 13:14:45,999:WARNING: 	 context_length: 77
2024-05-02 13:14:45,999:WARNING: 	 vocab_size: 49408
2024-05-02 13:14:45,999:WARNING: 	 transformer_width: 512
2024-05-02 13:14:45,999:WARNING: 	 transformer_heads: 8
2024-05-02 13:14:45,999:WARNING: 	 transformer_layers: 12
2024-05-02 13:14:45,999:WARNING: 		 linear_patch: 2d
2024-05-02 13:14:45,999:WARNING: 	 cut_top_layer: 0
2024-05-02 13:14:46,896:WARNING: 	 sim_header: meanP
2024-05-02 13:14:50,650:INFO: --------------------
2024-05-02 13:14:50,650:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:15:52,722:INFO: Effective parameters:
2024-05-02 13:15:52,722:INFO:   <<< batch_size: 1
2024-05-02 13:15:52,722:INFO:   <<< batch_size_val: 16
2024-05-02 13:15:52,722:INFO:   <<< cache_dir: 
2024-05-02 13:15:52,723:INFO:   <<< coef_lr: 0.001
2024-05-02 13:15:52,723:INFO:   <<< cross_model: cross-base
2024-05-02 13:15:52,723:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:15:52,723:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:15:52,723:INFO:   <<< datatype: msrvtt
2024-05-02 13:15:52,723:INFO:   <<< do_eval: True
2024-05-02 13:15:52,723:INFO:   <<< do_lower_case: False
2024-05-02 13:15:52,723:INFO:   <<< do_pretrain: False
2024-05-02 13:15:52,723:INFO:   <<< do_train: False
2024-05-02 13:15:52,723:INFO:   <<< epochs: 2
2024-05-02 13:15:52,723:INFO:   <<< eval_frame_order: 0
2024-05-02 13:15:52,723:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:15:52,723:INFO:   <<< feature_framerate: 1
2024-05-02 13:15:52,723:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:15:52,723:INFO:   <<< fp16: False
2024-05-02 13:15:52,723:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:15:52,723:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:15:52,724:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:15:52,724:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:15:52,724:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:15:52,724:INFO:   <<< linear_patch: 2d
2024-05-02 13:15:52,724:INFO:   <<< local_rank: 0
2024-05-02 13:15:52,724:INFO:   <<< loose_type: True
2024-05-02 13:15:52,724:INFO:   <<< lr: 0.0001
2024-05-02 13:15:52,724:INFO:   <<< lr_decay: 0.9
2024-05-02 13:15:52,724:INFO:   <<< margin: 0.1
2024-05-02 13:15:52,724:INFO:   <<< max_frames: 90
2024-05-02 13:15:52,724:INFO:   <<< max_words: 32
2024-05-02 13:15:52,724:INFO:   <<< n_display: 100
2024-05-02 13:15:52,724:INFO:   <<< n_gpu: 1
2024-05-02 13:15:52,724:INFO:   <<< n_pair: 1
2024-05-02 13:15:52,724:INFO:   <<< negative_weighting: 1
2024-05-02 13:15:52,724:INFO:   <<< num_thread_reader: 0
2024-05-02 13:15:52,724:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:15:52,725:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:15:52,725:INFO:   <<< rank: 0
2024-05-02 13:15:52,725:INFO:   <<< resume_model: None
2024-05-02 13:15:52,725:INFO:   <<< sampled_use_mil: False
2024-05-02 13:15:52,725:INFO:   <<< seed: 42
2024-05-02 13:15:52,725:INFO:   <<< sim_header: meanP
2024-05-02 13:15:52,725:INFO:   <<< slice_framepos: 2
2024-05-02 13:15:52,725:INFO:   <<< task_type: retrieval
2024-05-02 13:15:52,725:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:15:52,725:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:15:52,725:INFO:   <<< train_frame_order: 0
2024-05-02 13:15:52,725:INFO:   <<< use_mil: False
2024-05-02 13:15:52,725:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:15:52,725:INFO:   <<< video_dim: 1024
2024-05-02 13:15:52,725:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:15:52,725:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:15:52,725:INFO:   <<< world_size: 1
2024-05-02 13:15:52,726:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:15:52,872:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:15:53,385:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:15:53,387:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:15:53,387:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:15:53,387:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:15:53,387:WARNING: Test retrieval by loose type.
2024-05-02 13:15:53,389:WARNING: 	 embed_dim: 512
2024-05-02 13:15:53,389:WARNING: 	 image_resolution: 224
2024-05-02 13:15:53,389:WARNING: 	 vision_layers: 12
2024-05-02 13:15:53,389:WARNING: 	 vision_width: 768
2024-05-02 13:15:53,389:WARNING: 	 vision_patch_size: 32
2024-05-02 13:15:53,389:WARNING: 	 context_length: 77
2024-05-02 13:15:53,389:WARNING: 	 vocab_size: 49408
2024-05-02 13:15:53,390:WARNING: 	 transformer_width: 512
2024-05-02 13:15:53,390:WARNING: 	 transformer_heads: 8
2024-05-02 13:15:53,390:WARNING: 	 transformer_layers: 12
2024-05-02 13:15:53,390:WARNING: 		 linear_patch: 2d
2024-05-02 13:15:53,390:WARNING: 	 cut_top_layer: 0
2024-05-02 13:15:54,302:WARNING: 	 sim_header: meanP
2024-05-02 13:15:58,024:INFO: --------------------
2024-05-02 13:15:58,024:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:22:17,324:INFO: Effective parameters:
2024-05-02 13:22:17,325:INFO:   <<< batch_size: 1
2024-05-02 13:22:17,325:INFO:   <<< batch_size_val: 16
2024-05-02 13:22:17,325:INFO:   <<< cache_dir: 
2024-05-02 13:22:17,325:INFO:   <<< coef_lr: 0.001
2024-05-02 13:22:17,325:INFO:   <<< cross_model: cross-base
2024-05-02 13:22:17,325:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:22:17,325:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:22:17,325:INFO:   <<< datatype: msrvtt
2024-05-02 13:22:17,325:INFO:   <<< do_eval: True
2024-05-02 13:22:17,325:INFO:   <<< do_lower_case: False
2024-05-02 13:22:17,326:INFO:   <<< do_pretrain: False
2024-05-02 13:22:17,326:INFO:   <<< do_train: False
2024-05-02 13:22:17,326:INFO:   <<< epochs: 2
2024-05-02 13:22:17,326:INFO:   <<< eval_frame_order: 0
2024-05-02 13:22:17,326:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:22:17,326:INFO:   <<< feature_framerate: 1
2024-05-02 13:22:17,326:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:22:17,326:INFO:   <<< fp16: False
2024-05-02 13:22:17,326:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:22:17,326:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:22:17,326:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:22:17,326:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:22:17,326:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:22:17,326:INFO:   <<< linear_patch: 2d
2024-05-02 13:22:17,326:INFO:   <<< local_rank: 0
2024-05-02 13:22:17,326:INFO:   <<< loose_type: True
2024-05-02 13:22:17,326:INFO:   <<< lr: 0.0001
2024-05-02 13:22:17,327:INFO:   <<< lr_decay: 0.9
2024-05-02 13:22:17,327:INFO:   <<< margin: 0.1
2024-05-02 13:22:17,327:INFO:   <<< max_frames: 90
2024-05-02 13:22:17,327:INFO:   <<< max_words: 32
2024-05-02 13:22:17,327:INFO:   <<< n_display: 100
2024-05-02 13:22:17,327:INFO:   <<< n_gpu: 1
2024-05-02 13:22:17,327:INFO:   <<< n_pair: 1
2024-05-02 13:22:17,327:INFO:   <<< negative_weighting: 1
2024-05-02 13:22:17,327:INFO:   <<< num_thread_reader: 0
2024-05-02 13:22:17,327:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:22:17,327:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:22:17,327:INFO:   <<< rank: 0
2024-05-02 13:22:17,327:INFO:   <<< resume_model: None
2024-05-02 13:22:17,327:INFO:   <<< sampled_use_mil: False
2024-05-02 13:22:17,327:INFO:   <<< seed: 42
2024-05-02 13:22:17,327:INFO:   <<< sim_header: meanP
2024-05-02 13:22:17,327:INFO:   <<< slice_framepos: 2
2024-05-02 13:22:17,328:INFO:   <<< task_type: retrieval
2024-05-02 13:22:17,328:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:22:17,328:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:22:17,328:INFO:   <<< train_frame_order: 0
2024-05-02 13:22:17,328:INFO:   <<< use_mil: False
2024-05-02 13:22:17,328:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:22:17,328:INFO:   <<< video_dim: 1024
2024-05-02 13:22:17,328:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:22:17,328:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:22:17,328:INFO:   <<< world_size: 1
2024-05-02 13:22:17,328:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:22:18,634:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:22:19,879:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:22:19,881:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:22:19,881:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:22:19,882:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:22:19,882:WARNING: Test retrieval by loose type.
2024-05-02 13:22:19,883:WARNING: 	 embed_dim: 512
2024-05-02 13:22:19,883:WARNING: 	 image_resolution: 224
2024-05-02 13:22:19,883:WARNING: 	 vision_layers: 12
2024-05-02 13:22:19,883:WARNING: 	 vision_width: 768
2024-05-02 13:22:19,884:WARNING: 	 vision_patch_size: 32
2024-05-02 13:22:19,884:WARNING: 	 context_length: 77
2024-05-02 13:22:19,884:WARNING: 	 vocab_size: 49408
2024-05-02 13:22:19,884:WARNING: 	 transformer_width: 512
2024-05-02 13:22:19,884:WARNING: 	 transformer_heads: 8
2024-05-02 13:22:19,884:WARNING: 	 transformer_layers: 12
2024-05-02 13:22:19,884:WARNING: 		 linear_patch: 2d
2024-05-02 13:22:19,884:WARNING: 	 cut_top_layer: 0
2024-05-02 13:22:20,818:WARNING: 	 sim_header: meanP
2024-05-02 13:22:24,560:INFO: --------------------
2024-05-02 13:22:24,560:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:34:30,698:INFO: Effective parameters:
2024-05-02 13:34:30,698:INFO:   <<< batch_size: 16
2024-05-02 13:34:30,698:INFO:   <<< batch_size_val: 16
2024-05-02 13:34:30,698:INFO:   <<< cache_dir: 
2024-05-02 13:34:30,698:INFO:   <<< coef_lr: 0.001
2024-05-02 13:34:30,698:INFO:   <<< cross_model: cross-base
2024-05-02 13:34:30,698:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:34:30,698:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:34:30,698:INFO:   <<< datatype: msrvtt
2024-05-02 13:34:30,698:INFO:   <<< do_eval: True
2024-05-02 13:34:30,699:INFO:   <<< do_lower_case: False
2024-05-02 13:34:30,699:INFO:   <<< do_pretrain: False
2024-05-02 13:34:30,699:INFO:   <<< do_train: False
2024-05-02 13:34:30,699:INFO:   <<< epochs: 2
2024-05-02 13:34:30,699:INFO:   <<< eval_frame_order: 0
2024-05-02 13:34:30,699:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:34:30,699:INFO:   <<< feature_framerate: 1
2024-05-02 13:34:30,699:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:34:30,699:INFO:   <<< fp16: False
2024-05-02 13:34:30,699:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:34:30,699:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:34:30,699:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:34:30,699:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:34:30,699:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:34:30,700:INFO:   <<< linear_patch: 2d
2024-05-02 13:34:30,700:INFO:   <<< local_rank: 0
2024-05-02 13:34:30,700:INFO:   <<< loose_type: True
2024-05-02 13:34:30,700:INFO:   <<< lr: 0.0001
2024-05-02 13:34:30,700:INFO:   <<< lr_decay: 0.9
2024-05-02 13:34:30,700:INFO:   <<< margin: 0.1
2024-05-02 13:34:30,700:INFO:   <<< max_frames: 12
2024-05-02 13:34:30,700:INFO:   <<< max_words: 32
2024-05-02 13:34:30,700:INFO:   <<< n_display: 100
2024-05-02 13:34:30,700:INFO:   <<< n_gpu: 1
2024-05-02 13:34:30,700:INFO:   <<< n_pair: 1
2024-05-02 13:34:30,700:INFO:   <<< negative_weighting: 1
2024-05-02 13:34:30,700:INFO:   <<< num_thread_reader: 0
2024-05-02 13:34:30,701:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:34:30,701:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:34:30,701:INFO:   <<< rank: 0
2024-05-02 13:34:30,701:INFO:   <<< resume_model: None
2024-05-02 13:34:30,701:INFO:   <<< sampled_use_mil: False
2024-05-02 13:34:30,701:INFO:   <<< seed: 42
2024-05-02 13:34:30,701:INFO:   <<< sim_header: meanP
2024-05-02 13:34:30,701:INFO:   <<< slice_framepos: 2
2024-05-02 13:34:30,701:INFO:   <<< task_type: retrieval
2024-05-02 13:34:30,701:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:34:30,702:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:34:30,702:INFO:   <<< train_frame_order: 0
2024-05-02 13:34:30,702:INFO:   <<< use_mil: False
2024-05-02 13:34:30,702:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:34:30,702:INFO:   <<< video_dim: 1024
2024-05-02 13:34:30,702:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:34:30,702:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:34:30,702:INFO:   <<< world_size: 1
2024-05-02 13:34:30,702:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:34:30,979:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:34:31,461:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:34:31,463:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:34:31,463:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:34:31,464:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:34:31,464:WARNING: Test retrieval by loose type.
2024-05-02 13:34:31,465:WARNING: 	 embed_dim: 512
2024-05-02 13:34:31,465:WARNING: 	 image_resolution: 224
2024-05-02 13:34:31,465:WARNING: 	 vision_layers: 12
2024-05-02 13:34:31,465:WARNING: 	 vision_width: 768
2024-05-02 13:34:31,465:WARNING: 	 vision_patch_size: 32
2024-05-02 13:34:31,465:WARNING: 	 context_length: 77
2024-05-02 13:34:31,465:WARNING: 	 vocab_size: 49408
2024-05-02 13:34:31,465:WARNING: 	 transformer_width: 512
2024-05-02 13:34:31,465:WARNING: 	 transformer_heads: 8
2024-05-02 13:34:31,465:WARNING: 	 transformer_layers: 12
2024-05-02 13:34:31,466:WARNING: 		 linear_patch: 2d
2024-05-02 13:34:31,466:WARNING: 	 cut_top_layer: 0
2024-05-02 13:34:32,318:WARNING: 	 sim_header: meanP
2024-05-02 13:34:36,036:INFO: --------------------
2024-05-02 13:34:36,036:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2024-05-02 13:34:38,139:INFO: ***** Running test *****
2024-05-02 13:34:38,139:INFO:   Num examples = 22
2024-05-02 13:34:38,139:INFO:   Batch size = 16
2024-05-02 13:34:38,139:INFO:   Num steps = 2
2024-05-02 13:34:38,139:INFO: ***** Running val *****
2024-05-02 13:34:38,139:INFO:   Num examples = 22
2024-05-02 13:36:39,771:INFO: Effective parameters:
2024-05-02 13:36:39,771:INFO:   <<< batch_size: 1
2024-05-02 13:36:39,771:INFO:   <<< batch_size_val: 16
2024-05-02 13:36:39,771:INFO:   <<< cache_dir: 
2024-05-02 13:36:39,771:INFO:   <<< coef_lr: 0.001
2024-05-02 13:36:39,772:INFO:   <<< cross_model: cross-base
2024-05-02 13:36:39,772:INFO:   <<< cross_num_hidden_layers: 4
2024-05-02 13:36:39,772:INFO:   <<< data_path: /workspace/CLIP4Clip/data/MSRVTT_data.json
2024-05-02 13:36:39,772:INFO:   <<< datatype: msrvtt
2024-05-02 13:36:39,772:INFO:   <<< do_eval: True
2024-05-02 13:36:39,772:INFO:   <<< do_lower_case: False
2024-05-02 13:36:39,772:INFO:   <<< do_pretrain: False
2024-05-02 13:36:39,772:INFO:   <<< do_train: False
2024-05-02 13:36:39,772:INFO:   <<< epochs: 2
2024-05-02 13:36:39,772:INFO:   <<< eval_frame_order: 0
2024-05-02 13:36:39,772:INFO:   <<< expand_msrvtt_sentences: True
2024-05-02 13:36:39,772:INFO:   <<< feature_framerate: 1
2024-05-02 13:36:39,772:INFO:   <<< features_path: /workspace/CLIP4Clip/data/MSRVTT_Videos
2024-05-02 13:36:39,772:INFO:   <<< fp16: False
2024-05-02 13:36:39,772:INFO:   <<< fp16_opt_level: O1
2024-05-02 13:36:39,772:INFO:   <<< freeze_layer_num: 0
2024-05-02 13:36:39,772:INFO:   <<< gradient_accumulation_steps: 1
2024-05-02 13:36:39,773:INFO:   <<< hard_negative_rate: 0.5
2024-05-02 13:36:39,773:INFO:   <<< init_model: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:36:39,773:INFO:   <<< linear_patch: 2d
2024-05-02 13:36:39,773:INFO:   <<< local_rank: 0
2024-05-02 13:36:39,773:INFO:   <<< loose_type: True
2024-05-02 13:36:39,773:INFO:   <<< lr: 0.0001
2024-05-02 13:36:39,773:INFO:   <<< lr_decay: 0.9
2024-05-02 13:36:39,773:INFO:   <<< margin: 0.1
2024-05-02 13:36:39,773:INFO:   <<< max_frames: 90
2024-05-02 13:36:39,773:INFO:   <<< max_words: 32
2024-05-02 13:36:39,773:INFO:   <<< n_display: 100
2024-05-02 13:36:39,773:INFO:   <<< n_gpu: 1
2024-05-02 13:36:39,773:INFO:   <<< n_pair: 1
2024-05-02 13:36:39,773:INFO:   <<< negative_weighting: 1
2024-05-02 13:36:39,773:INFO:   <<< num_thread_reader: 0
2024-05-02 13:36:39,773:INFO:   <<< output_dir: /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType
2024-05-02 13:36:39,773:INFO:   <<< pretrained_clip_name: ViT-B/32
2024-05-02 13:36:39,774:INFO:   <<< rank: 0
2024-05-02 13:36:39,774:INFO:   <<< resume_model: None
2024-05-02 13:36:39,774:INFO:   <<< sampled_use_mil: False
2024-05-02 13:36:39,774:INFO:   <<< seed: 42
2024-05-02 13:36:39,774:INFO:   <<< sim_header: meanP
2024-05-02 13:36:39,774:INFO:   <<< slice_framepos: 2
2024-05-02 13:36:39,774:INFO:   <<< task_type: retrieval
2024-05-02 13:36:39,774:INFO:   <<< text_num_hidden_layers: 12
2024-05-02 13:36:39,774:INFO:   <<< train_csv: /workspace/CLIP4Clip/data/MSRVTT_train.9k.csv
2024-05-02 13:36:39,774:INFO:   <<< train_frame_order: 0
2024-05-02 13:36:39,774:INFO:   <<< use_mil: False
2024-05-02 13:36:39,774:INFO:   <<< val_csv: /workspace/CLIP4Clip/data/MSRVTT_JSFUSION_test.csv
2024-05-02 13:36:39,774:INFO:   <<< video_dim: 1024
2024-05-02 13:36:39,774:INFO:   <<< visual_num_hidden_layers: 12
2024-05-02 13:36:39,774:INFO:   <<< warmup_proportion: 0.1
2024-05-02 13:36:39,774:INFO:   <<< world_size: 1
2024-05-02 13:36:39,775:INFO: device: cuda:0 n_gpu: 1
2024-05-02 13:36:39,916:INFO: Model loaded from /workspace/CLIP4Clip/ckpts/ckpt_msrvtt_retrieval_looseType/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb_20230612-b9706e54.pth
2024-05-02 13:36:40,409:INFO: loading archive file /workspace/CLIP4Clip/modules/cross-base
2024-05-02 13:36:40,411:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2024-05-02 13:36:40,412:INFO: Weight doesn't exsits. /workspace/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin
2024-05-02 13:36:40,412:WARNING: Stage-One:True, Stage-Two:False
2024-05-02 13:36:40,412:WARNING: Test retrieval by loose type.
2024-05-02 13:36:40,413:WARNING: 	 embed_dim: 512
2024-05-02 13:36:40,413:WARNING: 	 image_resolution: 224
2024-05-02 13:36:40,414:WARNING: 	 vision_layers: 12
2024-05-02 13:36:40,414:WARNING: 	 vision_width: 768
2024-05-02 13:36:40,414:WARNING: 	 vision_patch_size: 32
2024-05-02 13:36:40,414:WARNING: 	 context_length: 77
2024-05-02 13:36:40,414:WARNING: 	 vocab_size: 49408
2024-05-02 13:36:40,414:WARNING: 	 transformer_width: 512
2024-05-02 13:36:40,414:WARNING: 	 transformer_heads: 8
2024-05-02 13:36:40,414:WARNING: 	 transformer_layers: 12
2024-05-02 13:36:40,414:WARNING: 		 linear_patch: 2d
2024-05-02 13:36:40,414:WARNING: 	 cut_top_layer: 0
2024-05-02 13:36:41,327:WARNING: 	 sim_header: meanP
2024-05-02 13:36:45,039:INFO: --------------------
2024-05-02 13:36:45,040:INFO: Weights from pretrained model not used in CLIP4Clip: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
